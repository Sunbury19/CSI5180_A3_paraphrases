{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'source_folder'\n",
    "saved_folder = 'saved_folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 35\n",
    "BATCH_SIZE = 8\n",
    "pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "unk_index = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "label_field = Field(sequential = False, use_vocab=False, batch_first=True)\n",
    "text_field = Field(use_vocab=False, tokenize = tokenizer.encode, include_lengths=False, \n",
    "                   batch_first=True, fix_length=max_len,pad_token = pad_index,unk_token=unk_index)\n",
    "fields = [('sentences', text_field), ('label', label_field)]\n",
    "\n",
    "\n",
    "\n",
    "train, dev, test = TabularDataset.splits(path=source_folder, train='train_df.csv',\n",
    "                                           validation='dev_df.csv',\n",
    "                                           test='test_df.csv', format='CSV', \n",
    "                                           fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.sentences),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "dev_iter = BucketIterator(dev, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.sentences),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for saving and loading model parameters and metrics.\n",
    "def save_checkpoint(path, model, valid_loss):\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}, path)\n",
    "\n",
    "    \n",
    "def load_checkpoint(path, model):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(path, train_loss_list, valid_loss_list, global_steps_list):   \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, path)\n",
    "\n",
    "\n",
    "def load_metrics(path):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with extra layers on top of RoBERTa\n",
    "class BertClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def forward(self,input_ids, attention_mask):\n",
    "        x = self.bert(input_ids=input_ids, attention_mask = attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter, dev_iter, epochs, scheduler):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    dev_loss = 0.0\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        count = 0\n",
    "        for (source, target), _ in train_iter:\n",
    "            mask = (source!=pad_index).type(torch.uint8)\n",
    "            y_pred = model(input_ids = source, attention_mask=mask)[0]\n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred,target)\n",
    "            print('batch_no[{}/{}]:'.format(count, int(len(train_iter))),'training_loss:', loss)\n",
    "            count+=1\n",
    "            loss.backward()\n",
    "            \n",
    "            scheduler.step()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_loss == loss.item()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step% len(train_iter)==0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for (source,target), _ in dev_iter:\n",
    "                        mask = (source != pad_index).type(torch.uint8)\n",
    "                        \n",
    "                        y_pred = model(input_ids = source, attention_mask = mask)[0]\n",
    "                        \n",
    "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "                        \n",
    "                        dev_loss += loss.item()\n",
    "                        \n",
    "                train_loss = train_loss/ len(train_iter)\n",
    "                dev_loss = dev_loss/len(dev_iter)\n",
    "                \n",
    "                model.train()\n",
    "                print('Epoch [{}/{}],global step [{}/{}], pt loss:{:.4f}, dev loss:{:.4f}'.format(epoch+1, epochs, \n",
    "                                                                                                  global_step, \n",
    "                                                                                                  epochs*len(train_iter),\n",
    "                                                                                                 train_loss,\n",
    "                                                                                          dev_loss))\n",
    "                train_loss = 0.0\n",
    "                dev_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[0/1442]: training_loss: tensor(0.5488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1/1442]: training_loss: tensor(0.5933, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\conda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[2/1442]: training_loss: tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[3/1442]: training_loss: tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[4/1442]: training_loss: tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[5/1442]: training_loss: tensor(0.4720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[6/1442]: training_loss: tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[7/1442]: training_loss: tensor(0.4579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[8/1442]: training_loss: tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[9/1442]: training_loss: tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[10/1442]: training_loss: tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[11/1442]: training_loss: tensor(0.4606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[12/1442]: training_loss: tensor(0.4638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[13/1442]: training_loss: tensor(0.5847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[14/1442]: training_loss: tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[15/1442]: training_loss: tensor(0.4688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[16/1442]: training_loss: tensor(0.5460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[17/1442]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[18/1442]: training_loss: tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[19/1442]: training_loss: tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[20/1442]: training_loss: tensor(0.5240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[21/1442]: training_loss: tensor(0.4622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[22/1442]: training_loss: tensor(0.5176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[23/1442]: training_loss: tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[24/1442]: training_loss: tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[25/1442]: training_loss: tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[26/1442]: training_loss: tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[27/1442]: training_loss: tensor(0.4949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[28/1442]: training_loss: tensor(0.4733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[29/1442]: training_loss: tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[30/1442]: training_loss: tensor(0.7397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[31/1442]: training_loss: tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[32/1442]: training_loss: tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[33/1442]: training_loss: tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[34/1442]: training_loss: tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[35/1442]: training_loss: tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[36/1442]: training_loss: tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[37/1442]: training_loss: tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[38/1442]: training_loss: tensor(0.4994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[39/1442]: training_loss: tensor(0.4752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[40/1442]: training_loss: tensor(0.4511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[41/1442]: training_loss: tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[42/1442]: training_loss: tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[43/1442]: training_loss: tensor(0.5976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[44/1442]: training_loss: tensor(0.7306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[45/1442]: training_loss: tensor(0.5158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[46/1442]: training_loss: tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[47/1442]: training_loss: tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[48/1442]: training_loss: tensor(0.5002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[49/1442]: training_loss: tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[50/1442]: training_loss: tensor(0.4418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[51/1442]: training_loss: tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[52/1442]: training_loss: tensor(0.4524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[53/1442]: training_loss: tensor(0.5537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[54/1442]: training_loss: tensor(0.5320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[55/1442]: training_loss: tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[56/1442]: training_loss: tensor(0.7969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[57/1442]: training_loss: tensor(0.5686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[58/1442]: training_loss: tensor(0.6507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[59/1442]: training_loss: tensor(0.4739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[60/1442]: training_loss: tensor(0.5125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[61/1442]: training_loss: tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[62/1442]: training_loss: tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[63/1442]: training_loss: tensor(0.6551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[64/1442]: training_loss: tensor(0.4827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[65/1442]: training_loss: tensor(0.5220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[66/1442]: training_loss: tensor(0.4665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[67/1442]: training_loss: tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[68/1442]: training_loss: tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[69/1442]: training_loss: tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[70/1442]: training_loss: tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[71/1442]: training_loss: tensor(0.5086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[72/1442]: training_loss: tensor(0.8100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[73/1442]: training_loss: tensor(0.4463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[74/1442]: training_loss: tensor(0.5182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[75/1442]: training_loss: tensor(0.4411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[76/1442]: training_loss: tensor(0.7507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[77/1442]: training_loss: tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[78/1442]: training_loss: tensor(0.4781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[79/1442]: training_loss: tensor(0.5654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[80/1442]: training_loss: tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[81/1442]: training_loss: tensor(0.5917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[82/1442]: training_loss: tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[83/1442]: training_loss: tensor(0.5469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[84/1442]: training_loss: tensor(0.5504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[85/1442]: training_loss: tensor(0.5000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[86/1442]: training_loss: tensor(0.4885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[87/1442]: training_loss: tensor(0.5220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[88/1442]: training_loss: tensor(0.7564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[89/1442]: training_loss: tensor(0.5013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[90/1442]: training_loss: tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[91/1442]: training_loss: tensor(0.4754, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[92/1442]: training_loss: tensor(0.5608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[93/1442]: training_loss: tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[94/1442]: training_loss: tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[95/1442]: training_loss: tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[96/1442]: training_loss: tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[97/1442]: training_loss: tensor(0.4847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[98/1442]: training_loss: tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[99/1442]: training_loss: tensor(0.7573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[100/1442]: training_loss: tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[101/1442]: training_loss: tensor(0.7670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[102/1442]: training_loss: tensor(0.6186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[103/1442]: training_loss: tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[104/1442]: training_loss: tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[105/1442]: training_loss: tensor(0.5306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[106/1442]: training_loss: tensor(0.4762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[107/1442]: training_loss: tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[108/1442]: training_loss: tensor(0.8317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[109/1442]: training_loss: tensor(0.5290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[110/1442]: training_loss: tensor(0.5823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[111/1442]: training_loss: tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[112/1442]: training_loss: tensor(0.7302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[113/1442]: training_loss: tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[114/1442]: training_loss: tensor(0.6720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[115/1442]: training_loss: tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[116/1442]: training_loss: tensor(0.5430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[117/1442]: training_loss: tensor(0.5502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[118/1442]: training_loss: tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[119/1442]: training_loss: tensor(0.6290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[120/1442]: training_loss: tensor(0.5713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[121/1442]: training_loss: tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[122/1442]: training_loss: tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[123/1442]: training_loss: tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[124/1442]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[125/1442]: training_loss: tensor(0.5274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[126/1442]: training_loss: tensor(0.6578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[127/1442]: training_loss: tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[128/1442]: training_loss: tensor(0.8705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[129/1442]: training_loss: tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[130/1442]: training_loss: tensor(0.4974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[131/1442]: training_loss: tensor(0.5270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[132/1442]: training_loss: tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[133/1442]: training_loss: tensor(0.5410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[134/1442]: training_loss: tensor(0.7866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[135/1442]: training_loss: tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[136/1442]: training_loss: tensor(0.8188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[137/1442]: training_loss: tensor(0.6419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[138/1442]: training_loss: tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[139/1442]: training_loss: tensor(0.7980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[140/1442]: training_loss: tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[141/1442]: training_loss: tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[142/1442]: training_loss: tensor(0.5461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[143/1442]: training_loss: tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[144/1442]: training_loss: tensor(0.8149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[145/1442]: training_loss: tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[146/1442]: training_loss: tensor(0.5002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[147/1442]: training_loss: tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[148/1442]: training_loss: tensor(0.7824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[149/1442]: training_loss: tensor(0.5301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[150/1442]: training_loss: tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[151/1442]: training_loss: tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[152/1442]: training_loss: tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[153/1442]: training_loss: tensor(0.5434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[154/1442]: training_loss: tensor(0.5252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[155/1442]: training_loss: tensor(0.5724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[156/1442]: training_loss: tensor(0.4938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[157/1442]: training_loss: tensor(0.6031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[158/1442]: training_loss: tensor(0.4714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[159/1442]: training_loss: tensor(0.4620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[160/1442]: training_loss: tensor(0.5029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[161/1442]: training_loss: tensor(0.5264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[162/1442]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[163/1442]: training_loss: tensor(0.4682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[164/1442]: training_loss: tensor(0.4799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[165/1442]: training_loss: tensor(0.6353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[166/1442]: training_loss: tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[167/1442]: training_loss: tensor(0.7257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[168/1442]: training_loss: tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[169/1442]: training_loss: tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[170/1442]: training_loss: tensor(0.5350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[171/1442]: training_loss: tensor(0.6308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[172/1442]: training_loss: tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[173/1442]: training_loss: tensor(0.7710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[174/1442]: training_loss: tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[175/1442]: training_loss: tensor(0.5046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[176/1442]: training_loss: tensor(0.7218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[177/1442]: training_loss: tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[178/1442]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[179/1442]: training_loss: tensor(0.7757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[180/1442]: training_loss: tensor(0.7734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[181/1442]: training_loss: tensor(0.7833, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[182/1442]: training_loss: tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[183/1442]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[184/1442]: training_loss: tensor(0.4753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[185/1442]: training_loss: tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[186/1442]: training_loss: tensor(0.5133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[187/1442]: training_loss: tensor(0.7732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[188/1442]: training_loss: tensor(0.9007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[189/1442]: training_loss: tensor(0.9159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[190/1442]: training_loss: tensor(0.4770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[191/1442]: training_loss: tensor(0.6252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[192/1442]: training_loss: tensor(0.7456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[193/1442]: training_loss: tensor(0.5351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[194/1442]: training_loss: tensor(0.5035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[195/1442]: training_loss: tensor(0.5289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[196/1442]: training_loss: tensor(0.5935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[197/1442]: training_loss: tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[198/1442]: training_loss: tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[199/1442]: training_loss: tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[200/1442]: training_loss: tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[201/1442]: training_loss: tensor(0.4827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[202/1442]: training_loss: tensor(0.7773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[203/1442]: training_loss: tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[204/1442]: training_loss: tensor(0.4556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[205/1442]: training_loss: tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[206/1442]: training_loss: tensor(0.8696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[207/1442]: training_loss: tensor(0.5633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[208/1442]: training_loss: tensor(0.5254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[209/1442]: training_loss: tensor(0.5222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[210/1442]: training_loss: tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[211/1442]: training_loss: tensor(0.4949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[212/1442]: training_loss: tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[213/1442]: training_loss: tensor(0.5243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[214/1442]: training_loss: tensor(0.7973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[215/1442]: training_loss: tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[216/1442]: training_loss: tensor(0.4878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[217/1442]: training_loss: tensor(0.5858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[218/1442]: training_loss: tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[219/1442]: training_loss: tensor(0.5177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[220/1442]: training_loss: tensor(0.4891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[221/1442]: training_loss: tensor(0.8303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[222/1442]: training_loss: tensor(0.5099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[223/1442]: training_loss: tensor(0.7622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[224/1442]: training_loss: tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[225/1442]: training_loss: tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[226/1442]: training_loss: tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[227/1442]: training_loss: tensor(0.5188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[228/1442]: training_loss: tensor(0.4885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[229/1442]: training_loss: tensor(0.6023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[230/1442]: training_loss: tensor(0.5822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[231/1442]: training_loss: tensor(0.5827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[232/1442]: training_loss: tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[233/1442]: training_loss: tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[234/1442]: training_loss: tensor(0.4918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[235/1442]: training_loss: tensor(0.6795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[236/1442]: training_loss: tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[237/1442]: training_loss: tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[238/1442]: training_loss: tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[239/1442]: training_loss: tensor(0.5298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[240/1442]: training_loss: tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[241/1442]: training_loss: tensor(0.7475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[242/1442]: training_loss: tensor(0.8548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[243/1442]: training_loss: tensor(0.7518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[244/1442]: training_loss: tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[245/1442]: training_loss: tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[246/1442]: training_loss: tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[247/1442]: training_loss: tensor(0.7786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[248/1442]: training_loss: tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[249/1442]: training_loss: tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[250/1442]: training_loss: tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[251/1442]: training_loss: tensor(0.5276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[252/1442]: training_loss: tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[253/1442]: training_loss: tensor(0.4481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[254/1442]: training_loss: tensor(0.5270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[255/1442]: training_loss: tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[256/1442]: training_loss: tensor(0.5184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[257/1442]: training_loss: tensor(0.5291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[258/1442]: training_loss: tensor(0.5298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[259/1442]: training_loss: tensor(0.6003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[260/1442]: training_loss: tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[261/1442]: training_loss: tensor(0.5634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[262/1442]: training_loss: tensor(0.5815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[263/1442]: training_loss: tensor(0.4942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[264/1442]: training_loss: tensor(0.4698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[265/1442]: training_loss: tensor(0.4866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[266/1442]: training_loss: tensor(0.5137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[267/1442]: training_loss: tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[268/1442]: training_loss: tensor(0.5824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[269/1442]: training_loss: tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[270/1442]: training_loss: tensor(0.6234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[271/1442]: training_loss: tensor(0.5066, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[272/1442]: training_loss: tensor(0.5047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[273/1442]: training_loss: tensor(0.5559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[274/1442]: training_loss: tensor(0.6808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[275/1442]: training_loss: tensor(0.5208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[276/1442]: training_loss: tensor(0.5990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[277/1442]: training_loss: tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[278/1442]: training_loss: tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[279/1442]: training_loss: tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[280/1442]: training_loss: tensor(0.5455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[281/1442]: training_loss: tensor(0.4832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[282/1442]: training_loss: tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[283/1442]: training_loss: tensor(0.9468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[284/1442]: training_loss: tensor(0.9099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[285/1442]: training_loss: tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[286/1442]: training_loss: tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[287/1442]: training_loss: tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[288/1442]: training_loss: tensor(0.9687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[289/1442]: training_loss: tensor(0.4973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[290/1442]: training_loss: tensor(0.6138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[291/1442]: training_loss: tensor(0.6647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[292/1442]: training_loss: tensor(0.7442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[293/1442]: training_loss: tensor(0.5504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[294/1442]: training_loss: tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[295/1442]: training_loss: tensor(0.7355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[296/1442]: training_loss: tensor(0.7924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[297/1442]: training_loss: tensor(0.8211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[298/1442]: training_loss: tensor(0.8470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[299/1442]: training_loss: tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[300/1442]: training_loss: tensor(0.5334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[301/1442]: training_loss: tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[302/1442]: training_loss: tensor(0.4702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[303/1442]: training_loss: tensor(0.4979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[304/1442]: training_loss: tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[305/1442]: training_loss: tensor(0.8072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[306/1442]: training_loss: tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[307/1442]: training_loss: tensor(0.4837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[308/1442]: training_loss: tensor(0.4663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[309/1442]: training_loss: tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[310/1442]: training_loss: tensor(0.4806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[311/1442]: training_loss: tensor(0.7267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[312/1442]: training_loss: tensor(0.5457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[313/1442]: training_loss: tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[314/1442]: training_loss: tensor(0.6326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[315/1442]: training_loss: tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[316/1442]: training_loss: tensor(0.8307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[317/1442]: training_loss: tensor(0.8487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[318/1442]: training_loss: tensor(0.5559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[319/1442]: training_loss: tensor(0.4513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[320/1442]: training_loss: tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[321/1442]: training_loss: tensor(0.6090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[322/1442]: training_loss: tensor(0.5433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[323/1442]: training_loss: tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[324/1442]: training_loss: tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[325/1442]: training_loss: tensor(0.5834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[326/1442]: training_loss: tensor(0.4737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[327/1442]: training_loss: tensor(0.5331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[328/1442]: training_loss: tensor(0.5630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[329/1442]: training_loss: tensor(0.6140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[330/1442]: training_loss: tensor(0.7942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[331/1442]: training_loss: tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[332/1442]: training_loss: tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[333/1442]: training_loss: tensor(0.7736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[334/1442]: training_loss: tensor(0.7429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[335/1442]: training_loss: tensor(0.8371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[336/1442]: training_loss: tensor(0.6692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[337/1442]: training_loss: tensor(0.8474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[338/1442]: training_loss: tensor(0.5035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[339/1442]: training_loss: tensor(0.6067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[340/1442]: training_loss: tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[341/1442]: training_loss: tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[342/1442]: training_loss: tensor(0.5751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[343/1442]: training_loss: tensor(0.5414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[344/1442]: training_loss: tensor(0.7279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[345/1442]: training_loss: tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[346/1442]: training_loss: tensor(0.8343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[347/1442]: training_loss: tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[348/1442]: training_loss: tensor(0.7313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[349/1442]: training_loss: tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[350/1442]: training_loss: tensor(0.5966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[351/1442]: training_loss: tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[352/1442]: training_loss: tensor(0.5494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[353/1442]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[354/1442]: training_loss: tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[355/1442]: training_loss: tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[356/1442]: training_loss: tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[357/1442]: training_loss: tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[358/1442]: training_loss: tensor(0.5298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[359/1442]: training_loss: tensor(0.5048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[360/1442]: training_loss: tensor(0.8738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[361/1442]: training_loss: tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[362/1442]: training_loss: tensor(0.5820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[363/1442]: training_loss: tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[364/1442]: training_loss: tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[365/1442]: training_loss: tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[366/1442]: training_loss: tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[367/1442]: training_loss: tensor(0.5713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[368/1442]: training_loss: tensor(0.5460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[369/1442]: training_loss: tensor(0.4306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[370/1442]: training_loss: tensor(0.4305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[371/1442]: training_loss: tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[372/1442]: training_loss: tensor(0.5569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[373/1442]: training_loss: tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[374/1442]: training_loss: tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[375/1442]: training_loss: tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[376/1442]: training_loss: tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[377/1442]: training_loss: tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[378/1442]: training_loss: tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[379/1442]: training_loss: tensor(0.4996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[380/1442]: training_loss: tensor(0.5489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[381/1442]: training_loss: tensor(0.5331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[382/1442]: training_loss: tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[383/1442]: training_loss: tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[384/1442]: training_loss: tensor(0.5184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[385/1442]: training_loss: tensor(0.4734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[386/1442]: training_loss: tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[387/1442]: training_loss: tensor(0.7890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[388/1442]: training_loss: tensor(0.4788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[389/1442]: training_loss: tensor(0.5051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[390/1442]: training_loss: tensor(0.5904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[391/1442]: training_loss: tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[392/1442]: training_loss: tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[393/1442]: training_loss: tensor(0.5891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[394/1442]: training_loss: tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[395/1442]: training_loss: tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[396/1442]: training_loss: tensor(0.5967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[397/1442]: training_loss: tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[398/1442]: training_loss: tensor(0.6061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[399/1442]: training_loss: tensor(0.5131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[400/1442]: training_loss: tensor(0.5940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[401/1442]: training_loss: tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[402/1442]: training_loss: tensor(0.5635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[403/1442]: training_loss: tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[404/1442]: training_loss: tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[405/1442]: training_loss: tensor(0.8078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[406/1442]: training_loss: tensor(0.8644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[407/1442]: training_loss: tensor(0.7188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[408/1442]: training_loss: tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[409/1442]: training_loss: tensor(0.8177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[410/1442]: training_loss: tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[411/1442]: training_loss: tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[412/1442]: training_loss: tensor(0.5851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[413/1442]: training_loss: tensor(0.8121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[414/1442]: training_loss: tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[415/1442]: training_loss: tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[416/1442]: training_loss: tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[417/1442]: training_loss: tensor(0.6740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[418/1442]: training_loss: tensor(0.9371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[419/1442]: training_loss: tensor(0.9055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[420/1442]: training_loss: tensor(0.4406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[421/1442]: training_loss: tensor(0.4628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[422/1442]: training_loss: tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[423/1442]: training_loss: tensor(0.4445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[424/1442]: training_loss: tensor(0.7755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[425/1442]: training_loss: tensor(0.9789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[426/1442]: training_loss: tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[427/1442]: training_loss: tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[428/1442]: training_loss: tensor(0.5501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[429/1442]: training_loss: tensor(0.5438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[430/1442]: training_loss: tensor(0.5950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[431/1442]: training_loss: tensor(0.8476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[432/1442]: training_loss: tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[433/1442]: training_loss: tensor(0.7274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[434/1442]: training_loss: tensor(0.7881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[435/1442]: training_loss: tensor(0.4579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[436/1442]: training_loss: tensor(0.4886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[437/1442]: training_loss: tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[438/1442]: training_loss: tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[439/1442]: training_loss: tensor(0.9096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[440/1442]: training_loss: tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[441/1442]: training_loss: tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[442/1442]: training_loss: tensor(0.5379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[443/1442]: training_loss: tensor(0.7524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[444/1442]: training_loss: tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[445/1442]: training_loss: tensor(0.5101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[446/1442]: training_loss: tensor(0.5213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[447/1442]: training_loss: tensor(0.5064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[448/1442]: training_loss: tensor(0.4910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[449/1442]: training_loss: tensor(0.7888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[450/1442]: training_loss: tensor(0.5440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[451/1442]: training_loss: tensor(0.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[452/1442]: training_loss: tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[453/1442]: training_loss: tensor(0.4313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[454/1442]: training_loss: tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[455/1442]: training_loss: tensor(0.8838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[456/1442]: training_loss: tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[457/1442]: training_loss: tensor(0.5251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[458/1442]: training_loss: tensor(0.8375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[459/1442]: training_loss: tensor(0.5687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[460/1442]: training_loss: tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[461/1442]: training_loss: tensor(0.5940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[462/1442]: training_loss: tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[463/1442]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[464/1442]: training_loss: tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[465/1442]: training_loss: tensor(0.8121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[466/1442]: training_loss: tensor(0.4491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[467/1442]: training_loss: tensor(0.4239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[468/1442]: training_loss: tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[469/1442]: training_loss: tensor(0.4662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[470/1442]: training_loss: tensor(0.4651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[471/1442]: training_loss: tensor(0.8874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[472/1442]: training_loss: tensor(0.8295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[473/1442]: training_loss: tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[474/1442]: training_loss: tensor(0.6343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[475/1442]: training_loss: tensor(0.7670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[476/1442]: training_loss: tensor(0.4571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[477/1442]: training_loss: tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[478/1442]: training_loss: tensor(0.7680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[479/1442]: training_loss: tensor(0.8381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[480/1442]: training_loss: tensor(0.5177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[481/1442]: training_loss: tensor(0.4751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[482/1442]: training_loss: tensor(0.4507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[483/1442]: training_loss: tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[484/1442]: training_loss: tensor(0.5864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[485/1442]: training_loss: tensor(0.8441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[486/1442]: training_loss: tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[487/1442]: training_loss: tensor(0.7012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[488/1442]: training_loss: tensor(0.6290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[489/1442]: training_loss: tensor(0.4703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[490/1442]: training_loss: tensor(0.7894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[491/1442]: training_loss: tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[492/1442]: training_loss: tensor(0.7928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[493/1442]: training_loss: tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[494/1442]: training_loss: tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[495/1442]: training_loss: tensor(0.8425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[496/1442]: training_loss: tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[497/1442]: training_loss: tensor(0.6090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[498/1442]: training_loss: tensor(0.7509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[499/1442]: training_loss: tensor(0.6399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[500/1442]: training_loss: tensor(0.4962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[501/1442]: training_loss: tensor(0.4492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[502/1442]: training_loss: tensor(0.5077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[503/1442]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[504/1442]: training_loss: tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[505/1442]: training_loss: tensor(0.5534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[506/1442]: training_loss: tensor(0.6065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[507/1442]: training_loss: tensor(0.5699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[508/1442]: training_loss: tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[509/1442]: training_loss: tensor(0.4926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[510/1442]: training_loss: tensor(0.4855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[511/1442]: training_loss: tensor(0.4961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[512/1442]: training_loss: tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[513/1442]: training_loss: tensor(0.4063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[514/1442]: training_loss: tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[515/1442]: training_loss: tensor(0.4378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[516/1442]: training_loss: tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[517/1442]: training_loss: tensor(0.7658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[518/1442]: training_loss: tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[519/1442]: training_loss: tensor(0.5891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[520/1442]: training_loss: tensor(0.5784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[521/1442]: training_loss: tensor(0.7987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[522/1442]: training_loss: tensor(0.8405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[523/1442]: training_loss: tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[524/1442]: training_loss: tensor(0.5264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[525/1442]: training_loss: tensor(0.4793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[526/1442]: training_loss: tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[527/1442]: training_loss: tensor(0.5509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[528/1442]: training_loss: tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[529/1442]: training_loss: tensor(0.7634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[530/1442]: training_loss: tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[531/1442]: training_loss: tensor(0.6249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[532/1442]: training_loss: tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[533/1442]: training_loss: tensor(0.4933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[534/1442]: training_loss: tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[535/1442]: training_loss: tensor(0.4522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[536/1442]: training_loss: tensor(0.7305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[537/1442]: training_loss: tensor(0.8774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[538/1442]: training_loss: tensor(0.9898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[539/1442]: training_loss: tensor(0.8615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[540/1442]: training_loss: tensor(0.5699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[541/1442]: training_loss: tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[542/1442]: training_loss: tensor(0.5194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[543/1442]: training_loss: tensor(0.8742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[544/1442]: training_loss: tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[545/1442]: training_loss: tensor(0.5141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[546/1442]: training_loss: tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[547/1442]: training_loss: tensor(0.5594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[548/1442]: training_loss: tensor(0.5302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[549/1442]: training_loss: tensor(0.5840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[550/1442]: training_loss: tensor(0.4212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[551/1442]: training_loss: tensor(0.7598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[552/1442]: training_loss: tensor(0.8529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[553/1442]: training_loss: tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[554/1442]: training_loss: tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[555/1442]: training_loss: tensor(0.5238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[556/1442]: training_loss: tensor(0.7176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[557/1442]: training_loss: tensor(0.5780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[558/1442]: training_loss: tensor(0.5660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[559/1442]: training_loss: tensor(0.4549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[560/1442]: training_loss: tensor(0.4471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[561/1442]: training_loss: tensor(0.7883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[562/1442]: training_loss: tensor(0.9574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[563/1442]: training_loss: tensor(0.5161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[564/1442]: training_loss: tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[565/1442]: training_loss: tensor(0.5284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[566/1442]: training_loss: tensor(0.5688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[567/1442]: training_loss: tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[568/1442]: training_loss: tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[569/1442]: training_loss: tensor(0.4890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[570/1442]: training_loss: tensor(0.7404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[571/1442]: training_loss: tensor(0.8087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[572/1442]: training_loss: tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[573/1442]: training_loss: tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[574/1442]: training_loss: tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[575/1442]: training_loss: tensor(0.5013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[576/1442]: training_loss: tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[577/1442]: training_loss: tensor(0.6042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[578/1442]: training_loss: tensor(0.4452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[579/1442]: training_loss: tensor(0.9087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[580/1442]: training_loss: tensor(0.4863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[581/1442]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[582/1442]: training_loss: tensor(0.7170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[583/1442]: training_loss: tensor(0.4851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[584/1442]: training_loss: tensor(0.4978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[585/1442]: training_loss: tensor(0.4696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[586/1442]: training_loss: tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[587/1442]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[588/1442]: training_loss: tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[589/1442]: training_loss: tensor(0.4935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[590/1442]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[591/1442]: training_loss: tensor(0.4822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[592/1442]: training_loss: tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[593/1442]: training_loss: tensor(0.4638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[594/1442]: training_loss: tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[595/1442]: training_loss: tensor(0.8590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[596/1442]: training_loss: tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[597/1442]: training_loss: tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[598/1442]: training_loss: tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[599/1442]: training_loss: tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[600/1442]: training_loss: tensor(0.8064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[601/1442]: training_loss: tensor(0.7771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[602/1442]: training_loss: tensor(0.5232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[603/1442]: training_loss: tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[604/1442]: training_loss: tensor(0.4446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[605/1442]: training_loss: tensor(0.5625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[606/1442]: training_loss: tensor(0.4462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[607/1442]: training_loss: tensor(0.5480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[608/1442]: training_loss: tensor(0.8732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[609/1442]: training_loss: tensor(0.9156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[610/1442]: training_loss: tensor(0.7908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[611/1442]: training_loss: tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[612/1442]: training_loss: tensor(0.5081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[613/1442]: training_loss: tensor(1.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[614/1442]: training_loss: tensor(0.9815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[615/1442]: training_loss: tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[616/1442]: training_loss: tensor(0.8193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[617/1442]: training_loss: tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[618/1442]: training_loss: tensor(0.4663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[619/1442]: training_loss: tensor(0.5659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[620/1442]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[621/1442]: training_loss: tensor(0.5362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[622/1442]: training_loss: tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[623/1442]: training_loss: tensor(0.4710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[624/1442]: training_loss: tensor(0.7151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[625/1442]: training_loss: tensor(0.7733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[626/1442]: training_loss: tensor(0.8851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[627/1442]: training_loss: tensor(0.5753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[628/1442]: training_loss: tensor(0.5987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[629/1442]: training_loss: tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[630/1442]: training_loss: tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[631/1442]: training_loss: tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[632/1442]: training_loss: tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[633/1442]: training_loss: tensor(0.6063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[634/1442]: training_loss: tensor(0.5510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[635/1442]: training_loss: tensor(0.7837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[636/1442]: training_loss: tensor(0.8574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[637/1442]: training_loss: tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[638/1442]: training_loss: tensor(0.8889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[639/1442]: training_loss: tensor(0.5772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[640/1442]: training_loss: tensor(0.5010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[641/1442]: training_loss: tensor(0.5625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[642/1442]: training_loss: tensor(0.5247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[643/1442]: training_loss: tensor(0.5029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[644/1442]: training_loss: tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[645/1442]: training_loss: tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[646/1442]: training_loss: tensor(0.5314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[647/1442]: training_loss: tensor(0.4514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[648/1442]: training_loss: tensor(0.6031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[649/1442]: training_loss: tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[650/1442]: training_loss: tensor(0.5064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[651/1442]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[652/1442]: training_loss: tensor(0.4162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[653/1442]: training_loss: tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[654/1442]: training_loss: tensor(0.4991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[655/1442]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[656/1442]: training_loss: tensor(0.4403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[657/1442]: training_loss: tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[658/1442]: training_loss: tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[659/1442]: training_loss: tensor(0.6684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[660/1442]: training_loss: tensor(0.4973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[661/1442]: training_loss: tensor(0.5441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[662/1442]: training_loss: tensor(0.4901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[663/1442]: training_loss: tensor(0.8016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[664/1442]: training_loss: tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[665/1442]: training_loss: tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[666/1442]: training_loss: tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[667/1442]: training_loss: tensor(0.4162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[668/1442]: training_loss: tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[669/1442]: training_loss: tensor(0.8308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[670/1442]: training_loss: tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[671/1442]: training_loss: tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[672/1442]: training_loss: tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[673/1442]: training_loss: tensor(0.7210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[674/1442]: training_loss: tensor(0.5389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[675/1442]: training_loss: tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[676/1442]: training_loss: tensor(0.6561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[677/1442]: training_loss: tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[678/1442]: training_loss: tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[679/1442]: training_loss: tensor(0.8398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[680/1442]: training_loss: tensor(0.8469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[681/1442]: training_loss: tensor(0.5204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[682/1442]: training_loss: tensor(0.8310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[683/1442]: training_loss: tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[684/1442]: training_loss: tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[685/1442]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[686/1442]: training_loss: tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[687/1442]: training_loss: tensor(0.7547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[688/1442]: training_loss: tensor(0.7398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[689/1442]: training_loss: tensor(0.4985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[690/1442]: training_loss: tensor(0.5116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[691/1442]: training_loss: tensor(0.5121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[692/1442]: training_loss: tensor(0.9730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[693/1442]: training_loss: tensor(0.9784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[694/1442]: training_loss: tensor(1.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[695/1442]: training_loss: tensor(0.8722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[696/1442]: training_loss: tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[697/1442]: training_loss: tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[698/1442]: training_loss: tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[699/1442]: training_loss: tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[700/1442]: training_loss: tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[701/1442]: training_loss: tensor(0.7689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[702/1442]: training_loss: tensor(0.9693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[703/1442]: training_loss: tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[704/1442]: training_loss: tensor(0.4700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[705/1442]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[706/1442]: training_loss: tensor(0.5247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[707/1442]: training_loss: tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[708/1442]: training_loss: tensor(0.5918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[709/1442]: training_loss: tensor(0.7958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[710/1442]: training_loss: tensor(0.6985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[711/1442]: training_loss: tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[712/1442]: training_loss: tensor(0.5785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[713/1442]: training_loss: tensor(0.4886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[714/1442]: training_loss: tensor(0.7968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[715/1442]: training_loss: tensor(0.7188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[716/1442]: training_loss: tensor(0.5060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[717/1442]: training_loss: tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[718/1442]: training_loss: tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[719/1442]: training_loss: tensor(0.5612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[720/1442]: training_loss: tensor(0.5100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[721/1442]: training_loss: tensor(0.5015, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[722/1442]: training_loss: tensor(0.7244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[723/1442]: training_loss: tensor(0.5198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[724/1442]: training_loss: tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[725/1442]: training_loss: tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[726/1442]: training_loss: tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[727/1442]: training_loss: tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[728/1442]: training_loss: tensor(0.5504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[729/1442]: training_loss: tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[730/1442]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[731/1442]: training_loss: tensor(0.4977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[732/1442]: training_loss: tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[733/1442]: training_loss: tensor(0.8206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[734/1442]: training_loss: tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[735/1442]: training_loss: tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[736/1442]: training_loss: tensor(0.4515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[737/1442]: training_loss: tensor(0.6311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[738/1442]: training_loss: tensor(0.4255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[739/1442]: training_loss: tensor(0.4374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[740/1442]: training_loss: tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[741/1442]: training_loss: tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[742/1442]: training_loss: tensor(0.8816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[743/1442]: training_loss: tensor(0.8203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[744/1442]: training_loss: tensor(0.5223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[745/1442]: training_loss: tensor(0.4830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[746/1442]: training_loss: tensor(0.5909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[747/1442]: training_loss: tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[748/1442]: training_loss: tensor(0.8426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[749/1442]: training_loss: tensor(0.5304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[750/1442]: training_loss: tensor(0.4643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[751/1442]: training_loss: tensor(0.4447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[752/1442]: training_loss: tensor(0.5540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[753/1442]: training_loss: tensor(0.5741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[754/1442]: training_loss: tensor(0.7420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[755/1442]: training_loss: tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[756/1442]: training_loss: tensor(0.5285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[757/1442]: training_loss: tensor(0.5689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[758/1442]: training_loss: tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[759/1442]: training_loss: tensor(0.7813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[760/1442]: training_loss: tensor(0.7828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[761/1442]: training_loss: tensor(0.6383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[762/1442]: training_loss: tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[763/1442]: training_loss: tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[764/1442]: training_loss: tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[765/1442]: training_loss: tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[766/1442]: training_loss: tensor(0.8181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[767/1442]: training_loss: tensor(0.5217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[768/1442]: training_loss: tensor(0.5692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[769/1442]: training_loss: tensor(0.4134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[770/1442]: training_loss: tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[771/1442]: training_loss: tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[772/1442]: training_loss: tensor(0.5355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[773/1442]: training_loss: tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[774/1442]: training_loss: tensor(0.5383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[775/1442]: training_loss: tensor(0.6863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[776/1442]: training_loss: tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[777/1442]: training_loss: tensor(0.4936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[778/1442]: training_loss: tensor(0.4225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[779/1442]: training_loss: tensor(0.4873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[780/1442]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[781/1442]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[782/1442]: training_loss: tensor(0.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[783/1442]: training_loss: tensor(0.4873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[784/1442]: training_loss: tensor(0.5044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[785/1442]: training_loss: tensor(0.5733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[786/1442]: training_loss: tensor(0.5541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[787/1442]: training_loss: tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[788/1442]: training_loss: tensor(0.8189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[789/1442]: training_loss: tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[790/1442]: training_loss: tensor(0.4927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[791/1442]: training_loss: tensor(0.5422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[792/1442]: training_loss: tensor(0.4486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[793/1442]: training_loss: tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[794/1442]: training_loss: tensor(0.9199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[795/1442]: training_loss: tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[796/1442]: training_loss: tensor(0.7052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[797/1442]: training_loss: tensor(0.6243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[798/1442]: training_loss: tensor(0.9327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[799/1442]: training_loss: tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[800/1442]: training_loss: tensor(0.7445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[801/1442]: training_loss: tensor(0.5961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[802/1442]: training_loss: tensor(0.5402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[803/1442]: training_loss: tensor(0.8928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[804/1442]: training_loss: tensor(0.8081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[805/1442]: training_loss: tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[806/1442]: training_loss: tensor(0.4205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[807/1442]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[808/1442]: training_loss: tensor(0.8537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[809/1442]: training_loss: tensor(0.7580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[810/1442]: training_loss: tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[811/1442]: training_loss: tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[812/1442]: training_loss: tensor(0.8996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[813/1442]: training_loss: tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[814/1442]: training_loss: tensor(0.8394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[815/1442]: training_loss: tensor(0.5419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[816/1442]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[817/1442]: training_loss: tensor(0.4622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[818/1442]: training_loss: tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[819/1442]: training_loss: tensor(0.9806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[820/1442]: training_loss: tensor(0.8361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[821/1442]: training_loss: tensor(0.9440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[822/1442]: training_loss: tensor(0.4817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[823/1442]: training_loss: tensor(0.6507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[824/1442]: training_loss: tensor(0.4627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[825/1442]: training_loss: tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[826/1442]: training_loss: tensor(0.9939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[827/1442]: training_loss: tensor(0.9127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[828/1442]: training_loss: tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[829/1442]: training_loss: tensor(0.4770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[830/1442]: training_loss: tensor(0.7887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[831/1442]: training_loss: tensor(0.8504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[832/1442]: training_loss: tensor(0.8465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[833/1442]: training_loss: tensor(0.8616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[834/1442]: training_loss: tensor(0.6007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[835/1442]: training_loss: tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[836/1442]: training_loss: tensor(0.4939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[837/1442]: training_loss: tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[838/1442]: training_loss: tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[839/1442]: training_loss: tensor(0.7792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[840/1442]: training_loss: tensor(0.7146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[841/1442]: training_loss: tensor(0.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[842/1442]: training_loss: tensor(0.4983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[843/1442]: training_loss: tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[844/1442]: training_loss: tensor(0.8783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[845/1442]: training_loss: tensor(0.6207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[846/1442]: training_loss: tensor(0.4990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[847/1442]: training_loss: tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[848/1442]: training_loss: tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[849/1442]: training_loss: tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[850/1442]: training_loss: tensor(0.5410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[851/1442]: training_loss: tensor(0.4600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[852/1442]: training_loss: tensor(0.5616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[853/1442]: training_loss: tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[854/1442]: training_loss: tensor(0.5076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[855/1442]: training_loss: tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[856/1442]: training_loss: tensor(0.8254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[857/1442]: training_loss: tensor(0.4926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[858/1442]: training_loss: tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[859/1442]: training_loss: tensor(0.4455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[860/1442]: training_loss: tensor(0.5278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[861/1442]: training_loss: tensor(0.7385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[862/1442]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[863/1442]: training_loss: tensor(0.9165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[864/1442]: training_loss: tensor(0.7546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[865/1442]: training_loss: tensor(0.4188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[866/1442]: training_loss: tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[867/1442]: training_loss: tensor(0.5845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[868/1442]: training_loss: tensor(1.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[869/1442]: training_loss: tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[870/1442]: training_loss: tensor(0.9119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[871/1442]: training_loss: tensor(0.4824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[872/1442]: training_loss: tensor(0.5648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[873/1442]: training_loss: tensor(0.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[874/1442]: training_loss: tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[875/1442]: training_loss: tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[876/1442]: training_loss: tensor(1.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[877/1442]: training_loss: tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[878/1442]: training_loss: tensor(0.5248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[879/1442]: training_loss: tensor(0.3858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[880/1442]: training_loss: tensor(0.4989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[881/1442]: training_loss: tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[882/1442]: training_loss: tensor(0.8145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[883/1442]: training_loss: tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[884/1442]: training_loss: tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[885/1442]: training_loss: tensor(0.7787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[886/1442]: training_loss: tensor(0.9115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[887/1442]: training_loss: tensor(0.5721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[888/1442]: training_loss: tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[889/1442]: training_loss: tensor(0.7678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[890/1442]: training_loss: tensor(0.4371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[891/1442]: training_loss: tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[892/1442]: training_loss: tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[893/1442]: training_loss: tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[894/1442]: training_loss: tensor(0.5103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[895/1442]: training_loss: tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[896/1442]: training_loss: tensor(0.4655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[897/1442]: training_loss: tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[898/1442]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[899/1442]: training_loss: tensor(0.4307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[900/1442]: training_loss: tensor(0.4225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[901/1442]: training_loss: tensor(0.5234, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[902/1442]: training_loss: tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[903/1442]: training_loss: tensor(0.4348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[904/1442]: training_loss: tensor(0.7898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[905/1442]: training_loss: tensor(0.7519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[906/1442]: training_loss: tensor(0.4739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[907/1442]: training_loss: tensor(0.7583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[908/1442]: training_loss: tensor(0.5342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[909/1442]: training_loss: tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[910/1442]: training_loss: tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[911/1442]: training_loss: tensor(0.8902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[912/1442]: training_loss: tensor(0.6028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[913/1442]: training_loss: tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[914/1442]: training_loss: tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[915/1442]: training_loss: tensor(0.4819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[916/1442]: training_loss: tensor(0.7885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[917/1442]: training_loss: tensor(0.4340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[918/1442]: training_loss: tensor(0.8640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[919/1442]: training_loss: tensor(0.8016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[920/1442]: training_loss: tensor(0.4951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[921/1442]: training_loss: tensor(0.5087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[922/1442]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[923/1442]: training_loss: tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[924/1442]: training_loss: tensor(0.6740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[925/1442]: training_loss: tensor(0.3956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[926/1442]: training_loss: tensor(0.5256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[927/1442]: training_loss: tensor(0.6455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[928/1442]: training_loss: tensor(0.9362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[929/1442]: training_loss: tensor(0.4503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[930/1442]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[931/1442]: training_loss: tensor(0.7183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[932/1442]: training_loss: tensor(0.9570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[933/1442]: training_loss: tensor(0.6645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[934/1442]: training_loss: tensor(1.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[935/1442]: training_loss: tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[936/1442]: training_loss: tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[937/1442]: training_loss: tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[938/1442]: training_loss: tensor(0.9305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[939/1442]: training_loss: tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[940/1442]: training_loss: tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[941/1442]: training_loss: tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[942/1442]: training_loss: tensor(0.8388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[943/1442]: training_loss: tensor(0.7739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[944/1442]: training_loss: tensor(0.9265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[945/1442]: training_loss: tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[946/1442]: training_loss: tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[947/1442]: training_loss: tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[948/1442]: training_loss: tensor(0.7478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[949/1442]: training_loss: tensor(0.9370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[950/1442]: training_loss: tensor(0.4267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[951/1442]: training_loss: tensor(0.5817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[952/1442]: training_loss: tensor(0.7240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[953/1442]: training_loss: tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[954/1442]: training_loss: tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[955/1442]: training_loss: tensor(0.5087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[956/1442]: training_loss: tensor(0.5955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[957/1442]: training_loss: tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[958/1442]: training_loss: tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[959/1442]: training_loss: tensor(0.6840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[960/1442]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[961/1442]: training_loss: tensor(0.5399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[962/1442]: training_loss: tensor(0.4842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[963/1442]: training_loss: tensor(0.9017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[964/1442]: training_loss: tensor(0.8668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[965/1442]: training_loss: tensor(0.9257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[966/1442]: training_loss: tensor(0.7491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[967/1442]: training_loss: tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[968/1442]: training_loss: tensor(0.6645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[969/1442]: training_loss: tensor(0.4842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[970/1442]: training_loss: tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[971/1442]: training_loss: tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[972/1442]: training_loss: tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[973/1442]: training_loss: tensor(0.7959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[974/1442]: training_loss: tensor(0.6362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[975/1442]: training_loss: tensor(0.5379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[976/1442]: training_loss: tensor(0.5714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[977/1442]: training_loss: tensor(0.5209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[978/1442]: training_loss: tensor(0.7189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[979/1442]: training_loss: tensor(0.6229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[980/1442]: training_loss: tensor(0.5380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[981/1442]: training_loss: tensor(0.4443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[982/1442]: training_loss: tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[983/1442]: training_loss: tensor(0.5135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[984/1442]: training_loss: tensor(0.9369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[985/1442]: training_loss: tensor(0.8659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[986/1442]: training_loss: tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[987/1442]: training_loss: tensor(0.4382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[988/1442]: training_loss: tensor(0.8113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[989/1442]: training_loss: tensor(0.8716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[990/1442]: training_loss: tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[991/1442]: training_loss: tensor(0.8125, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[992/1442]: training_loss: tensor(0.7710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[993/1442]: training_loss: tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[994/1442]: training_loss: tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[995/1442]: training_loss: tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[996/1442]: training_loss: tensor(0.5648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[997/1442]: training_loss: tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[998/1442]: training_loss: tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[999/1442]: training_loss: tensor(0.7474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1000/1442]: training_loss: tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1001/1442]: training_loss: tensor(0.7753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1002/1442]: training_loss: tensor(0.4983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1003/1442]: training_loss: tensor(0.4782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1004/1442]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1005/1442]: training_loss: tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1006/1442]: training_loss: tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1007/1442]: training_loss: tensor(0.4337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1008/1442]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1009/1442]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1010/1442]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1011/1442]: training_loss: tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1012/1442]: training_loss: tensor(0.7478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1013/1442]: training_loss: tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1014/1442]: training_loss: tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1015/1442]: training_loss: tensor(0.5380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1016/1442]: training_loss: tensor(0.4151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1017/1442]: training_loss: tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1018/1442]: training_loss: tensor(0.7872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1019/1442]: training_loss: tensor(0.7082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1020/1442]: training_loss: tensor(0.5678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1021/1442]: training_loss: tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1022/1442]: training_loss: tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1023/1442]: training_loss: tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1024/1442]: training_loss: tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1025/1442]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1026/1442]: training_loss: tensor(1.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1027/1442]: training_loss: tensor(0.7102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1028/1442]: training_loss: tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1029/1442]: training_loss: tensor(0.5690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1030/1442]: training_loss: tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1031/1442]: training_loss: tensor(0.6373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1032/1442]: training_loss: tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1033/1442]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1034/1442]: training_loss: tensor(0.8268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1035/1442]: training_loss: tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1036/1442]: training_loss: tensor(0.4980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1037/1442]: training_loss: tensor(0.8398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1038/1442]: training_loss: tensor(0.7853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1039/1442]: training_loss: tensor(0.8284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1040/1442]: training_loss: tensor(0.4877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1041/1442]: training_loss: tensor(0.7718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1042/1442]: training_loss: tensor(0.7235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1043/1442]: training_loss: tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1044/1442]: training_loss: tensor(0.7432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1045/1442]: training_loss: tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1046/1442]: training_loss: tensor(0.5439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1047/1442]: training_loss: tensor(0.9292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1048/1442]: training_loss: tensor(0.7483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1049/1442]: training_loss: tensor(0.5055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1050/1442]: training_loss: tensor(0.5435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1051/1442]: training_loss: tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1052/1442]: training_loss: tensor(0.5059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1053/1442]: training_loss: tensor(0.9444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1054/1442]: training_loss: tensor(0.5677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1055/1442]: training_loss: tensor(0.5949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1056/1442]: training_loss: tensor(0.5199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1057/1442]: training_loss: tensor(0.8658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1058/1442]: training_loss: tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1059/1442]: training_loss: tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1060/1442]: training_loss: tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1061/1442]: training_loss: tensor(0.5218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1062/1442]: training_loss: tensor(0.4581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1063/1442]: training_loss: tensor(0.4157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1064/1442]: training_loss: tensor(0.5576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1065/1442]: training_loss: tensor(0.5814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1066/1442]: training_loss: tensor(0.8065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1067/1442]: training_loss: tensor(0.8753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1068/1442]: training_loss: tensor(0.5320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1069/1442]: training_loss: tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1070/1442]: training_loss: tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1071/1442]: training_loss: tensor(0.4926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1072/1442]: training_loss: tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1073/1442]: training_loss: tensor(0.4921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1074/1442]: training_loss: tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1075/1442]: training_loss: tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1076/1442]: training_loss: tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1077/1442]: training_loss: tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1078/1442]: training_loss: tensor(0.4798, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1079/1442]: training_loss: tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1080/1442]: training_loss: tensor(0.5190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1081/1442]: training_loss: tensor(0.9261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1082/1442]: training_loss: tensor(0.8247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1083/1442]: training_loss: tensor(0.8087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1084/1442]: training_loss: tensor(0.5196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1085/1442]: training_loss: tensor(0.8642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1086/1442]: training_loss: tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1087/1442]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1088/1442]: training_loss: tensor(0.7166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1089/1442]: training_loss: tensor(0.6341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1090/1442]: training_loss: tensor(0.4574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1091/1442]: training_loss: tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1092/1442]: training_loss: tensor(0.5444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1093/1442]: training_loss: tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1094/1442]: training_loss: tensor(0.6168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1095/1442]: training_loss: tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1096/1442]: training_loss: tensor(0.6484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1097/1442]: training_loss: tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1098/1442]: training_loss: tensor(0.4096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1099/1442]: training_loss: tensor(0.4805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1100/1442]: training_loss: tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1101/1442]: training_loss: tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1102/1442]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1103/1442]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1104/1442]: training_loss: tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1105/1442]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1106/1442]: training_loss: tensor(0.7939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1107/1442]: training_loss: tensor(0.8588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1108/1442]: training_loss: tensor(0.5116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1109/1442]: training_loss: tensor(0.4414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1110/1442]: training_loss: tensor(0.5453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1111/1442]: training_loss: tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1112/1442]: training_loss: tensor(0.7607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1113/1442]: training_loss: tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1114/1442]: training_loss: tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1115/1442]: training_loss: tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1116/1442]: training_loss: tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1117/1442]: training_loss: tensor(0.9306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1118/1442]: training_loss: tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1119/1442]: training_loss: tensor(0.5805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1120/1442]: training_loss: tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1121/1442]: training_loss: tensor(0.6229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1122/1442]: training_loss: tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1123/1442]: training_loss: tensor(0.6290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1124/1442]: training_loss: tensor(0.4838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1125/1442]: training_loss: tensor(0.7506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1126/1442]: training_loss: tensor(0.8716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1127/1442]: training_loss: tensor(0.9358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1128/1442]: training_loss: tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1129/1442]: training_loss: tensor(0.6728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1130/1442]: training_loss: tensor(0.4524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1131/1442]: training_loss: tensor(0.8438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1132/1442]: training_loss: tensor(0.8047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1133/1442]: training_loss: tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1134/1442]: training_loss: tensor(0.5366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1135/1442]: training_loss: tensor(0.5316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1136/1442]: training_loss: tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1137/1442]: training_loss: tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1138/1442]: training_loss: tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1139/1442]: training_loss: tensor(0.7790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1140/1442]: training_loss: tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1141/1442]: training_loss: tensor(0.5375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1142/1442]: training_loss: tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1143/1442]: training_loss: tensor(0.5812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1144/1442]: training_loss: tensor(0.9808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1145/1442]: training_loss: tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1146/1442]: training_loss: tensor(0.7276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1147/1442]: training_loss: tensor(0.6643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1148/1442]: training_loss: tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1149/1442]: training_loss: tensor(0.5804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1150/1442]: training_loss: tensor(0.6002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1151/1442]: training_loss: tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1152/1442]: training_loss: tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1153/1442]: training_loss: tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1154/1442]: training_loss: tensor(0.8300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1155/1442]: training_loss: tensor(0.5682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1156/1442]: training_loss: tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1157/1442]: training_loss: tensor(0.7281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1158/1442]: training_loss: tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1159/1442]: training_loss: tensor(0.5246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1160/1442]: training_loss: tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1161/1442]: training_loss: tensor(0.4420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1162/1442]: training_loss: tensor(0.7748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1163/1442]: training_loss: tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1164/1442]: training_loss: tensor(0.8557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1165/1442]: training_loss: tensor(0.7739, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1166/1442]: training_loss: tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1167/1442]: training_loss: tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1168/1442]: training_loss: tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1169/1442]: training_loss: tensor(1.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1170/1442]: training_loss: tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1171/1442]: training_loss: tensor(0.5508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1172/1442]: training_loss: tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1173/1442]: training_loss: tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1174/1442]: training_loss: tensor(0.5596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1175/1442]: training_loss: tensor(0.7444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1176/1442]: training_loss: tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1177/1442]: training_loss: tensor(0.5982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1178/1442]: training_loss: tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1179/1442]: training_loss: tensor(0.8353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1180/1442]: training_loss: tensor(0.7883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1181/1442]: training_loss: tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1182/1442]: training_loss: tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1183/1442]: training_loss: tensor(0.7701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1184/1442]: training_loss: tensor(0.8713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1185/1442]: training_loss: tensor(0.4819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1186/1442]: training_loss: tensor(0.4643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1187/1442]: training_loss: tensor(0.5817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1188/1442]: training_loss: tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1189/1442]: training_loss: tensor(0.5169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1190/1442]: training_loss: tensor(0.4647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1191/1442]: training_loss: tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1192/1442]: training_loss: tensor(0.7616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1193/1442]: training_loss: tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1194/1442]: training_loss: tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1195/1442]: training_loss: tensor(0.7250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1196/1442]: training_loss: tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1197/1442]: training_loss: tensor(0.8834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1198/1442]: training_loss: tensor(0.7278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1199/1442]: training_loss: tensor(0.5305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1200/1442]: training_loss: tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1201/1442]: training_loss: tensor(0.5336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1202/1442]: training_loss: tensor(0.6341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1203/1442]: training_loss: tensor(0.6353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1204/1442]: training_loss: tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1205/1442]: training_loss: tensor(0.7704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1206/1442]: training_loss: tensor(0.7962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1207/1442]: training_loss: tensor(0.8309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1208/1442]: training_loss: tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1209/1442]: training_loss: tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1210/1442]: training_loss: tensor(0.8434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1211/1442]: training_loss: tensor(0.5381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1212/1442]: training_loss: tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1213/1442]: training_loss: tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1214/1442]: training_loss: tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1215/1442]: training_loss: tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1216/1442]: training_loss: tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1217/1442]: training_loss: tensor(0.4959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1218/1442]: training_loss: tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1219/1442]: training_loss: tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1220/1442]: training_loss: tensor(0.6998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1221/1442]: training_loss: tensor(0.7848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1222/1442]: training_loss: tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1223/1442]: training_loss: tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1224/1442]: training_loss: tensor(0.5143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1225/1442]: training_loss: tensor(0.5197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1226/1442]: training_loss: tensor(0.6032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1227/1442]: training_loss: tensor(0.6423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1228/1442]: training_loss: tensor(0.6389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1229/1442]: training_loss: tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1230/1442]: training_loss: tensor(0.4994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1231/1442]: training_loss: tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1232/1442]: training_loss: tensor(0.5382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1233/1442]: training_loss: tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1234/1442]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1235/1442]: training_loss: tensor(0.6507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1236/1442]: training_loss: tensor(0.4983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1237/1442]: training_loss: tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1238/1442]: training_loss: tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1239/1442]: training_loss: tensor(0.6107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1240/1442]: training_loss: tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1241/1442]: training_loss: tensor(0.7513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1242/1442]: training_loss: tensor(0.4754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1243/1442]: training_loss: tensor(0.9160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1244/1442]: training_loss: tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1245/1442]: training_loss: tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1246/1442]: training_loss: tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1247/1442]: training_loss: tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1248/1442]: training_loss: tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1249/1442]: training_loss: tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1250/1442]: training_loss: tensor(0.8340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1251/1442]: training_loss: tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1252/1442]: training_loss: tensor(0.8186, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1253/1442]: training_loss: tensor(0.5429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1254/1442]: training_loss: tensor(0.6737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1255/1442]: training_loss: tensor(0.7642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1256/1442]: training_loss: tensor(0.4208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1257/1442]: training_loss: tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1258/1442]: training_loss: tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1259/1442]: training_loss: tensor(0.5595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1260/1442]: training_loss: tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1261/1442]: training_loss: tensor(0.6202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1262/1442]: training_loss: tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1263/1442]: training_loss: tensor(0.8174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1264/1442]: training_loss: tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1265/1442]: training_loss: tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1266/1442]: training_loss: tensor(0.5313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1267/1442]: training_loss: tensor(0.8825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1268/1442]: training_loss: tensor(0.5936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1269/1442]: training_loss: tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1270/1442]: training_loss: tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1271/1442]: training_loss: tensor(0.9052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1272/1442]: training_loss: tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1273/1442]: training_loss: tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1274/1442]: training_loss: tensor(0.7970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1275/1442]: training_loss: tensor(0.4642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1276/1442]: training_loss: tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1277/1442]: training_loss: tensor(0.5711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1278/1442]: training_loss: tensor(0.5858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1279/1442]: training_loss: tensor(0.5091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1280/1442]: training_loss: tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1281/1442]: training_loss: tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1282/1442]: training_loss: tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1283/1442]: training_loss: tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1284/1442]: training_loss: tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1285/1442]: training_loss: tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1286/1442]: training_loss: tensor(0.6523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1287/1442]: training_loss: tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1288/1442]: training_loss: tensor(0.7578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1289/1442]: training_loss: tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1290/1442]: training_loss: tensor(0.6560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1291/1442]: training_loss: tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1292/1442]: training_loss: tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1293/1442]: training_loss: tensor(0.5774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1294/1442]: training_loss: tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1295/1442]: training_loss: tensor(0.4658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1296/1442]: training_loss: tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1297/1442]: training_loss: tensor(0.7671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1298/1442]: training_loss: tensor(0.8148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1299/1442]: training_loss: tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1300/1442]: training_loss: tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1301/1442]: training_loss: tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1302/1442]: training_loss: tensor(0.7313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1303/1442]: training_loss: tensor(0.8040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1304/1442]: training_loss: tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1305/1442]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1306/1442]: training_loss: tensor(0.5847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1307/1442]: training_loss: tensor(0.5055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1308/1442]: training_loss: tensor(0.6392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1309/1442]: training_loss: tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1310/1442]: training_loss: tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1311/1442]: training_loss: tensor(0.8645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1312/1442]: training_loss: tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1313/1442]: training_loss: tensor(0.6378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1314/1442]: training_loss: tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1315/1442]: training_loss: tensor(0.7588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1316/1442]: training_loss: tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1317/1442]: training_loss: tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1318/1442]: training_loss: tensor(0.5313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1319/1442]: training_loss: tensor(0.7471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1320/1442]: training_loss: tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1321/1442]: training_loss: tensor(0.6189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1322/1442]: training_loss: tensor(0.7146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1323/1442]: training_loss: tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1324/1442]: training_loss: tensor(0.6278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1325/1442]: training_loss: tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1326/1442]: training_loss: tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1327/1442]: training_loss: tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1328/1442]: training_loss: tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1329/1442]: training_loss: tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1330/1442]: training_loss: tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1331/1442]: training_loss: tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1332/1442]: training_loss: tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1333/1442]: training_loss: tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1334/1442]: training_loss: tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1335/1442]: training_loss: tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1336/1442]: training_loss: tensor(0.5599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1337/1442]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1338/1442]: training_loss: tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1339/1442]: training_loss: tensor(0.6364, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1340/1442]: training_loss: tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1341/1442]: training_loss: tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1342/1442]: training_loss: tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1343/1442]: training_loss: tensor(0.7904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1344/1442]: training_loss: tensor(0.6468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1345/1442]: training_loss: tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1346/1442]: training_loss: tensor(0.7616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1347/1442]: training_loss: tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1348/1442]: training_loss: tensor(0.8067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1349/1442]: training_loss: tensor(0.4888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1350/1442]: training_loss: tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1351/1442]: training_loss: tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1352/1442]: training_loss: tensor(0.7323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1353/1442]: training_loss: tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1354/1442]: training_loss: tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1355/1442]: training_loss: tensor(0.5879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1356/1442]: training_loss: tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1357/1442]: training_loss: tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1358/1442]: training_loss: tensor(0.6504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1359/1442]: training_loss: tensor(0.6182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1360/1442]: training_loss: tensor(0.5596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1361/1442]: training_loss: tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1362/1442]: training_loss: tensor(0.7689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1363/1442]: training_loss: tensor(0.7634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1364/1442]: training_loss: tensor(0.5579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1365/1442]: training_loss: tensor(0.4670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1366/1442]: training_loss: tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1367/1442]: training_loss: tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1368/1442]: training_loss: tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1369/1442]: training_loss: tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1370/1442]: training_loss: tensor(0.4875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1371/1442]: training_loss: tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1372/1442]: training_loss: tensor(0.6319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1373/1442]: training_loss: tensor(0.8166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1374/1442]: training_loss: tensor(0.7146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1375/1442]: training_loss: tensor(0.4647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1376/1442]: training_loss: tensor(0.6111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1377/1442]: training_loss: tensor(0.7688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1378/1442]: training_loss: tensor(0.6481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1379/1442]: training_loss: tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1380/1442]: training_loss: tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1381/1442]: training_loss: tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1382/1442]: training_loss: tensor(0.7455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1383/1442]: training_loss: tensor(0.5693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1384/1442]: training_loss: tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1385/1442]: training_loss: tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1386/1442]: training_loss: tensor(0.8359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1387/1442]: training_loss: tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1388/1442]: training_loss: tensor(0.7435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1389/1442]: training_loss: tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1390/1442]: training_loss: tensor(0.5500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1391/1442]: training_loss: tensor(0.5845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1392/1442]: training_loss: tensor(0.5496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1393/1442]: training_loss: tensor(0.6552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1394/1442]: training_loss: tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1395/1442]: training_loss: tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1396/1442]: training_loss: tensor(0.6019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1397/1442]: training_loss: tensor(0.4482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1398/1442]: training_loss: tensor(0.5317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1399/1442]: training_loss: tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1400/1442]: training_loss: tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1401/1442]: training_loss: tensor(0.7650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1402/1442]: training_loss: tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1403/1442]: training_loss: tensor(0.6409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1404/1442]: training_loss: tensor(0.5342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1405/1442]: training_loss: tensor(0.5847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1406/1442]: training_loss: tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1407/1442]: training_loss: tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1408/1442]: training_loss: tensor(0.5167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1409/1442]: training_loss: tensor(0.8333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1410/1442]: training_loss: tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1411/1442]: training_loss: tensor(0.6149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1412/1442]: training_loss: tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1413/1442]: training_loss: tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1414/1442]: training_loss: tensor(0.7384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1415/1442]: training_loss: tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1416/1442]: training_loss: tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1417/1442]: training_loss: tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1418/1442]: training_loss: tensor(0.4383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1419/1442]: training_loss: tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1420/1442]: training_loss: tensor(0.6615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1421/1442]: training_loss: tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1422/1442]: training_loss: tensor(0.5728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1423/1442]: training_loss: tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1424/1442]: training_loss: tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1425/1442]: training_loss: tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1426/1442]: training_loss: tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1427/1442]: training_loss: tensor(0.7267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1428/1442]: training_loss: tensor(0.5806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1429/1442]: training_loss: tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1430/1442]: training_loss: tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1431/1442]: training_loss: tensor(0.5680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1432/1442]: training_loss: tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1433/1442]: training_loss: tensor(0.7443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1434/1442]: training_loss: tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1435/1442]: training_loss: tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1436/1442]: training_loss: tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1437/1442]: training_loss: tensor(0.6449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1438/1442]: training_loss: tensor(0.6278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1439/1442]: training_loss: tensor(0.7317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1440/1442]: training_loss: tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1441/1442]: training_loss: tensor(0.9287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/5],global step [1442/7210], pt loss:0.0000, dev loss:0.6027\n",
      "batch_no[0/1442]: training_loss: tensor(0.5058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1/1442]: training_loss: tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[2/1442]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[3/1442]: training_loss: tensor(0.7388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[4/1442]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[5/1442]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[6/1442]: training_loss: tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[7/1442]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[8/1442]: training_loss: tensor(0.5868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[9/1442]: training_loss: tensor(0.4846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[10/1442]: training_loss: tensor(0.5868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[11/1442]: training_loss: tensor(0.4047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[12/1442]: training_loss: tensor(0.3622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[13/1442]: training_loss: tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[14/1442]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[15/1442]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[16/1442]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[17/1442]: training_loss: tensor(0.4721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[18/1442]: training_loss: tensor(0.5893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[19/1442]: training_loss: tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[20/1442]: training_loss: tensor(0.4131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[21/1442]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[22/1442]: training_loss: tensor(0.4855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[23/1442]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[24/1442]: training_loss: tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[25/1442]: training_loss: tensor(0.5442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[26/1442]: training_loss: tensor(0.6553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[27/1442]: training_loss: tensor(0.4666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[28/1442]: training_loss: tensor(0.5110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[29/1442]: training_loss: tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[30/1442]: training_loss: tensor(0.7270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[31/1442]: training_loss: tensor(0.6086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[32/1442]: training_loss: tensor(0.5868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[33/1442]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[34/1442]: training_loss: tensor(0.4587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[35/1442]: training_loss: tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[36/1442]: training_loss: tensor(0.7181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[37/1442]: training_loss: tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[38/1442]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[39/1442]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[40/1442]: training_loss: tensor(0.4169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[41/1442]: training_loss: tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[42/1442]: training_loss: tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[43/1442]: training_loss: tensor(0.4857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[44/1442]: training_loss: tensor(0.7946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[45/1442]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[46/1442]: training_loss: tensor(0.5951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[47/1442]: training_loss: tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[48/1442]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[49/1442]: training_loss: tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[50/1442]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[51/1442]: training_loss: tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[52/1442]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[53/1442]: training_loss: tensor(0.5223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[54/1442]: training_loss: tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[55/1442]: training_loss: tensor(0.5816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[56/1442]: training_loss: tensor(0.9113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[57/1442]: training_loss: tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[58/1442]: training_loss: tensor(0.7582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[59/1442]: training_loss: tensor(0.3538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[60/1442]: training_loss: tensor(0.4824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[61/1442]: training_loss: tensor(0.6138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[62/1442]: training_loss: tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[63/1442]: training_loss: tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[64/1442]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[65/1442]: training_loss: tensor(0.4682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[66/1442]: training_loss: tensor(0.3597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[67/1442]: training_loss: tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[68/1442]: training_loss: tensor(0.5308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[69/1442]: training_loss: tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[70/1442]: training_loss: tensor(0.4484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[71/1442]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[72/1442]: training_loss: tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[73/1442]: training_loss: tensor(0.4117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[74/1442]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[75/1442]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[76/1442]: training_loss: tensor(0.7178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[77/1442]: training_loss: tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[78/1442]: training_loss: tensor(0.4093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[79/1442]: training_loss: tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[80/1442]: training_loss: tensor(0.5062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[81/1442]: training_loss: tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[82/1442]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[83/1442]: training_loss: tensor(0.4548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[84/1442]: training_loss: tensor(0.4596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[85/1442]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[86/1442]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[87/1442]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[88/1442]: training_loss: tensor(0.8798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[89/1442]: training_loss: tensor(0.4342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[90/1442]: training_loss: tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[91/1442]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[92/1442]: training_loss: tensor(0.4749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[93/1442]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[94/1442]: training_loss: tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[95/1442]: training_loss: tensor(0.8089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[96/1442]: training_loss: tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[97/1442]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[98/1442]: training_loss: tensor(0.5081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[99/1442]: training_loss: tensor(0.8415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[100/1442]: training_loss: tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[101/1442]: training_loss: tensor(0.9385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[102/1442]: training_loss: tensor(0.4496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[103/1442]: training_loss: tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[104/1442]: training_loss: tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[105/1442]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[106/1442]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[107/1442]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[108/1442]: training_loss: tensor(0.9567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[109/1442]: training_loss: tensor(0.4871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[110/1442]: training_loss: tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[111/1442]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[112/1442]: training_loss: tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[113/1442]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[114/1442]: training_loss: tensor(0.7178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[115/1442]: training_loss: tensor(0.5492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[116/1442]: training_loss: tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[117/1442]: training_loss: tensor(0.4839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[118/1442]: training_loss: tensor(0.4866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[119/1442]: training_loss: tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[120/1442]: training_loss: tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[121/1442]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[122/1442]: training_loss: tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[123/1442]: training_loss: tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[124/1442]: training_loss: tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[125/1442]: training_loss: tensor(0.4743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[126/1442]: training_loss: tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[127/1442]: training_loss: tensor(0.7463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[128/1442]: training_loss: tensor(0.8813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[129/1442]: training_loss: tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[130/1442]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[131/1442]: training_loss: tensor(0.4615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[132/1442]: training_loss: tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[133/1442]: training_loss: tensor(0.4663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[134/1442]: training_loss: tensor(0.7256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[135/1442]: training_loss: tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[136/1442]: training_loss: tensor(0.8740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[137/1442]: training_loss: tensor(0.5244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[138/1442]: training_loss: tensor(0.7388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[139/1442]: training_loss: tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[140/1442]: training_loss: tensor(0.4057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[141/1442]: training_loss: tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[142/1442]: training_loss: tensor(0.4610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[143/1442]: training_loss: tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[144/1442]: training_loss: tensor(0.7600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[145/1442]: training_loss: tensor(0.4594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[146/1442]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[147/1442]: training_loss: tensor(0.5952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[148/1442]: training_loss: tensor(0.6609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[149/1442]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[150/1442]: training_loss: tensor(0.4724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[151/1442]: training_loss: tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[152/1442]: training_loss: tensor(0.7216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[153/1442]: training_loss: tensor(0.4373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[154/1442]: training_loss: tensor(0.4733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[155/1442]: training_loss: tensor(0.5768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[156/1442]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[157/1442]: training_loss: tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[158/1442]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[159/1442]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[160/1442]: training_loss: tensor(0.3642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[161/1442]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[162/1442]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[163/1442]: training_loss: tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[164/1442]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[165/1442]: training_loss: tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[166/1442]: training_loss: tensor(0.5577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[167/1442]: training_loss: tensor(0.7580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[168/1442]: training_loss: tensor(0.5363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[169/1442]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[170/1442]: training_loss: tensor(0.4480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[171/1442]: training_loss: tensor(0.6362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[172/1442]: training_loss: tensor(0.5632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[173/1442]: training_loss: tensor(0.7102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[174/1442]: training_loss: tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[175/1442]: training_loss: tensor(0.4418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[176/1442]: training_loss: tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[177/1442]: training_loss: tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[178/1442]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[179/1442]: training_loss: tensor(0.7379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[180/1442]: training_loss: tensor(0.8792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[181/1442]: training_loss: tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[182/1442]: training_loss: tensor(0.5995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[183/1442]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[184/1442]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[185/1442]: training_loss: tensor(0.7755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[186/1442]: training_loss: tensor(0.4156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[187/1442]: training_loss: tensor(0.7990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[188/1442]: training_loss: tensor(0.9262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[189/1442]: training_loss: tensor(0.9346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[190/1442]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[191/1442]: training_loss: tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[192/1442]: training_loss: tensor(0.7618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[193/1442]: training_loss: tensor(0.4788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[194/1442]: training_loss: tensor(0.4065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[195/1442]: training_loss: tensor(0.4737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[196/1442]: training_loss: tensor(0.5590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[197/1442]: training_loss: tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[198/1442]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[199/1442]: training_loss: tensor(0.4529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[200/1442]: training_loss: tensor(0.5169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[201/1442]: training_loss: tensor(0.5031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[202/1442]: training_loss: tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[203/1442]: training_loss: tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[204/1442]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[205/1442]: training_loss: tensor(0.5069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[206/1442]: training_loss: tensor(0.8884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[207/1442]: training_loss: tensor(0.5432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[208/1442]: training_loss: tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[209/1442]: training_loss: tensor(0.4653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[210/1442]: training_loss: tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[211/1442]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[212/1442]: training_loss: tensor(0.4517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[213/1442]: training_loss: tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[214/1442]: training_loss: tensor(0.7210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[215/1442]: training_loss: tensor(0.7092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[216/1442]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[217/1442]: training_loss: tensor(0.5443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[218/1442]: training_loss: tensor(0.4593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[219/1442]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[220/1442]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[221/1442]: training_loss: tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[222/1442]: training_loss: tensor(0.4256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[223/1442]: training_loss: tensor(0.6392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[224/1442]: training_loss: tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[225/1442]: training_loss: tensor(0.8441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[226/1442]: training_loss: tensor(0.6466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[227/1442]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[228/1442]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[229/1442]: training_loss: tensor(0.5996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[230/1442]: training_loss: tensor(0.4693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[231/1442]: training_loss: tensor(0.4610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[232/1442]: training_loss: tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[233/1442]: training_loss: tensor(0.5745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[234/1442]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[235/1442]: training_loss: tensor(0.6041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[236/1442]: training_loss: tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[237/1442]: training_loss: tensor(0.6699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[238/1442]: training_loss: tensor(0.6319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[239/1442]: training_loss: tensor(0.4691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[240/1442]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[241/1442]: training_loss: tensor(0.6493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[242/1442]: training_loss: tensor(0.7780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[243/1442]: training_loss: tensor(0.6487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[244/1442]: training_loss: tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[245/1442]: training_loss: tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[246/1442]: training_loss: tensor(0.5777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[247/1442]: training_loss: tensor(0.4847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[248/1442]: training_loss: tensor(0.6179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[249/1442]: training_loss: tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[250/1442]: training_loss: tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[251/1442]: training_loss: tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[252/1442]: training_loss: tensor(0.6794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[253/1442]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[254/1442]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[255/1442]: training_loss: tensor(0.5661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[256/1442]: training_loss: tensor(0.4793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[257/1442]: training_loss: tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[258/1442]: training_loss: tensor(0.4664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[259/1442]: training_loss: tensor(0.5479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[260/1442]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[261/1442]: training_loss: tensor(0.4394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[262/1442]: training_loss: tensor(0.5370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[263/1442]: training_loss: tensor(0.2951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[264/1442]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[265/1442]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[266/1442]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[267/1442]: training_loss: tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[268/1442]: training_loss: tensor(0.6065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[269/1442]: training_loss: tensor(0.4511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[270/1442]: training_loss: tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[271/1442]: training_loss: tensor(0.3603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[272/1442]: training_loss: tensor(0.5080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[273/1442]: training_loss: tensor(0.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[274/1442]: training_loss: tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[275/1442]: training_loss: tensor(0.4354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[276/1442]: training_loss: tensor(0.5266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[277/1442]: training_loss: tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[278/1442]: training_loss: tensor(0.5171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[279/1442]: training_loss: tensor(0.4177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[280/1442]: training_loss: tensor(0.4241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[281/1442]: training_loss: tensor(0.4392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[282/1442]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[283/1442]: training_loss: tensor(0.9027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[284/1442]: training_loss: tensor(0.8605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[285/1442]: training_loss: tensor(0.5973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[286/1442]: training_loss: tensor(0.4262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[287/1442]: training_loss: tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[288/1442]: training_loss: tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[289/1442]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[290/1442]: training_loss: tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[291/1442]: training_loss: tensor(0.6660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[292/1442]: training_loss: tensor(0.8791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[293/1442]: training_loss: tensor(0.4492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[294/1442]: training_loss: tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[295/1442]: training_loss: tensor(0.4184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[296/1442]: training_loss: tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[297/1442]: training_loss: tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[298/1442]: training_loss: tensor(0.7949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[299/1442]: training_loss: tensor(0.4455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[300/1442]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[301/1442]: training_loss: tensor(0.7063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[302/1442]: training_loss: tensor(0.4429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[303/1442]: training_loss: tensor(0.4297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[304/1442]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[305/1442]: training_loss: tensor(0.6667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[306/1442]: training_loss: tensor(0.7717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[307/1442]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[308/1442]: training_loss: tensor(0.4114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[309/1442]: training_loss: tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[310/1442]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[311/1442]: training_loss: tensor(0.4936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[312/1442]: training_loss: tensor(0.4849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[313/1442]: training_loss: tensor(0.5473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[314/1442]: training_loss: tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[315/1442]: training_loss: tensor(0.5738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[316/1442]: training_loss: tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[317/1442]: training_loss: tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[318/1442]: training_loss: tensor(0.4210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[319/1442]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[320/1442]: training_loss: tensor(0.4946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[321/1442]: training_loss: tensor(0.5338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[322/1442]: training_loss: tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[323/1442]: training_loss: tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[324/1442]: training_loss: tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[325/1442]: training_loss: tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[326/1442]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[327/1442]: training_loss: tensor(0.4348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[328/1442]: training_loss: tensor(0.5732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[329/1442]: training_loss: tensor(0.4271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[330/1442]: training_loss: tensor(0.7066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[331/1442]: training_loss: tensor(0.4677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[332/1442]: training_loss: tensor(0.4464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[333/1442]: training_loss: tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[334/1442]: training_loss: tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[335/1442]: training_loss: tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[336/1442]: training_loss: tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[337/1442]: training_loss: tensor(0.6003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[338/1442]: training_loss: tensor(0.3585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[339/1442]: training_loss: tensor(0.4744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[340/1442]: training_loss: tensor(0.5678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[341/1442]: training_loss: tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[342/1442]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[343/1442]: training_loss: tensor(0.4489, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[344/1442]: training_loss: tensor(0.5070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[345/1442]: training_loss: tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[346/1442]: training_loss: tensor(0.9346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[347/1442]: training_loss: tensor(0.5402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[348/1442]: training_loss: tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[349/1442]: training_loss: tensor(0.4463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[350/1442]: training_loss: tensor(0.4541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[351/1442]: training_loss: tensor(0.4500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[352/1442]: training_loss: tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[353/1442]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[354/1442]: training_loss: tensor(0.6784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[355/1442]: training_loss: tensor(0.5155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[356/1442]: training_loss: tensor(0.7384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[357/1442]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[358/1442]: training_loss: tensor(0.4431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[359/1442]: training_loss: tensor(0.2979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[360/1442]: training_loss: tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[361/1442]: training_loss: tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[362/1442]: training_loss: tensor(0.5255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[363/1442]: training_loss: tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[364/1442]: training_loss: tensor(0.7590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[365/1442]: training_loss: tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[366/1442]: training_loss: tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[367/1442]: training_loss: tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[368/1442]: training_loss: tensor(0.4603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[369/1442]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[370/1442]: training_loss: tensor(0.3286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[371/1442]: training_loss: tensor(0.4822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[372/1442]: training_loss: tensor(0.5059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[373/1442]: training_loss: tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[374/1442]: training_loss: tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[375/1442]: training_loss: tensor(0.4325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[376/1442]: training_loss: tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[377/1442]: training_loss: tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[378/1442]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[379/1442]: training_loss: tensor(0.4110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[380/1442]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[381/1442]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[382/1442]: training_loss: tensor(0.4131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[383/1442]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[384/1442]: training_loss: tensor(0.4562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[385/1442]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[386/1442]: training_loss: tensor(0.4776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[387/1442]: training_loss: tensor(0.7870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[388/1442]: training_loss: tensor(0.4611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[389/1442]: training_loss: tensor(0.4154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[390/1442]: training_loss: tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[391/1442]: training_loss: tensor(0.8061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[392/1442]: training_loss: tensor(0.7949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[393/1442]: training_loss: tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[394/1442]: training_loss: tensor(0.7205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[395/1442]: training_loss: tensor(0.4568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[396/1442]: training_loss: tensor(0.5015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[397/1442]: training_loss: tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[398/1442]: training_loss: tensor(0.4173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[399/1442]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[400/1442]: training_loss: tensor(0.4814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[401/1442]: training_loss: tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[402/1442]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[403/1442]: training_loss: tensor(0.5387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[404/1442]: training_loss: tensor(0.4506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[405/1442]: training_loss: tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[406/1442]: training_loss: tensor(0.7554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[407/1442]: training_loss: tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[408/1442]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[409/1442]: training_loss: tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[410/1442]: training_loss: tensor(0.4088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[411/1442]: training_loss: tensor(0.4997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[412/1442]: training_loss: tensor(0.5578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[413/1442]: training_loss: tensor(0.8321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[414/1442]: training_loss: tensor(0.4274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[415/1442]: training_loss: tensor(0.7427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[416/1442]: training_loss: tensor(0.4042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[417/1442]: training_loss: tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[418/1442]: training_loss: tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[419/1442]: training_loss: tensor(0.5987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[420/1442]: training_loss: tensor(0.4415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[421/1442]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[422/1442]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[423/1442]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[424/1442]: training_loss: tensor(0.6050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[425/1442]: training_loss: tensor(0.8404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[426/1442]: training_loss: tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[427/1442]: training_loss: tensor(0.4942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[428/1442]: training_loss: tensor(0.4560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[429/1442]: training_loss: tensor(0.4561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[430/1442]: training_loss: tensor(0.4959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[431/1442]: training_loss: tensor(0.7594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[432/1442]: training_loss: tensor(0.5931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[433/1442]: training_loss: tensor(0.3555, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[434/1442]: training_loss: tensor(0.5380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[435/1442]: training_loss: tensor(0.3659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[436/1442]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[437/1442]: training_loss: tensor(0.5888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[438/1442]: training_loss: tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[439/1442]: training_loss: tensor(0.8301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[440/1442]: training_loss: tensor(0.5858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[441/1442]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[442/1442]: training_loss: tensor(0.4520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[443/1442]: training_loss: tensor(0.5979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[444/1442]: training_loss: tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[445/1442]: training_loss: tensor(0.4508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[446/1442]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[447/1442]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[448/1442]: training_loss: tensor(0.4484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[449/1442]: training_loss: tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[450/1442]: training_loss: tensor(0.4248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[451/1442]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[452/1442]: training_loss: tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[453/1442]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[454/1442]: training_loss: tensor(0.6310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[455/1442]: training_loss: tensor(0.7727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[456/1442]: training_loss: tensor(0.5845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[457/1442]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[458/1442]: training_loss: tensor(0.7876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[459/1442]: training_loss: tensor(0.4460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[460/1442]: training_loss: tensor(0.4699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[461/1442]: training_loss: tensor(0.5060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[462/1442]: training_loss: tensor(0.4984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[463/1442]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[464/1442]: training_loss: tensor(0.5175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[465/1442]: training_loss: tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[466/1442]: training_loss: tensor(0.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[467/1442]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[468/1442]: training_loss: tensor(0.3156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[469/1442]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[470/1442]: training_loss: tensor(0.4474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[471/1442]: training_loss: tensor(0.4445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[472/1442]: training_loss: tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[473/1442]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[474/1442]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[475/1442]: training_loss: tensor(0.4596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[476/1442]: training_loss: tensor(0.3156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[477/1442]: training_loss: tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[478/1442]: training_loss: tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[479/1442]: training_loss: tensor(0.8050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[480/1442]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[481/1442]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[482/1442]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[483/1442]: training_loss: tensor(0.5109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[484/1442]: training_loss: tensor(0.4726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[485/1442]: training_loss: tensor(0.5744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[486/1442]: training_loss: tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[487/1442]: training_loss: tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[488/1442]: training_loss: tensor(0.6200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[489/1442]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[490/1442]: training_loss: tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[491/1442]: training_loss: tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[492/1442]: training_loss: tensor(0.5320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[493/1442]: training_loss: tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[494/1442]: training_loss: tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[495/1442]: training_loss: tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[496/1442]: training_loss: tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[497/1442]: training_loss: tensor(0.4822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[498/1442]: training_loss: tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[499/1442]: training_loss: tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[500/1442]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[501/1442]: training_loss: tensor(0.3114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[502/1442]: training_loss: tensor(0.4634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[503/1442]: training_loss: tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[504/1442]: training_loss: tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[505/1442]: training_loss: tensor(0.4672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[506/1442]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[507/1442]: training_loss: tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[508/1442]: training_loss: tensor(0.7462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[509/1442]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[510/1442]: training_loss: tensor(0.3525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[511/1442]: training_loss: tensor(0.4553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[512/1442]: training_loss: tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[513/1442]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[514/1442]: training_loss: tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[515/1442]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[516/1442]: training_loss: tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[517/1442]: training_loss: tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[518/1442]: training_loss: tensor(0.5476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[519/1442]: training_loss: tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[520/1442]: training_loss: tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[521/1442]: training_loss: tensor(0.8882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[522/1442]: training_loss: tensor(0.8291, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[523/1442]: training_loss: tensor(0.6493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[524/1442]: training_loss: tensor(0.3406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[525/1442]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[526/1442]: training_loss: tensor(0.4081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[527/1442]: training_loss: tensor(0.5505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[528/1442]: training_loss: tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[529/1442]: training_loss: tensor(0.8033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[530/1442]: training_loss: tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[531/1442]: training_loss: tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[532/1442]: training_loss: tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[533/1442]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[534/1442]: training_loss: tensor(0.5590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[535/1442]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[536/1442]: training_loss: tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[537/1442]: training_loss: tensor(0.5509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[538/1442]: training_loss: tensor(0.4928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[539/1442]: training_loss: tensor(0.6018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[540/1442]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[541/1442]: training_loss: tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[542/1442]: training_loss: tensor(0.3885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[543/1442]: training_loss: tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[544/1442]: training_loss: tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[545/1442]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[546/1442]: training_loss: tensor(0.4388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[547/1442]: training_loss: tensor(0.4898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[548/1442]: training_loss: tensor(0.4977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[549/1442]: training_loss: tensor(0.5718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[550/1442]: training_loss: tensor(0.3374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[551/1442]: training_loss: tensor(0.5121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[552/1442]: training_loss: tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[553/1442]: training_loss: tensor(0.4376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[554/1442]: training_loss: tensor(0.4846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[555/1442]: training_loss: tensor(0.4323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[556/1442]: training_loss: tensor(0.7772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[557/1442]: training_loss: tensor(0.4861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[558/1442]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[559/1442]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[560/1442]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[561/1442]: training_loss: tensor(0.5453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[562/1442]: training_loss: tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[563/1442]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[564/1442]: training_loss: tensor(0.4617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[565/1442]: training_loss: tensor(0.5262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[566/1442]: training_loss: tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[567/1442]: training_loss: tensor(0.4881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[568/1442]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[569/1442]: training_loss: tensor(0.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[570/1442]: training_loss: tensor(0.4969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[571/1442]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[572/1442]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[573/1442]: training_loss: tensor(0.5500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[574/1442]: training_loss: tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[575/1442]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[576/1442]: training_loss: tensor(0.4722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[577/1442]: training_loss: tensor(0.6503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[578/1442]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[579/1442]: training_loss: tensor(0.8504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[580/1442]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[581/1442]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[582/1442]: training_loss: tensor(0.7396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[583/1442]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[584/1442]: training_loss: tensor(0.4400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[585/1442]: training_loss: tensor(0.3281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[586/1442]: training_loss: tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[587/1442]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[588/1442]: training_loss: tensor(0.5066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[589/1442]: training_loss: tensor(0.3092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[590/1442]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[591/1442]: training_loss: tensor(0.4548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[592/1442]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[593/1442]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[594/1442]: training_loss: tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[595/1442]: training_loss: tensor(0.7197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[596/1442]: training_loss: tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[597/1442]: training_loss: tensor(0.9405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[598/1442]: training_loss: tensor(0.4915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[599/1442]: training_loss: tensor(0.5601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[600/1442]: training_loss: tensor(0.5633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[601/1442]: training_loss: tensor(0.8258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[602/1442]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[603/1442]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[604/1442]: training_loss: tensor(0.2600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[605/1442]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[606/1442]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[607/1442]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[608/1442]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[609/1442]: training_loss: tensor(0.5196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[610/1442]: training_loss: tensor(0.4814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[611/1442]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[612/1442]: training_loss: tensor(0.3354, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[613/1442]: training_loss: tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[614/1442]: training_loss: tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[615/1442]: training_loss: tensor(0.4569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[616/1442]: training_loss: tensor(0.4909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[617/1442]: training_loss: tensor(0.4444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[618/1442]: training_loss: tensor(0.4207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[619/1442]: training_loss: tensor(0.4939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[620/1442]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[621/1442]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[622/1442]: training_loss: tensor(0.5760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[623/1442]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[624/1442]: training_loss: tensor(0.6485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[625/1442]: training_loss: tensor(0.5203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[626/1442]: training_loss: tensor(0.7407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[627/1442]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[628/1442]: training_loss: tensor(0.5827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[629/1442]: training_loss: tensor(0.5371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[630/1442]: training_loss: tensor(0.5061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[631/1442]: training_loss: tensor(0.5406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[632/1442]: training_loss: tensor(0.4315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[633/1442]: training_loss: tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[634/1442]: training_loss: tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[635/1442]: training_loss: tensor(0.7313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[636/1442]: training_loss: tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[637/1442]: training_loss: tensor(0.4483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[638/1442]: training_loss: tensor(0.4602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[639/1442]: training_loss: tensor(0.4711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[640/1442]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[641/1442]: training_loss: tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[642/1442]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[643/1442]: training_loss: tensor(0.4256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[644/1442]: training_loss: tensor(0.4714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[645/1442]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[646/1442]: training_loss: tensor(0.4252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[647/1442]: training_loss: tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[648/1442]: training_loss: tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[649/1442]: training_loss: tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[650/1442]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[651/1442]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[652/1442]: training_loss: tensor(0.2936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[653/1442]: training_loss: tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[654/1442]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[655/1442]: training_loss: tensor(0.2120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[656/1442]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[657/1442]: training_loss: tensor(0.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[658/1442]: training_loss: tensor(0.5073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[659/1442]: training_loss: tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[660/1442]: training_loss: tensor(0.4393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[661/1442]: training_loss: tensor(0.5466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[662/1442]: training_loss: tensor(0.4504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[663/1442]: training_loss: tensor(0.9215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[664/1442]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[665/1442]: training_loss: tensor(0.5018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[666/1442]: training_loss: tensor(0.5428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[667/1442]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[668/1442]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[669/1442]: training_loss: tensor(0.7138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[670/1442]: training_loss: tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[671/1442]: training_loss: tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[672/1442]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[673/1442]: training_loss: tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[674/1442]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[675/1442]: training_loss: tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[676/1442]: training_loss: tensor(0.6742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[677/1442]: training_loss: tensor(0.5819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[678/1442]: training_loss: tensor(0.4397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[679/1442]: training_loss: tensor(0.4779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[680/1442]: training_loss: tensor(0.5502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[681/1442]: training_loss: tensor(0.4600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[682/1442]: training_loss: tensor(0.8429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[683/1442]: training_loss: tensor(0.6265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[684/1442]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[685/1442]: training_loss: tensor(0.2700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[686/1442]: training_loss: tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[687/1442]: training_loss: tensor(0.7542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[688/1442]: training_loss: tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[689/1442]: training_loss: tensor(0.4477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[690/1442]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[691/1442]: training_loss: tensor(0.3382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[692/1442]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[693/1442]: training_loss: tensor(0.4666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[694/1442]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[695/1442]: training_loss: tensor(0.6310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[696/1442]: training_loss: tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[697/1442]: training_loss: tensor(0.7138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[698/1442]: training_loss: tensor(0.7692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[699/1442]: training_loss: tensor(0.4547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[700/1442]: training_loss: tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[701/1442]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[702/1442]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[703/1442]: training_loss: tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[704/1442]: training_loss: tensor(0.3296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[705/1442]: training_loss: tensor(0.4746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[706/1442]: training_loss: tensor(0.4778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[707/1442]: training_loss: tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[708/1442]: training_loss: tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[709/1442]: training_loss: tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[710/1442]: training_loss: tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[711/1442]: training_loss: tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[712/1442]: training_loss: tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[713/1442]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[714/1442]: training_loss: tensor(0.4885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[715/1442]: training_loss: tensor(0.5289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[716/1442]: training_loss: tensor(0.4483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[717/1442]: training_loss: tensor(0.4767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[718/1442]: training_loss: tensor(0.7302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[719/1442]: training_loss: tensor(0.4923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[720/1442]: training_loss: tensor(0.4564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[721/1442]: training_loss: tensor(0.3343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[722/1442]: training_loss: tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[723/1442]: training_loss: tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[724/1442]: training_loss: tensor(0.4716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[725/1442]: training_loss: tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[726/1442]: training_loss: tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[727/1442]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[728/1442]: training_loss: tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[729/1442]: training_loss: tensor(0.6455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[730/1442]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[731/1442]: training_loss: tensor(0.3931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[732/1442]: training_loss: tensor(0.5196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[733/1442]: training_loss: tensor(0.8522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[734/1442]: training_loss: tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[735/1442]: training_loss: tensor(0.4517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[736/1442]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[737/1442]: training_loss: tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[738/1442]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[739/1442]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[740/1442]: training_loss: tensor(0.4984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[741/1442]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[742/1442]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[743/1442]: training_loss: tensor(0.5493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[744/1442]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[745/1442]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[746/1442]: training_loss: tensor(0.4957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[747/1442]: training_loss: tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[748/1442]: training_loss: tensor(0.5687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[749/1442]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[750/1442]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[751/1442]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[752/1442]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[753/1442]: training_loss: tensor(0.4618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[754/1442]: training_loss: tensor(0.7216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[755/1442]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[756/1442]: training_loss: tensor(0.4663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[757/1442]: training_loss: tensor(0.4426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[758/1442]: training_loss: tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[759/1442]: training_loss: tensor(0.8697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[760/1442]: training_loss: tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[761/1442]: training_loss: tensor(0.4302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[762/1442]: training_loss: tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[763/1442]: training_loss: tensor(0.5057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[764/1442]: training_loss: tensor(0.4510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[765/1442]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[766/1442]: training_loss: tensor(0.8277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[767/1442]: training_loss: tensor(0.5020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[768/1442]: training_loss: tensor(0.4894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[769/1442]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[770/1442]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[771/1442]: training_loss: tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[772/1442]: training_loss: tensor(0.4464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[773/1442]: training_loss: tensor(0.6405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[774/1442]: training_loss: tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[775/1442]: training_loss: tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[776/1442]: training_loss: tensor(0.4935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[777/1442]: training_loss: tensor(0.4245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[778/1442]: training_loss: tensor(0.2120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[779/1442]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[780/1442]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[781/1442]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[782/1442]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[783/1442]: training_loss: tensor(0.3570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[784/1442]: training_loss: tensor(0.4702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[785/1442]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[786/1442]: training_loss: tensor(0.4695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[787/1442]: training_loss: tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[788/1442]: training_loss: tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[789/1442]: training_loss: tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[790/1442]: training_loss: tensor(0.4477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[791/1442]: training_loss: tensor(0.4229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[792/1442]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[793/1442]: training_loss: tensor(0.4675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[794/1442]: training_loss: tensor(0.7802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[795/1442]: training_loss: tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[796/1442]: training_loss: tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[797/1442]: training_loss: tensor(0.4807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[798/1442]: training_loss: tensor(0.9110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[799/1442]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[800/1442]: training_loss: tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[801/1442]: training_loss: tensor(0.7385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[802/1442]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[803/1442]: training_loss: tensor(0.3842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[804/1442]: training_loss: tensor(0.5491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[805/1442]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[806/1442]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[807/1442]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[808/1442]: training_loss: tensor(0.5132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[809/1442]: training_loss: tensor(0.4532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[810/1442]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[811/1442]: training_loss: tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[812/1442]: training_loss: tensor(0.7427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[813/1442]: training_loss: tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[814/1442]: training_loss: tensor(0.5422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[815/1442]: training_loss: tensor(0.5214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[816/1442]: training_loss: tensor(0.2733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[817/1442]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[818/1442]: training_loss: tensor(0.4580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[819/1442]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[820/1442]: training_loss: tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[821/1442]: training_loss: tensor(0.6019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[822/1442]: training_loss: tensor(0.4878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[823/1442]: training_loss: tensor(0.7718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[824/1442]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[825/1442]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[826/1442]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[827/1442]: training_loss: tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[828/1442]: training_loss: tensor(0.4156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[829/1442]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[830/1442]: training_loss: tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[831/1442]: training_loss: tensor(0.5716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[832/1442]: training_loss: tensor(0.4135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[833/1442]: training_loss: tensor(0.5229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[834/1442]: training_loss: tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[835/1442]: training_loss: tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[836/1442]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[837/1442]: training_loss: tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[838/1442]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[839/1442]: training_loss: tensor(0.4553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[840/1442]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[841/1442]: training_loss: tensor(0.2685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[842/1442]: training_loss: tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[843/1442]: training_loss: tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[844/1442]: training_loss: tensor(0.7731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[845/1442]: training_loss: tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[846/1442]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[847/1442]: training_loss: tensor(0.5033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[848/1442]: training_loss: tensor(0.5051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[849/1442]: training_loss: tensor(0.4962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[850/1442]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[851/1442]: training_loss: tensor(0.5492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[852/1442]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[853/1442]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[854/1442]: training_loss: tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[855/1442]: training_loss: tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[856/1442]: training_loss: tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[857/1442]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[858/1442]: training_loss: tensor(0.5493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[859/1442]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[860/1442]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[861/1442]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[862/1442]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[863/1442]: training_loss: tensor(0.2687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[864/1442]: training_loss: tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[865/1442]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[866/1442]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[867/1442]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[868/1442]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[869/1442]: training_loss: tensor(0.6767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[870/1442]: training_loss: tensor(0.5017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[871/1442]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[872/1442]: training_loss: tensor(0.5002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[873/1442]: training_loss: tensor(0.5025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[874/1442]: training_loss: tensor(0.5345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[875/1442]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[876/1442]: training_loss: tensor(0.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[877/1442]: training_loss: tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[878/1442]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[879/1442]: training_loss: tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[880/1442]: training_loss: tensor(0.4062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[881/1442]: training_loss: tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[882/1442]: training_loss: tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[883/1442]: training_loss: tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[884/1442]: training_loss: tensor(0.4277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[885/1442]: training_loss: tensor(0.4743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[886/1442]: training_loss: tensor(0.9542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[887/1442]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[888/1442]: training_loss: tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[889/1442]: training_loss: tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[890/1442]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[891/1442]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[892/1442]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[893/1442]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[894/1442]: training_loss: tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[895/1442]: training_loss: tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[896/1442]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[897/1442]: training_loss: tensor(0.4437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[898/1442]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[899/1442]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[900/1442]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[901/1442]: training_loss: tensor(0.4771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[902/1442]: training_loss: tensor(0.5372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[903/1442]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[904/1442]: training_loss: tensor(0.6523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[905/1442]: training_loss: tensor(0.9442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[906/1442]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[907/1442]: training_loss: tensor(0.8783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[908/1442]: training_loss: tensor(0.4805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[909/1442]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[910/1442]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[911/1442]: training_loss: tensor(0.5405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[912/1442]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[913/1442]: training_loss: tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[914/1442]: training_loss: tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[915/1442]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[916/1442]: training_loss: tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[917/1442]: training_loss: tensor(0.3237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[918/1442]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[919/1442]: training_loss: tensor(0.4146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[920/1442]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[921/1442]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[922/1442]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[923/1442]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[924/1442]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[925/1442]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[926/1442]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[927/1442]: training_loss: tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[928/1442]: training_loss: tensor(0.9702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[929/1442]: training_loss: tensor(0.4157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[930/1442]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[931/1442]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[932/1442]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[933/1442]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[934/1442]: training_loss: tensor(0.8424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[935/1442]: training_loss: tensor(0.8039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[936/1442]: training_loss: tensor(0.7263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[937/1442]: training_loss: tensor(0.4622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[938/1442]: training_loss: tensor(0.7689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[939/1442]: training_loss: tensor(0.3292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[940/1442]: training_loss: tensor(0.4865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[941/1442]: training_loss: tensor(0.5380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[942/1442]: training_loss: tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[943/1442]: training_loss: tensor(0.4706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[944/1442]: training_loss: tensor(0.6308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[945/1442]: training_loss: tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[946/1442]: training_loss: tensor(0.8200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[947/1442]: training_loss: tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[948/1442]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[949/1442]: training_loss: tensor(0.4999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[950/1442]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[951/1442]: training_loss: tensor(0.5815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[952/1442]: training_loss: tensor(0.8880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[953/1442]: training_loss: tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[954/1442]: training_loss: tensor(0.5054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[955/1442]: training_loss: tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[956/1442]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[957/1442]: training_loss: tensor(0.4185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[958/1442]: training_loss: tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[959/1442]: training_loss: tensor(0.2944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[960/1442]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[961/1442]: training_loss: tensor(0.4173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[962/1442]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[963/1442]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[964/1442]: training_loss: tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[965/1442]: training_loss: tensor(0.4722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[966/1442]: training_loss: tensor(0.5190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[967/1442]: training_loss: tensor(0.5609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[968/1442]: training_loss: tensor(0.2733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[969/1442]: training_loss: tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[970/1442]: training_loss: tensor(0.4609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[971/1442]: training_loss: tensor(0.4708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[972/1442]: training_loss: tensor(0.3151, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[973/1442]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[974/1442]: training_loss: tensor(0.4908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[975/1442]: training_loss: tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[976/1442]: training_loss: tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[977/1442]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[978/1442]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[979/1442]: training_loss: tensor(0.5690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[980/1442]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[981/1442]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[982/1442]: training_loss: tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[983/1442]: training_loss: tensor(0.4509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[984/1442]: training_loss: tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[985/1442]: training_loss: tensor(0.3399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[986/1442]: training_loss: tensor(0.4409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[987/1442]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[988/1442]: training_loss: tensor(0.6044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[989/1442]: training_loss: tensor(0.5741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[990/1442]: training_loss: tensor(0.5742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[991/1442]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[992/1442]: training_loss: tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[993/1442]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[994/1442]: training_loss: tensor(1.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[995/1442]: training_loss: tensor(0.4667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[996/1442]: training_loss: tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[997/1442]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[998/1442]: training_loss: tensor(0.7981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[999/1442]: training_loss: tensor(0.5021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1000/1442]: training_loss: tensor(0.5374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1001/1442]: training_loss: tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1002/1442]: training_loss: tensor(0.4469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1003/1442]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1004/1442]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1005/1442]: training_loss: tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1006/1442]: training_loss: tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1007/1442]: training_loss: tensor(0.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1008/1442]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1009/1442]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1010/1442]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1011/1442]: training_loss: tensor(0.4146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1012/1442]: training_loss: tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1013/1442]: training_loss: tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1014/1442]: training_loss: tensor(0.5690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1015/1442]: training_loss: tensor(0.3092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1016/1442]: training_loss: tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1017/1442]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1018/1442]: training_loss: tensor(0.7166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1019/1442]: training_loss: tensor(0.4941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1020/1442]: training_loss: tensor(0.5400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1021/1442]: training_loss: tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1022/1442]: training_loss: tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1023/1442]: training_loss: tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1024/1442]: training_loss: tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1025/1442]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1026/1442]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1027/1442]: training_loss: tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1028/1442]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1029/1442]: training_loss: tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1030/1442]: training_loss: tensor(0.5460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1031/1442]: training_loss: tensor(0.4488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1032/1442]: training_loss: tensor(0.4764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1033/1442]: training_loss: tensor(0.4977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1034/1442]: training_loss: tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1035/1442]: training_loss: tensor(0.8164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1036/1442]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1037/1442]: training_loss: tensor(0.4091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1038/1442]: training_loss: tensor(0.5313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1039/1442]: training_loss: tensor(0.3762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1040/1442]: training_loss: tensor(0.5452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1041/1442]: training_loss: tensor(0.4397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1042/1442]: training_loss: tensor(0.5456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1043/1442]: training_loss: tensor(0.4736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1044/1442]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1045/1442]: training_loss: tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1046/1442]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1047/1442]: training_loss: tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1048/1442]: training_loss: tensor(0.5658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1049/1442]: training_loss: tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1050/1442]: training_loss: tensor(0.4199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1051/1442]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1052/1442]: training_loss: tensor(0.4682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1053/1442]: training_loss: tensor(1.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1054/1442]: training_loss: tensor(0.4970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1055/1442]: training_loss: tensor(0.6547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1056/1442]: training_loss: tensor(0.5187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1057/1442]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1058/1442]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1059/1442]: training_loss: tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1060/1442]: training_loss: tensor(0.5748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1061/1442]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1062/1442]: training_loss: tensor(0.4254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1063/1442]: training_loss: tensor(0.4190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1064/1442]: training_loss: tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1065/1442]: training_loss: tensor(0.5741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1066/1442]: training_loss: tensor(0.4531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1067/1442]: training_loss: tensor(0.5014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1068/1442]: training_loss: tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1069/1442]: training_loss: tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1070/1442]: training_loss: tensor(0.4654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1071/1442]: training_loss: tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1072/1442]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1073/1442]: training_loss: tensor(0.5489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1074/1442]: training_loss: tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1075/1442]: training_loss: tensor(0.3813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1076/1442]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1077/1442]: training_loss: tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1078/1442]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1079/1442]: training_loss: tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1080/1442]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1081/1442]: training_loss: tensor(0.5326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1082/1442]: training_loss: tensor(0.4545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1083/1442]: training_loss: tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1084/1442]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1085/1442]: training_loss: tensor(0.8514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1086/1442]: training_loss: tensor(0.4881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1087/1442]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1088/1442]: training_loss: tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1089/1442]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1090/1442]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1091/1442]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1092/1442]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1093/1442]: training_loss: tensor(0.5245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1094/1442]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1095/1442]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1096/1442]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1097/1442]: training_loss: tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1098/1442]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1099/1442]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1100/1442]: training_loss: tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1101/1442]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1102/1442]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1103/1442]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1104/1442]: training_loss: tensor(0.4404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1105/1442]: training_loss: tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1106/1442]: training_loss: tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1107/1442]: training_loss: tensor(0.8590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1108/1442]: training_loss: tensor(0.3617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1109/1442]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1110/1442]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1111/1442]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1112/1442]: training_loss: tensor(0.8253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1113/1442]: training_loss: tensor(0.4646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1114/1442]: training_loss: tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1115/1442]: training_loss: tensor(0.9516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1116/1442]: training_loss: tensor(0.4302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1117/1442]: training_loss: tensor(0.5222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1118/1442]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1119/1442]: training_loss: tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1120/1442]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1121/1442]: training_loss: tensor(0.6591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1122/1442]: training_loss: tensor(0.9675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1123/1442]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1124/1442]: training_loss: tensor(0.4536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1125/1442]: training_loss: tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1126/1442]: training_loss: tensor(0.4711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1127/1442]: training_loss: tensor(0.3112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1128/1442]: training_loss: tensor(0.2114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1129/1442]: training_loss: tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1130/1442]: training_loss: tensor(0.3297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1131/1442]: training_loss: tensor(0.2813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1132/1442]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1133/1442]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1134/1442]: training_loss: tensor(0.6606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1135/1442]: training_loss: tensor(0.5328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1136/1442]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1137/1442]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1138/1442]: training_loss: tensor(0.5438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1139/1442]: training_loss: tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1140/1442]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1141/1442]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1142/1442]: training_loss: tensor(0.5081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1143/1442]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1144/1442]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1145/1442]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1146/1442]: training_loss: tensor(0.5296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1147/1442]: training_loss: tensor(0.4690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1148/1442]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1149/1442]: training_loss: tensor(0.4970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1150/1442]: training_loss: tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1151/1442]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1152/1442]: training_loss: tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1153/1442]: training_loss: tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1154/1442]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1155/1442]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1156/1442]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1157/1442]: training_loss: tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1158/1442]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1159/1442]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1160/1442]: training_loss: tensor(0.2832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1161/1442]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1162/1442]: training_loss: tensor(0.4908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1163/1442]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1164/1442]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1165/1442]: training_loss: tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1166/1442]: training_loss: tensor(0.4601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1167/1442]: training_loss: tensor(0.8564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1168/1442]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1169/1442]: training_loss: tensor(0.4329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1170/1442]: training_loss: tensor(0.4461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1171/1442]: training_loss: tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1172/1442]: training_loss: tensor(0.5096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1173/1442]: training_loss: tensor(0.5114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1174/1442]: training_loss: tensor(0.4839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1175/1442]: training_loss: tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1176/1442]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1177/1442]: training_loss: tensor(0.4847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1178/1442]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1179/1442]: training_loss: tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1180/1442]: training_loss: tensor(0.2895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1181/1442]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1182/1442]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1183/1442]: training_loss: tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1184/1442]: training_loss: tensor(0.7564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1185/1442]: training_loss: tensor(0.4123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1186/1442]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1187/1442]: training_loss: tensor(0.4374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1188/1442]: training_loss: tensor(0.3689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1189/1442]: training_loss: tensor(0.4921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1190/1442]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1191/1442]: training_loss: tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1192/1442]: training_loss: tensor(0.4885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1193/1442]: training_loss: tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1194/1442]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1195/1442]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1196/1442]: training_loss: tensor(0.5892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1197/1442]: training_loss: tensor(0.3501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1198/1442]: training_loss: tensor(0.5349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1199/1442]: training_loss: tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1200/1442]: training_loss: tensor(0.5453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1201/1442]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1202/1442]: training_loss: tensor(0.4081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1203/1442]: training_loss: tensor(0.4821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1204/1442]: training_loss: tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1205/1442]: training_loss: tensor(0.5469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1206/1442]: training_loss: tensor(0.4637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1207/1442]: training_loss: tensor(0.5714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1208/1442]: training_loss: tensor(0.4208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1209/1442]: training_loss: tensor(0.6075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1210/1442]: training_loss: tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1211/1442]: training_loss: tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1212/1442]: training_loss: tensor(0.6246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1213/1442]: training_loss: tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1214/1442]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1215/1442]: training_loss: tensor(0.5078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1216/1442]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1217/1442]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1218/1442]: training_loss: tensor(0.4753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1219/1442]: training_loss: tensor(0.8136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1220/1442]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1221/1442]: training_loss: tensor(0.2836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1222/1442]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1223/1442]: training_loss: tensor(0.2642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1224/1442]: training_loss: tensor(0.3379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1225/1442]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1226/1442]: training_loss: tensor(0.4544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1227/1442]: training_loss: tensor(0.7921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1228/1442]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1229/1442]: training_loss: tensor(0.3345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1230/1442]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1231/1442]: training_loss: tensor(0.3546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1232/1442]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1233/1442]: training_loss: tensor(0.4713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1234/1442]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1235/1442]: training_loss: tensor(0.4735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1236/1442]: training_loss: tensor(0.3613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1237/1442]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1238/1442]: training_loss: tensor(0.5479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1239/1442]: training_loss: tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1240/1442]: training_loss: tensor(0.4611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1241/1442]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1242/1442]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1243/1442]: training_loss: tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1244/1442]: training_loss: tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1245/1442]: training_loss: tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1246/1442]: training_loss: tensor(0.5137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1247/1442]: training_loss: tensor(0.4162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1248/1442]: training_loss: tensor(0.5109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1249/1442]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1250/1442]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1251/1442]: training_loss: tensor(0.4923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1252/1442]: training_loss: tensor(0.6740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1253/1442]: training_loss: tensor(0.3560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1254/1442]: training_loss: tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1255/1442]: training_loss: tensor(0.8033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1256/1442]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1257/1442]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1258/1442]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1259/1442]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1260/1442]: training_loss: tensor(0.6546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1261/1442]: training_loss: tensor(0.4883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1262/1442]: training_loss: tensor(0.4334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1263/1442]: training_loss: tensor(0.5409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1264/1442]: training_loss: tensor(0.4142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1265/1442]: training_loss: tensor(0.4298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1266/1442]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1267/1442]: training_loss: tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1268/1442]: training_loss: tensor(0.4978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1269/1442]: training_loss: tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1270/1442]: training_loss: tensor(0.5551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1271/1442]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1272/1442]: training_loss: tensor(0.2667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1273/1442]: training_loss: tensor(0.5905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1274/1442]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1275/1442]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1276/1442]: training_loss: tensor(0.8645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1277/1442]: training_loss: tensor(0.3333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1278/1442]: training_loss: tensor(0.4159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1279/1442]: training_loss: tensor(0.4389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1280/1442]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1281/1442]: training_loss: tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1282/1442]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1283/1442]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1284/1442]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1285/1442]: training_loss: tensor(0.3411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1286/1442]: training_loss: tensor(0.5332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1287/1442]: training_loss: tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1288/1442]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1289/1442]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1290/1442]: training_loss: tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1291/1442]: training_loss: tensor(0.2783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1292/1442]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1293/1442]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1294/1442]: training_loss: tensor(0.4802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1295/1442]: training_loss: tensor(0.5298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1296/1442]: training_loss: tensor(0.2087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1297/1442]: training_loss: tensor(0.7909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1298/1442]: training_loss: tensor(0.8652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1299/1442]: training_loss: tensor(0.4489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1300/1442]: training_loss: tensor(0.5786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1301/1442]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1302/1442]: training_loss: tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1303/1442]: training_loss: tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1304/1442]: training_loss: tensor(0.2123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1305/1442]: training_loss: tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1306/1442]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1307/1442]: training_loss: tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1308/1442]: training_loss: tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1309/1442]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1310/1442]: training_loss: tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1311/1442]: training_loss: tensor(0.6524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1312/1442]: training_loss: tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1313/1442]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1314/1442]: training_loss: tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1315/1442]: training_loss: tensor(0.9216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1316/1442]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1317/1442]: training_loss: tensor(0.5170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1318/1442]: training_loss: tensor(0.5103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1319/1442]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1320/1442]: training_loss: tensor(0.3158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1321/1442]: training_loss: tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1322/1442]: training_loss: tensor(0.7059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1323/1442]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1324/1442]: training_loss: tensor(0.4934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1325/1442]: training_loss: tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1326/1442]: training_loss: tensor(0.7957, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1327/1442]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1328/1442]: training_loss: tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1329/1442]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1330/1442]: training_loss: tensor(0.4244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1331/1442]: training_loss: tensor(0.4783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1332/1442]: training_loss: tensor(0.4264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1333/1442]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1334/1442]: training_loss: tensor(0.4897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1335/1442]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1336/1442]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1337/1442]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1338/1442]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1339/1442]: training_loss: tensor(0.8849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1340/1442]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1341/1442]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1342/1442]: training_loss: tensor(0.3295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1343/1442]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1344/1442]: training_loss: tensor(0.6685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1345/1442]: training_loss: tensor(0.4893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1346/1442]: training_loss: tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1347/1442]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1348/1442]: training_loss: tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1349/1442]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1350/1442]: training_loss: tensor(0.4720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1351/1442]: training_loss: tensor(0.5398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1352/1442]: training_loss: tensor(0.4857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1353/1442]: training_loss: tensor(0.7541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1354/1442]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1355/1442]: training_loss: tensor(0.4191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1356/1442]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1357/1442]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1358/1442]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1359/1442]: training_loss: tensor(0.3433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1360/1442]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1361/1442]: training_loss: tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1362/1442]: training_loss: tensor(0.5302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1363/1442]: training_loss: tensor(0.6315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1364/1442]: training_loss: tensor(0.5853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1365/1442]: training_loss: tensor(0.4131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1366/1442]: training_loss: tensor(0.4886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1367/1442]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1368/1442]: training_loss: tensor(0.8035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1369/1442]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1370/1442]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1371/1442]: training_loss: tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1372/1442]: training_loss: tensor(0.4844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1373/1442]: training_loss: tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1374/1442]: training_loss: tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1375/1442]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1376/1442]: training_loss: tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1377/1442]: training_loss: tensor(0.3125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1378/1442]: training_loss: tensor(0.4036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1379/1442]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1380/1442]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1381/1442]: training_loss: tensor(0.4996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1382/1442]: training_loss: tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1383/1442]: training_loss: tensor(0.3804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1384/1442]: training_loss: tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1385/1442]: training_loss: tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1386/1442]: training_loss: tensor(0.4343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1387/1442]: training_loss: tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1388/1442]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1389/1442]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1390/1442]: training_loss: tensor(0.5024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1391/1442]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1392/1442]: training_loss: tensor(0.4857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1393/1442]: training_loss: tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1394/1442]: training_loss: tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1395/1442]: training_loss: tensor(0.4949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1396/1442]: training_loss: tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1397/1442]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1398/1442]: training_loss: tensor(0.4309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1399/1442]: training_loss: tensor(0.5494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1400/1442]: training_loss: tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1401/1442]: training_loss: tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1402/1442]: training_loss: tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1403/1442]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1404/1442]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1405/1442]: training_loss: tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1406/1442]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1407/1442]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1408/1442]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1409/1442]: training_loss: tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1410/1442]: training_loss: tensor(0.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1411/1442]: training_loss: tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1412/1442]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1413/1442]: training_loss: tensor(0.4524, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1414/1442]: training_loss: tensor(1.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1415/1442]: training_loss: tensor(0.4341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1416/1442]: training_loss: tensor(0.5157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1417/1442]: training_loss: tensor(0.4438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1418/1442]: training_loss: tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1419/1442]: training_loss: tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1420/1442]: training_loss: tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1421/1442]: training_loss: tensor(0.4686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1422/1442]: training_loss: tensor(0.4298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1423/1442]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1424/1442]: training_loss: tensor(0.4868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1425/1442]: training_loss: tensor(0.4889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1426/1442]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1427/1442]: training_loss: tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1428/1442]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1429/1442]: training_loss: tensor(0.4636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1430/1442]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1431/1442]: training_loss: tensor(0.4841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1432/1442]: training_loss: tensor(0.5370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1433/1442]: training_loss: tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1434/1442]: training_loss: tensor(0.4670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1435/1442]: training_loss: tensor(0.4734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1436/1442]: training_loss: tensor(0.4081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1437/1442]: training_loss: tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1438/1442]: training_loss: tensor(0.8365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1439/1442]: training_loss: tensor(0.7929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1440/1442]: training_loss: tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1441/1442]: training_loss: tensor(0.4494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [2/5],global step [2884/7210], pt loss:0.0000, dev loss:0.5359\n",
      "batch_no[0/1442]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1/1442]: training_loss: tensor(0.4432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[2/1442]: training_loss: tensor(0.1855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[3/1442]: training_loss: tensor(0.5113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[4/1442]: training_loss: tensor(0.2905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[5/1442]: training_loss: tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[6/1442]: training_loss: tensor(0.6737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[7/1442]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[8/1442]: training_loss: tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[9/1442]: training_loss: tensor(0.4500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[10/1442]: training_loss: tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[11/1442]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[12/1442]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[13/1442]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[14/1442]: training_loss: tensor(0.4576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[15/1442]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[16/1442]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[17/1442]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[18/1442]: training_loss: tensor(0.6252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[19/1442]: training_loss: tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[20/1442]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[21/1442]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[22/1442]: training_loss: tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[23/1442]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[24/1442]: training_loss: tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[25/1442]: training_loss: tensor(0.5814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[26/1442]: training_loss: tensor(0.4634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[27/1442]: training_loss: tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[28/1442]: training_loss: tensor(0.2619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[29/1442]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[30/1442]: training_loss: tensor(0.8615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[31/1442]: training_loss: tensor(0.4693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[32/1442]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[33/1442]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[34/1442]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[35/1442]: training_loss: tensor(0.7412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[36/1442]: training_loss: tensor(0.5220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[37/1442]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[38/1442]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[39/1442]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[40/1442]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[41/1442]: training_loss: tensor(0.6659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[42/1442]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[43/1442]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[44/1442]: training_loss: tensor(0.4770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[45/1442]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[46/1442]: training_loss: tensor(0.4789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[47/1442]: training_loss: tensor(0.4481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[48/1442]: training_loss: tensor(0.3333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[49/1442]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[50/1442]: training_loss: tensor(0.1717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[51/1442]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[52/1442]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[53/1442]: training_loss: tensor(0.4702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[54/1442]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[55/1442]: training_loss: tensor(0.5476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[56/1442]: training_loss: tensor(1.2909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[57/1442]: training_loss: tensor(0.6733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[58/1442]: training_loss: tensor(0.7193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[59/1442]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[60/1442]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[61/1442]: training_loss: tensor(0.3294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[62/1442]: training_loss: tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[63/1442]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[64/1442]: training_loss: tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[65/1442]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[66/1442]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[67/1442]: training_loss: tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[68/1442]: training_loss: tensor(0.3870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[69/1442]: training_loss: tensor(0.4853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[70/1442]: training_loss: tensor(0.3670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[71/1442]: training_loss: tensor(0.3807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[72/1442]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[73/1442]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[74/1442]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[75/1442]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[76/1442]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[77/1442]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[78/1442]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[79/1442]: training_loss: tensor(0.4773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[80/1442]: training_loss: tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[81/1442]: training_loss: tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[82/1442]: training_loss: tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[83/1442]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[84/1442]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[85/1442]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[86/1442]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[87/1442]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[88/1442]: training_loss: tensor(0.8247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[89/1442]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[90/1442]: training_loss: tensor(0.2955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[91/1442]: training_loss: tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[92/1442]: training_loss: tensor(0.3611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[93/1442]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[94/1442]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[95/1442]: training_loss: tensor(0.7263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[96/1442]: training_loss: tensor(0.6519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[97/1442]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[98/1442]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[99/1442]: training_loss: tensor(0.5646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[100/1442]: training_loss: tensor(0.2866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[101/1442]: training_loss: tensor(1.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[102/1442]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[103/1442]: training_loss: tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[104/1442]: training_loss: tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[105/1442]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[106/1442]: training_loss: tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[107/1442]: training_loss: tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[108/1442]: training_loss: tensor(0.3262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[109/1442]: training_loss: tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[110/1442]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[111/1442]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[112/1442]: training_loss: tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[113/1442]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[114/1442]: training_loss: tensor(0.3161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[115/1442]: training_loss: tensor(0.4315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[116/1442]: training_loss: tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[117/1442]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[118/1442]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[119/1442]: training_loss: tensor(0.4464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[120/1442]: training_loss: tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[121/1442]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[122/1442]: training_loss: tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[123/1442]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[124/1442]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[125/1442]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[126/1442]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[127/1442]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[128/1442]: training_loss: tensor(0.3501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[129/1442]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[130/1442]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[131/1442]: training_loss: tensor(0.4245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[132/1442]: training_loss: tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[133/1442]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[134/1442]: training_loss: tensor(0.4494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[135/1442]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[136/1442]: training_loss: tensor(1.1583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[137/1442]: training_loss: tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[138/1442]: training_loss: tensor(0.7158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[139/1442]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[140/1442]: training_loss: tensor(0.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[141/1442]: training_loss: tensor(0.6485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[142/1442]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[143/1442]: training_loss: tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[144/1442]: training_loss: tensor(0.4251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[145/1442]: training_loss: tensor(0.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[146/1442]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[147/1442]: training_loss: tensor(0.3941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[148/1442]: training_loss: tensor(0.5351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[149/1442]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[150/1442]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[151/1442]: training_loss: tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[152/1442]: training_loss: tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[153/1442]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[154/1442]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[155/1442]: training_loss: tensor(0.4838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[156/1442]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[157/1442]: training_loss: tensor(0.5285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[158/1442]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[159/1442]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[160/1442]: training_loss: tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[161/1442]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[162/1442]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[163/1442]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[164/1442]: training_loss: tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[165/1442]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[166/1442]: training_loss: tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[167/1442]: training_loss: tensor(0.9020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[168/1442]: training_loss: tensor(0.5003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[169/1442]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[170/1442]: training_loss: tensor(0.4140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[171/1442]: training_loss: tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[172/1442]: training_loss: tensor(0.3403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[173/1442]: training_loss: tensor(0.4285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[174/1442]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[175/1442]: training_loss: tensor(0.3580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[176/1442]: training_loss: tensor(0.2768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[177/1442]: training_loss: tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[178/1442]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[179/1442]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[180/1442]: training_loss: tensor(0.8095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[181/1442]: training_loss: tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[182/1442]: training_loss: tensor(0.4874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[183/1442]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[184/1442]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[185/1442]: training_loss: tensor(0.7743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[186/1442]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[187/1442]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[188/1442]: training_loss: tensor(0.4154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[189/1442]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[190/1442]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[191/1442]: training_loss: tensor(0.5276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[192/1442]: training_loss: tensor(0.5461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[193/1442]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[194/1442]: training_loss: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[195/1442]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[196/1442]: training_loss: tensor(0.3253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[197/1442]: training_loss: tensor(0.8868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[198/1442]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[199/1442]: training_loss: tensor(0.6056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[200/1442]: training_loss: tensor(0.5135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[201/1442]: training_loss: tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[202/1442]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[203/1442]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[204/1442]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[205/1442]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[206/1442]: training_loss: tensor(0.5820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[207/1442]: training_loss: tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[208/1442]: training_loss: tensor(0.5097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[209/1442]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[210/1442]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[211/1442]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[212/1442]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[213/1442]: training_loss: tensor(0.3601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[214/1442]: training_loss: tensor(0.2786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[215/1442]: training_loss: tensor(0.4957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[216/1442]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[217/1442]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[218/1442]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[219/1442]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[220/1442]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[221/1442]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[222/1442]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[223/1442]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[224/1442]: training_loss: tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[225/1442]: training_loss: tensor(1.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[226/1442]: training_loss: tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[227/1442]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[228/1442]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[229/1442]: training_loss: tensor(0.5590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[230/1442]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[231/1442]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[232/1442]: training_loss: tensor(0.3677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[233/1442]: training_loss: tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[234/1442]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[235/1442]: training_loss: tensor(0.2750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[236/1442]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[237/1442]: training_loss: tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[238/1442]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[239/1442]: training_loss: tensor(0.4212, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[240/1442]: training_loss: tensor(0.4269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[241/1442]: training_loss: tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[242/1442]: training_loss: tensor(0.5320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[243/1442]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[244/1442]: training_loss: tensor(0.6045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[245/1442]: training_loss: tensor(0.6670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[246/1442]: training_loss: tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[247/1442]: training_loss: tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[248/1442]: training_loss: tensor(0.5351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[249/1442]: training_loss: tensor(0.8181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[250/1442]: training_loss: tensor(0.6174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[251/1442]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[252/1442]: training_loss: tensor(0.6270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[253/1442]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[254/1442]: training_loss: tensor(0.3998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[255/1442]: training_loss: tensor(0.4910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[256/1442]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[257/1442]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[258/1442]: training_loss: tensor(0.4782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[259/1442]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[260/1442]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[261/1442]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[262/1442]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[263/1442]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[264/1442]: training_loss: tensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[265/1442]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[266/1442]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[267/1442]: training_loss: tensor(0.8675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[268/1442]: training_loss: tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[269/1442]: training_loss: tensor(0.3951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[270/1442]: training_loss: tensor(0.3592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[271/1442]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[272/1442]: training_loss: tensor(0.3433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[273/1442]: training_loss: tensor(0.4313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[274/1442]: training_loss: tensor(0.3648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[275/1442]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[276/1442]: training_loss: tensor(0.4908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[277/1442]: training_loss: tensor(0.6173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[278/1442]: training_loss: tensor(0.2936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[279/1442]: training_loss: tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[280/1442]: training_loss: tensor(0.4371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[281/1442]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[282/1442]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[283/1442]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[284/1442]: training_loss: tensor(0.5889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[285/1442]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[286/1442]: training_loss: tensor(0.4262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[287/1442]: training_loss: tensor(0.6541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[288/1442]: training_loss: tensor(0.5716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[289/1442]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[290/1442]: training_loss: tensor(0.5406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[291/1442]: training_loss: tensor(0.5942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[292/1442]: training_loss: tensor(1.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[293/1442]: training_loss: tensor(0.3567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[294/1442]: training_loss: tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[295/1442]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[296/1442]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[297/1442]: training_loss: tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[298/1442]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[299/1442]: training_loss: tensor(0.4731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[300/1442]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[301/1442]: training_loss: tensor(0.6337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[302/1442]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[303/1442]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[304/1442]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[305/1442]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[306/1442]: training_loss: tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[307/1442]: training_loss: tensor(0.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[308/1442]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[309/1442]: training_loss: tensor(0.4100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[310/1442]: training_loss: tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[311/1442]: training_loss: tensor(0.3563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[312/1442]: training_loss: tensor(0.2834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[313/1442]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[314/1442]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[315/1442]: training_loss: tensor(0.5166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[316/1442]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[317/1442]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[318/1442]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[319/1442]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[320/1442]: training_loss: tensor(0.4936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[321/1442]: training_loss: tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[322/1442]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[323/1442]: training_loss: tensor(1.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[324/1442]: training_loss: tensor(1.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[325/1442]: training_loss: tensor(0.3573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[326/1442]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[327/1442]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[328/1442]: training_loss: tensor(0.4450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[329/1442]: training_loss: tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[330/1442]: training_loss: tensor(1.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[331/1442]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[332/1442]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[333/1442]: training_loss: tensor(0.4458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[334/1442]: training_loss: tensor(0.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[335/1442]: training_loss: tensor(0.4934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[336/1442]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[337/1442]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[338/1442]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[339/1442]: training_loss: tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[340/1442]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[341/1442]: training_loss: tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[342/1442]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[343/1442]: training_loss: tensor(0.5506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[344/1442]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[345/1442]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[346/1442]: training_loss: tensor(1.2898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[347/1442]: training_loss: tensor(0.5893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[348/1442]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[349/1442]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[350/1442]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[351/1442]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[352/1442]: training_loss: tensor(0.5277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[353/1442]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[354/1442]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[355/1442]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[356/1442]: training_loss: tensor(0.5349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[357/1442]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[358/1442]: training_loss: tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[359/1442]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[360/1442]: training_loss: tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[361/1442]: training_loss: tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[362/1442]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[363/1442]: training_loss: tensor(0.6210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[364/1442]: training_loss: tensor(0.8683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[365/1442]: training_loss: tensor(0.4880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[366/1442]: training_loss: tensor(0.4091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[367/1442]: training_loss: tensor(0.4716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[368/1442]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[369/1442]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[370/1442]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[371/1442]: training_loss: tensor(0.5134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[372/1442]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[373/1442]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[374/1442]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[375/1442]: training_loss: tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[376/1442]: training_loss: tensor(0.5023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[377/1442]: training_loss: tensor(0.7869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[378/1442]: training_loss: tensor(0.2728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[379/1442]: training_loss: tensor(0.3348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[380/1442]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[381/1442]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[382/1442]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[383/1442]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[384/1442]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[385/1442]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[386/1442]: training_loss: tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[387/1442]: training_loss: tensor(0.5424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[388/1442]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[389/1442]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[390/1442]: training_loss: tensor(0.5270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[391/1442]: training_loss: tensor(0.8483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[392/1442]: training_loss: tensor(0.8147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[393/1442]: training_loss: tensor(0.5005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[394/1442]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[395/1442]: training_loss: tensor(0.5064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[396/1442]: training_loss: tensor(0.4287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[397/1442]: training_loss: tensor(0.4574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[398/1442]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[399/1442]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[400/1442]: training_loss: tensor(0.2892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[401/1442]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[402/1442]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[403/1442]: training_loss: tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[404/1442]: training_loss: tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[405/1442]: training_loss: tensor(0.4651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[406/1442]: training_loss: tensor(0.3474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[407/1442]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[408/1442]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[409/1442]: training_loss: tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[410/1442]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[411/1442]: training_loss: tensor(0.4488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[412/1442]: training_loss: tensor(0.3547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[413/1442]: training_loss: tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[414/1442]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[415/1442]: training_loss: tensor(0.7116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[416/1442]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[417/1442]: training_loss: tensor(0.8148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[418/1442]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[419/1442]: training_loss: tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[420/1442]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[421/1442]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[422/1442]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[423/1442]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[424/1442]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[425/1442]: training_loss: tensor(0.2717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[426/1442]: training_loss: tensor(0.2730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[427/1442]: training_loss: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[428/1442]: training_loss: tensor(0.4155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[429/1442]: training_loss: tensor(0.2840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[430/1442]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[431/1442]: training_loss: tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[432/1442]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[433/1442]: training_loss: tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[434/1442]: training_loss: tensor(0.4500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[435/1442]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[436/1442]: training_loss: tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[437/1442]: training_loss: tensor(0.4793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[438/1442]: training_loss: tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[439/1442]: training_loss: tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[440/1442]: training_loss: tensor(0.5371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[441/1442]: training_loss: tensor(0.3258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[442/1442]: training_loss: tensor(0.5179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[443/1442]: training_loss: tensor(0.6619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[444/1442]: training_loss: tensor(0.7288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[445/1442]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[446/1442]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[447/1442]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[448/1442]: training_loss: tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[449/1442]: training_loss: tensor(0.9011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[450/1442]: training_loss: tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[451/1442]: training_loss: tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[452/1442]: training_loss: tensor(0.5869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[453/1442]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[454/1442]: training_loss: tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[455/1442]: training_loss: tensor(0.7758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[456/1442]: training_loss: tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[457/1442]: training_loss: tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[458/1442]: training_loss: tensor(0.5326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[459/1442]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[460/1442]: training_loss: tensor(0.5204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[461/1442]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[462/1442]: training_loss: tensor(0.3125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[463/1442]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[464/1442]: training_loss: tensor(0.5457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[465/1442]: training_loss: tensor(0.5218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[466/1442]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[467/1442]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[468/1442]: training_loss: tensor(0.3689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[469/1442]: training_loss: tensor(0.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[470/1442]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[471/1442]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[472/1442]: training_loss: tensor(0.7965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[473/1442]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[474/1442]: training_loss: tensor(0.4307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[475/1442]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[476/1442]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[477/1442]: training_loss: tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[478/1442]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[479/1442]: training_loss: tensor(0.8763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[480/1442]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[481/1442]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[482/1442]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[483/1442]: training_loss: tensor(0.4345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[484/1442]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[485/1442]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[486/1442]: training_loss: tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[487/1442]: training_loss: tensor(0.4815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[488/1442]: training_loss: tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[489/1442]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[490/1442]: training_loss: tensor(0.3132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[491/1442]: training_loss: tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[492/1442]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[493/1442]: training_loss: tensor(0.8337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[494/1442]: training_loss: tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[495/1442]: training_loss: tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[496/1442]: training_loss: tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[497/1442]: training_loss: tensor(0.3252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[498/1442]: training_loss: tensor(0.4230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[499/1442]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[500/1442]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[501/1442]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[502/1442]: training_loss: tensor(0.3354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[503/1442]: training_loss: tensor(0.4447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[504/1442]: training_loss: tensor(0.4903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[505/1442]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[506/1442]: training_loss: tensor(0.3645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[507/1442]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[508/1442]: training_loss: tensor(0.8151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[509/1442]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[510/1442]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[511/1442]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[512/1442]: training_loss: tensor(0.4614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[513/1442]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[514/1442]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[515/1442]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[516/1442]: training_loss: tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[517/1442]: training_loss: tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[518/1442]: training_loss: tensor(0.7755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[519/1442]: training_loss: tensor(0.5326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[520/1442]: training_loss: tensor(0.4264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[521/1442]: training_loss: tensor(0.8388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[522/1442]: training_loss: tensor(0.9355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[523/1442]: training_loss: tensor(0.7839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[524/1442]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[525/1442]: training_loss: tensor(0.3625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[526/1442]: training_loss: tensor(0.3894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[527/1442]: training_loss: tensor(0.4331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[528/1442]: training_loss: tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[529/1442]: training_loss: tensor(0.7359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[530/1442]: training_loss: tensor(0.3645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[531/1442]: training_loss: tensor(0.4403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[532/1442]: training_loss: tensor(0.4340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[533/1442]: training_loss: tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[534/1442]: training_loss: tensor(0.6818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[535/1442]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[536/1442]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[537/1442]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[538/1442]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[539/1442]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[540/1442]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[541/1442]: training_loss: tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[542/1442]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[543/1442]: training_loss: tensor(0.4839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[544/1442]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[545/1442]: training_loss: tensor(0.3348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[546/1442]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[547/1442]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[548/1442]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[549/1442]: training_loss: tensor(0.5082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[550/1442]: training_loss: tensor(0.3123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[551/1442]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[552/1442]: training_loss: tensor(0.4130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[553/1442]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[554/1442]: training_loss: tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[555/1442]: training_loss: tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[556/1442]: training_loss: tensor(0.8455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[557/1442]: training_loss: tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[558/1442]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[559/1442]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[560/1442]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[561/1442]: training_loss: tensor(0.4569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[562/1442]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[563/1442]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[564/1442]: training_loss: tensor(0.5006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[565/1442]: training_loss: tensor(0.4370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[566/1442]: training_loss: tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[567/1442]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[568/1442]: training_loss: tensor(0.4324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[569/1442]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[570/1442]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[571/1442]: training_loss: tensor(0.4926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[572/1442]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[573/1442]: training_loss: tensor(0.4977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[574/1442]: training_loss: tensor(0.4485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[575/1442]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[576/1442]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[577/1442]: training_loss: tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[578/1442]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[579/1442]: training_loss: tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[580/1442]: training_loss: tensor(0.3303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[581/1442]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[582/1442]: training_loss: tensor(0.7780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[583/1442]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[584/1442]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[585/1442]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[586/1442]: training_loss: tensor(0.4586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[587/1442]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[588/1442]: training_loss: tensor(0.4167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[589/1442]: training_loss: tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[590/1442]: training_loss: tensor(0.4202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[591/1442]: training_loss: tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[592/1442]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[593/1442]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[594/1442]: training_loss: tensor(0.7894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[595/1442]: training_loss: tensor(0.4782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[596/1442]: training_loss: tensor(0.4618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[597/1442]: training_loss: tensor(0.8307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[598/1442]: training_loss: tensor(0.5099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[599/1442]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[600/1442]: training_loss: tensor(0.7720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[601/1442]: training_loss: tensor(0.5592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[602/1442]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[603/1442]: training_loss: tensor(0.4045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[604/1442]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[605/1442]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[606/1442]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[607/1442]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[608/1442]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[609/1442]: training_loss: tensor(0.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[610/1442]: training_loss: tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[611/1442]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[612/1442]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[613/1442]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[614/1442]: training_loss: tensor(0.4749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[615/1442]: training_loss: tensor(0.4695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[616/1442]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[617/1442]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[618/1442]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[619/1442]: training_loss: tensor(0.4242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[620/1442]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[621/1442]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[622/1442]: training_loss: tensor(0.7437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[623/1442]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[624/1442]: training_loss: tensor(0.5951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[625/1442]: training_loss: tensor(0.4100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[626/1442]: training_loss: tensor(0.6724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[627/1442]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[628/1442]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[629/1442]: training_loss: tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[630/1442]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[631/1442]: training_loss: tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[632/1442]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[633/1442]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[634/1442]: training_loss: tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[635/1442]: training_loss: tensor(0.4920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[636/1442]: training_loss: tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[637/1442]: training_loss: tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[638/1442]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[639/1442]: training_loss: tensor(0.4608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[640/1442]: training_loss: tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[641/1442]: training_loss: tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[642/1442]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[643/1442]: training_loss: tensor(0.2837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[644/1442]: training_loss: tensor(0.4841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[645/1442]: training_loss: tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[646/1442]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[647/1442]: training_loss: tensor(0.4318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[648/1442]: training_loss: tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[649/1442]: training_loss: tensor(0.6638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[650/1442]: training_loss: tensor(0.4086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[651/1442]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[652/1442]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[653/1442]: training_loss: tensor(0.5114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[654/1442]: training_loss: tensor(0.2842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[655/1442]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[656/1442]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[657/1442]: training_loss: tensor(0.2918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[658/1442]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[659/1442]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[660/1442]: training_loss: tensor(0.4567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[661/1442]: training_loss: tensor(0.4287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[662/1442]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[663/1442]: training_loss: tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[664/1442]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[665/1442]: training_loss: tensor(0.4379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[666/1442]: training_loss: tensor(0.5563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[667/1442]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[668/1442]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[669/1442]: training_loss: tensor(0.7240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[670/1442]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[671/1442]: training_loss: tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[672/1442]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[673/1442]: training_loss: tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[674/1442]: training_loss: tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[675/1442]: training_loss: tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[676/1442]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[677/1442]: training_loss: tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[678/1442]: training_loss: tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[679/1442]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[680/1442]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[681/1442]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[682/1442]: training_loss: tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[683/1442]: training_loss: tensor(0.5589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[684/1442]: training_loss: tensor(0.2834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[685/1442]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[686/1442]: training_loss: tensor(0.6042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[687/1442]: training_loss: tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[688/1442]: training_loss: tensor(0.4243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[689/1442]: training_loss: tensor(0.5091, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[690/1442]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[691/1442]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[692/1442]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[693/1442]: training_loss: tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[694/1442]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[695/1442]: training_loss: tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[696/1442]: training_loss: tensor(0.4861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[697/1442]: training_loss: tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[698/1442]: training_loss: tensor(0.8089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[699/1442]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[700/1442]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[701/1442]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[702/1442]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[703/1442]: training_loss: tensor(0.4704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[704/1442]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[705/1442]: training_loss: tensor(0.3521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[706/1442]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[707/1442]: training_loss: tensor(0.5254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[708/1442]: training_loss: tensor(0.9272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[709/1442]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[710/1442]: training_loss: tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[711/1442]: training_loss: tensor(0.3611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[712/1442]: training_loss: tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[713/1442]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[714/1442]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[715/1442]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[716/1442]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[717/1442]: training_loss: tensor(0.3354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[718/1442]: training_loss: tensor(0.5695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[719/1442]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[720/1442]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[721/1442]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[722/1442]: training_loss: tensor(0.5134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[723/1442]: training_loss: tensor(0.2961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[724/1442]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[725/1442]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[726/1442]: training_loss: tensor(0.7240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[727/1442]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[728/1442]: training_loss: tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[729/1442]: training_loss: tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[730/1442]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[731/1442]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[732/1442]: training_loss: tensor(0.8119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[733/1442]: training_loss: tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[734/1442]: training_loss: tensor(0.5783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[735/1442]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[736/1442]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[737/1442]: training_loss: tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[738/1442]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[739/1442]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[740/1442]: training_loss: tensor(0.4656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[741/1442]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[742/1442]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[743/1442]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[744/1442]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[745/1442]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[746/1442]: training_loss: tensor(0.4502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[747/1442]: training_loss: tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[748/1442]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[749/1442]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[750/1442]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[751/1442]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[752/1442]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[753/1442]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[754/1442]: training_loss: tensor(0.4877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[755/1442]: training_loss: tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[756/1442]: training_loss: tensor(0.4491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[757/1442]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[758/1442]: training_loss: tensor(0.5094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[759/1442]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[760/1442]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[761/1442]: training_loss: tensor(0.3303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[762/1442]: training_loss: tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[763/1442]: training_loss: tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[764/1442]: training_loss: tensor(0.3656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[765/1442]: training_loss: tensor(0.4320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[766/1442]: training_loss: tensor(0.8563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[767/1442]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[768/1442]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[769/1442]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[770/1442]: training_loss: tensor(0.2915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[771/1442]: training_loss: tensor(0.5410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[772/1442]: training_loss: tensor(0.5111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[773/1442]: training_loss: tensor(0.8010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[774/1442]: training_loss: tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[775/1442]: training_loss: tensor(0.2819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[776/1442]: training_loss: tensor(0.5476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[777/1442]: training_loss: tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[778/1442]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[779/1442]: training_loss: tensor(0.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[780/1442]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[781/1442]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[782/1442]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[783/1442]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[784/1442]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[785/1442]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[786/1442]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[787/1442]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[788/1442]: training_loss: tensor(0.4919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[789/1442]: training_loss: tensor(0.6480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[790/1442]: training_loss: tensor(0.4163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[791/1442]: training_loss: tensor(0.4105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[792/1442]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[793/1442]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[794/1442]: training_loss: tensor(0.8550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[795/1442]: training_loss: tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[796/1442]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[797/1442]: training_loss: tensor(0.2897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[798/1442]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[799/1442]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[800/1442]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[801/1442]: training_loss: tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[802/1442]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[803/1442]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[804/1442]: training_loss: tensor(0.4385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[805/1442]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[806/1442]: training_loss: tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[807/1442]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[808/1442]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[809/1442]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[810/1442]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[811/1442]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[812/1442]: training_loss: tensor(0.5654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[813/1442]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[814/1442]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[815/1442]: training_loss: tensor(0.5379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[816/1442]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[817/1442]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[818/1442]: training_loss: tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[819/1442]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[820/1442]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[821/1442]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[822/1442]: training_loss: tensor(0.3842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[823/1442]: training_loss: tensor(0.7463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[824/1442]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[825/1442]: training_loss: tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[826/1442]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[827/1442]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[828/1442]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[829/1442]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[830/1442]: training_loss: tensor(0.7467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[831/1442]: training_loss: tensor(0.5278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[832/1442]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[833/1442]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[834/1442]: training_loss: tensor(0.7444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[835/1442]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[836/1442]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[837/1442]: training_loss: tensor(0.4659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[838/1442]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[839/1442]: training_loss: tensor(0.2801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[840/1442]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[841/1442]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[842/1442]: training_loss: tensor(0.4354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[843/1442]: training_loss: tensor(0.4872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[844/1442]: training_loss: tensor(0.6191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[845/1442]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[846/1442]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[847/1442]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[848/1442]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[849/1442]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[850/1442]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[851/1442]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[852/1442]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[853/1442]: training_loss: tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[854/1442]: training_loss: tensor(0.4369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[855/1442]: training_loss: tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[856/1442]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[857/1442]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[858/1442]: training_loss: tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[859/1442]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[860/1442]: training_loss: tensor(0.3554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[861/1442]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[862/1442]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[863/1442]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[864/1442]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[865/1442]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[866/1442]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[867/1442]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[868/1442]: training_loss: tensor(0.2619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[869/1442]: training_loss: tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[870/1442]: training_loss: tensor(0.4613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[871/1442]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[872/1442]: training_loss: tensor(0.4447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[873/1442]: training_loss: tensor(0.4671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[874/1442]: training_loss: tensor(0.4223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[875/1442]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[876/1442]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[877/1442]: training_loss: tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[878/1442]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[879/1442]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[880/1442]: training_loss: tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[881/1442]: training_loss: tensor(0.6544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[882/1442]: training_loss: tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[883/1442]: training_loss: tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[884/1442]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[885/1442]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[886/1442]: training_loss: tensor(0.9453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[887/1442]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[888/1442]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[889/1442]: training_loss: tensor(0.9062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[890/1442]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[891/1442]: training_loss: tensor(0.5917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[892/1442]: training_loss: tensor(0.2869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[893/1442]: training_loss: tensor(0.3348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[894/1442]: training_loss: tensor(0.4599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[895/1442]: training_loss: tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[896/1442]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[897/1442]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[898/1442]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[899/1442]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[900/1442]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[901/1442]: training_loss: tensor(0.4375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[902/1442]: training_loss: tensor(0.5124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[903/1442]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[904/1442]: training_loss: tensor(0.5701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[905/1442]: training_loss: tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[906/1442]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[907/1442]: training_loss: tensor(0.9992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[908/1442]: training_loss: tensor(0.4638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[909/1442]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[910/1442]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[911/1442]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[912/1442]: training_loss: tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[913/1442]: training_loss: tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[914/1442]: training_loss: tensor(0.4642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[915/1442]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[916/1442]: training_loss: tensor(0.5073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[917/1442]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[918/1442]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[919/1442]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[920/1442]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[921/1442]: training_loss: tensor(0.5566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[922/1442]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[923/1442]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[924/1442]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[925/1442]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[926/1442]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[927/1442]: training_loss: tensor(0.4441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[928/1442]: training_loss: tensor(1.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[929/1442]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[930/1442]: training_loss: tensor(0.4069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[931/1442]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[932/1442]: training_loss: tensor(0.1602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[933/1442]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[934/1442]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[935/1442]: training_loss: tensor(0.8637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[936/1442]: training_loss: tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[937/1442]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[938/1442]: training_loss: tensor(0.4841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[939/1442]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[940/1442]: training_loss: tensor(0.4953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[941/1442]: training_loss: tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[942/1442]: training_loss: tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[943/1442]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[944/1442]: training_loss: tensor(0.7176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[945/1442]: training_loss: tensor(0.4813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[946/1442]: training_loss: tensor(0.9102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[947/1442]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[948/1442]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[949/1442]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[950/1442]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[951/1442]: training_loss: tensor(0.4782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[952/1442]: training_loss: tensor(0.8328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[953/1442]: training_loss: tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[954/1442]: training_loss: tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[955/1442]: training_loss: tensor(0.5864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[956/1442]: training_loss: tensor(0.2684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[957/1442]: training_loss: tensor(0.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[958/1442]: training_loss: tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[959/1442]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[960/1442]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[961/1442]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[962/1442]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[963/1442]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[964/1442]: training_loss: tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[965/1442]: training_loss: tensor(0.1921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[966/1442]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[967/1442]: training_loss: tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[968/1442]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[969/1442]: training_loss: tensor(0.4306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[970/1442]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[971/1442]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[972/1442]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[973/1442]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[974/1442]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[975/1442]: training_loss: tensor(0.4903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[976/1442]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[977/1442]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[978/1442]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[979/1442]: training_loss: tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[980/1442]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[981/1442]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[982/1442]: training_loss: tensor(0.3565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[983/1442]: training_loss: tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[984/1442]: training_loss: tensor(0.3956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[985/1442]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[986/1442]: training_loss: tensor(0.4737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[987/1442]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[988/1442]: training_loss: tensor(0.5772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[989/1442]: training_loss: tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[990/1442]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[991/1442]: training_loss: tensor(0.4139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[992/1442]: training_loss: tensor(0.8060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[993/1442]: training_loss: tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[994/1442]: training_loss: tensor(0.9645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[995/1442]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[996/1442]: training_loss: tensor(0.6044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[997/1442]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[998/1442]: training_loss: tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[999/1442]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1000/1442]: training_loss: tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1001/1442]: training_loss: tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1002/1442]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1003/1442]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1004/1442]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1005/1442]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1006/1442]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1007/1442]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1008/1442]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1009/1442]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1010/1442]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1011/1442]: training_loss: tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1012/1442]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1013/1442]: training_loss: tensor(0.7479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1014/1442]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1015/1442]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1016/1442]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1017/1442]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1018/1442]: training_loss: tensor(0.9447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1019/1442]: training_loss: tensor(0.5534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1020/1442]: training_loss: tensor(0.4384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1021/1442]: training_loss: tensor(0.4890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1022/1442]: training_loss: tensor(0.5828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1023/1442]: training_loss: tensor(0.7224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1024/1442]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1025/1442]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1026/1442]: training_loss: tensor(0.2969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1027/1442]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1028/1442]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1029/1442]: training_loss: tensor(0.9274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1030/1442]: training_loss: tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1031/1442]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1032/1442]: training_loss: tensor(0.5244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1033/1442]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1034/1442]: training_loss: tensor(0.7856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1035/1442]: training_loss: tensor(0.9798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1036/1442]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1037/1442]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1038/1442]: training_loss: tensor(0.5686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1039/1442]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1040/1442]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1041/1442]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1042/1442]: training_loss: tensor(0.4270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1043/1442]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1044/1442]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1045/1442]: training_loss: tensor(0.7043, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1046/1442]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1047/1442]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1048/1442]: training_loss: tensor(0.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1049/1442]: training_loss: tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1050/1442]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1051/1442]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1052/1442]: training_loss: tensor(0.4270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1053/1442]: training_loss: tensor(1.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1054/1442]: training_loss: tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1055/1442]: training_loss: tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1056/1442]: training_loss: tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1057/1442]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1058/1442]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1059/1442]: training_loss: tensor(0.5472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1060/1442]: training_loss: tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1061/1442]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1062/1442]: training_loss: tensor(0.4424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1063/1442]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1064/1442]: training_loss: tensor(0.4897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1065/1442]: training_loss: tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1066/1442]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1067/1442]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1068/1442]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1069/1442]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1070/1442]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1071/1442]: training_loss: tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1072/1442]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1073/1442]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1074/1442]: training_loss: tensor(0.3540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1075/1442]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1076/1442]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1077/1442]: training_loss: tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1078/1442]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1079/1442]: training_loss: tensor(0.6338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1080/1442]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1081/1442]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1082/1442]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1083/1442]: training_loss: tensor(0.4583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1084/1442]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1085/1442]: training_loss: tensor(0.5282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1086/1442]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1087/1442]: training_loss: tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1088/1442]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1089/1442]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1090/1442]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1091/1442]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1092/1442]: training_loss: tensor(0.5223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1093/1442]: training_loss: tensor(0.4288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1094/1442]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1095/1442]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1096/1442]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1097/1442]: training_loss: tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1098/1442]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1099/1442]: training_loss: tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1100/1442]: training_loss: tensor(0.6684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1101/1442]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1102/1442]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1103/1442]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1104/1442]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1105/1442]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1106/1442]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1107/1442]: training_loss: tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1108/1442]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1109/1442]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1110/1442]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1111/1442]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1112/1442]: training_loss: tensor(0.8775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1113/1442]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1114/1442]: training_loss: tensor(0.4683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1115/1442]: training_loss: tensor(0.9263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1116/1442]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1117/1442]: training_loss: tensor(0.4413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1118/1442]: training_loss: tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1119/1442]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1120/1442]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1121/1442]: training_loss: tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1122/1442]: training_loss: tensor(0.5946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1123/1442]: training_loss: tensor(0.2961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1124/1442]: training_loss: tensor(0.4160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1125/1442]: training_loss: tensor(0.4299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1126/1442]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1127/1442]: training_loss: tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1128/1442]: training_loss: tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1129/1442]: training_loss: tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1130/1442]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1131/1442]: training_loss: tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1132/1442]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1133/1442]: training_loss: tensor(0.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1134/1442]: training_loss: tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1135/1442]: training_loss: tensor(0.5128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1136/1442]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1137/1442]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1138/1442]: training_loss: tensor(0.7388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1139/1442]: training_loss: tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1140/1442]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1141/1442]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1142/1442]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1143/1442]: training_loss: tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1144/1442]: training_loss: tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1145/1442]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1146/1442]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1147/1442]: training_loss: tensor(0.2782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1148/1442]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1149/1442]: training_loss: tensor(0.7312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1150/1442]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1151/1442]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1152/1442]: training_loss: tensor(0.4773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1153/1442]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1154/1442]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1155/1442]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1156/1442]: training_loss: tensor(0.1633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1157/1442]: training_loss: tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1158/1442]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1159/1442]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1160/1442]: training_loss: tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1161/1442]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1162/1442]: training_loss: tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1163/1442]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1164/1442]: training_loss: tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1165/1442]: training_loss: tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1166/1442]: training_loss: tensor(0.4888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1167/1442]: training_loss: tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1168/1442]: training_loss: tensor(0.4971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1169/1442]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1170/1442]: training_loss: tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1171/1442]: training_loss: tensor(0.7378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1172/1442]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1173/1442]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1174/1442]: training_loss: tensor(0.2085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1175/1442]: training_loss: tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1176/1442]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1177/1442]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1178/1442]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1179/1442]: training_loss: tensor(0.7180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1180/1442]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1181/1442]: training_loss: tensor(0.4404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1182/1442]: training_loss: tensor(0.3452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1183/1442]: training_loss: tensor(0.4662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1184/1442]: training_loss: tensor(0.9277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1185/1442]: training_loss: tensor(0.3796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1186/1442]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1187/1442]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1188/1442]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1189/1442]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1190/1442]: training_loss: tensor(0.3570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1191/1442]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1192/1442]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1193/1442]: training_loss: tensor(0.7704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1194/1442]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1195/1442]: training_loss: tensor(0.3396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1196/1442]: training_loss: tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1197/1442]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1198/1442]: training_loss: tensor(0.6514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1199/1442]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1200/1442]: training_loss: tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1201/1442]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1202/1442]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1203/1442]: training_loss: tensor(0.3337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1204/1442]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1205/1442]: training_loss: tensor(0.5094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1206/1442]: training_loss: tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1207/1442]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1208/1442]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1209/1442]: training_loss: tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1210/1442]: training_loss: tensor(0.5882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1211/1442]: training_loss: tensor(0.5246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1212/1442]: training_loss: tensor(0.4498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1213/1442]: training_loss: tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1214/1442]: training_loss: tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1215/1442]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1216/1442]: training_loss: tensor(0.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1217/1442]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1218/1442]: training_loss: tensor(0.2505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1219/1442]: training_loss: tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1220/1442]: training_loss: tensor(0.5639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1221/1442]: training_loss: tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1222/1442]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1223/1442]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1224/1442]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1225/1442]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1226/1442]: training_loss: tensor(0.4804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1227/1442]: training_loss: tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1228/1442]: training_loss: tensor(0.4980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1229/1442]: training_loss: tensor(0.2505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1230/1442]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1231/1442]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1232/1442]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1233/1442]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1234/1442]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1235/1442]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1236/1442]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1237/1442]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1238/1442]: training_loss: tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1239/1442]: training_loss: tensor(0.3372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1240/1442]: training_loss: tensor(0.4562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1241/1442]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1242/1442]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1243/1442]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1244/1442]: training_loss: tensor(0.5100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1245/1442]: training_loss: tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1246/1442]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1247/1442]: training_loss: tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1248/1442]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1249/1442]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1250/1442]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1251/1442]: training_loss: tensor(0.5333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1252/1442]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1253/1442]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1254/1442]: training_loss: tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1255/1442]: training_loss: tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1256/1442]: training_loss: tensor(0.2810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1257/1442]: training_loss: tensor(0.3555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1258/1442]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1259/1442]: training_loss: tensor(0.4086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1260/1442]: training_loss: tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1261/1442]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1262/1442]: training_loss: tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1263/1442]: training_loss: tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1264/1442]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1265/1442]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1266/1442]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1267/1442]: training_loss: tensor(0.4323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1268/1442]: training_loss: tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1269/1442]: training_loss: tensor(0.8286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1270/1442]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1271/1442]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1272/1442]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1273/1442]: training_loss: tensor(0.4574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1274/1442]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1275/1442]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1276/1442]: training_loss: tensor(0.7452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1277/1442]: training_loss: tensor(0.4600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1278/1442]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1279/1442]: training_loss: tensor(0.4267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1280/1442]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1281/1442]: training_loss: tensor(0.3762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1282/1442]: training_loss: tensor(0.2883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1283/1442]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1284/1442]: training_loss: tensor(0.2892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1285/1442]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1286/1442]: training_loss: tensor(0.4239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1287/1442]: training_loss: tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1288/1442]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1289/1442]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1290/1442]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1291/1442]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1292/1442]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1293/1442]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1294/1442]: training_loss: tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1295/1442]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1296/1442]: training_loss: tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1297/1442]: training_loss: tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1298/1442]: training_loss: tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1299/1442]: training_loss: tensor(0.2809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1300/1442]: training_loss: tensor(0.5172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1301/1442]: training_loss: tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1302/1442]: training_loss: tensor(0.5410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1303/1442]: training_loss: tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1304/1442]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1305/1442]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1306/1442]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1307/1442]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1308/1442]: training_loss: tensor(0.4512, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1309/1442]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1310/1442]: training_loss: tensor(0.6078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1311/1442]: training_loss: tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1312/1442]: training_loss: tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1313/1442]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1314/1442]: training_loss: tensor(0.3462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1315/1442]: training_loss: tensor(1.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1316/1442]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1317/1442]: training_loss: tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1318/1442]: training_loss: tensor(0.3428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1319/1442]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1320/1442]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1321/1442]: training_loss: tensor(0.5926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1322/1442]: training_loss: tensor(0.8998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1323/1442]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1324/1442]: training_loss: tensor(0.4920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1325/1442]: training_loss: tensor(0.4822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1326/1442]: training_loss: tensor(0.9640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1327/1442]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1328/1442]: training_loss: tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1329/1442]: training_loss: tensor(0.5476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1330/1442]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1331/1442]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1332/1442]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1333/1442]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1334/1442]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1335/1442]: training_loss: tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1336/1442]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1337/1442]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1338/1442]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1339/1442]: training_loss: tensor(0.8587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1340/1442]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1341/1442]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1342/1442]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1343/1442]: training_loss: tensor(0.3807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1344/1442]: training_loss: tensor(0.4233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1345/1442]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1346/1442]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1347/1442]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1348/1442]: training_loss: tensor(0.4065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1349/1442]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1350/1442]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1351/1442]: training_loss: tensor(0.5357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1352/1442]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1353/1442]: training_loss: tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1354/1442]: training_loss: tensor(0.3857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1355/1442]: training_loss: tensor(0.4418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1356/1442]: training_loss: tensor(0.2138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1357/1442]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1358/1442]: training_loss: tensor(0.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1359/1442]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1360/1442]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1361/1442]: training_loss: tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1362/1442]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1363/1442]: training_loss: tensor(0.7704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1364/1442]: training_loss: tensor(0.5310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1365/1442]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1366/1442]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1367/1442]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1368/1442]: training_loss: tensor(0.8127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1369/1442]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1370/1442]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1371/1442]: training_loss: tensor(0.5648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1372/1442]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1373/1442]: training_loss: tensor(0.9235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1374/1442]: training_loss: tensor(0.8238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1375/1442]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1376/1442]: training_loss: tensor(0.4274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1377/1442]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1378/1442]: training_loss: tensor(0.3685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1379/1442]: training_loss: tensor(0.3264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1380/1442]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1381/1442]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1382/1442]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1383/1442]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1384/1442]: training_loss: tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1385/1442]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1386/1442]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1387/1442]: training_loss: tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1388/1442]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1389/1442]: training_loss: tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1390/1442]: training_loss: tensor(0.3674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1391/1442]: training_loss: tensor(0.1789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1392/1442]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1393/1442]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1394/1442]: training_loss: tensor(0.5000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1395/1442]: training_loss: tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1396/1442]: training_loss: tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1397/1442]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1398/1442]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1399/1442]: training_loss: tensor(0.4231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1400/1442]: training_loss: tensor(0.4105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1401/1442]: training_loss: tensor(0.8038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1402/1442]: training_loss: tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1403/1442]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1404/1442]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1405/1442]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1406/1442]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1407/1442]: training_loss: tensor(0.5675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1408/1442]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1409/1442]: training_loss: tensor(0.8256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1410/1442]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1411/1442]: training_loss: tensor(0.6392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1412/1442]: training_loss: tensor(0.3109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1413/1442]: training_loss: tensor(0.4064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1414/1442]: training_loss: tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1415/1442]: training_loss: tensor(0.4379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1416/1442]: training_loss: tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1417/1442]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1418/1442]: training_loss: tensor(0.2887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1419/1442]: training_loss: tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1420/1442]: training_loss: tensor(0.5112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1421/1442]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1422/1442]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1423/1442]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1424/1442]: training_loss: tensor(0.5481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1425/1442]: training_loss: tensor(0.3430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1426/1442]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1427/1442]: training_loss: tensor(0.7217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1428/1442]: training_loss: tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1429/1442]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1430/1442]: training_loss: tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1431/1442]: training_loss: tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1432/1442]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1433/1442]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1434/1442]: training_loss: tensor(0.4478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1435/1442]: training_loss: tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1436/1442]: training_loss: tensor(0.4343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1437/1442]: training_loss: tensor(0.4969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1438/1442]: training_loss: tensor(0.5969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1439/1442]: training_loss: tensor(0.8486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1440/1442]: training_loss: tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1441/1442]: training_loss: tensor(1.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/5],global step [4326/7210], pt loss:0.0000, dev loss:0.5317\n",
      "batch_no[0/1442]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1/1442]: training_loss: tensor(0.5479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[2/1442]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[3/1442]: training_loss: tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[4/1442]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[5/1442]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[6/1442]: training_loss: tensor(0.5047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[7/1442]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[8/1442]: training_loss: tensor(0.5686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[9/1442]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[10/1442]: training_loss: tensor(0.5412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[11/1442]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[12/1442]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[13/1442]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[14/1442]: training_loss: tensor(0.5078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[15/1442]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[16/1442]: training_loss: tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[17/1442]: training_loss: tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[18/1442]: training_loss: tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[19/1442]: training_loss: tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[20/1442]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[21/1442]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[22/1442]: training_loss: tensor(0.4313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[23/1442]: training_loss: tensor(0.2542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[24/1442]: training_loss: tensor(0.5409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[25/1442]: training_loss: tensor(0.4278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[26/1442]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[27/1442]: training_loss: tensor(0.4430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[28/1442]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[29/1442]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[30/1442]: training_loss: tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[31/1442]: training_loss: tensor(0.6181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[32/1442]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[33/1442]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[34/1442]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[35/1442]: training_loss: tensor(0.7241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[36/1442]: training_loss: tensor(0.4677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[37/1442]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[38/1442]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[39/1442]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[40/1442]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[41/1442]: training_loss: tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[42/1442]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[43/1442]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[44/1442]: training_loss: tensor(0.2937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[45/1442]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[46/1442]: training_loss: tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[47/1442]: training_loss: tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[48/1442]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[49/1442]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[50/1442]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[51/1442]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[52/1442]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[53/1442]: training_loss: tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[54/1442]: training_loss: tensor(0.4412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[55/1442]: training_loss: tensor(0.5892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[56/1442]: training_loss: tensor(1.5198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[57/1442]: training_loss: tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[58/1442]: training_loss: tensor(0.8859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[59/1442]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[60/1442]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[61/1442]: training_loss: tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[62/1442]: training_loss: tensor(0.4575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[63/1442]: training_loss: tensor(0.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[64/1442]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[65/1442]: training_loss: tensor(0.4427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[66/1442]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[67/1442]: training_loss: tensor(0.4603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[68/1442]: training_loss: tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[69/1442]: training_loss: tensor(0.6175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[70/1442]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[71/1442]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[72/1442]: training_loss: tensor(0.2642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[73/1442]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[74/1442]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[75/1442]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[76/1442]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[77/1442]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[78/1442]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[79/1442]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[80/1442]: training_loss: tensor(0.4992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[81/1442]: training_loss: tensor(0.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[82/1442]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[83/1442]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[84/1442]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[85/1442]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[86/1442]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[87/1442]: training_loss: tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[88/1442]: training_loss: tensor(0.6249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[89/1442]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[90/1442]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[91/1442]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[92/1442]: training_loss: tensor(0.2904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[93/1442]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[94/1442]: training_loss: tensor(0.4689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[95/1442]: training_loss: tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[96/1442]: training_loss: tensor(0.4704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[97/1442]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[98/1442]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[99/1442]: training_loss: tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[100/1442]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[101/1442]: training_loss: tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[102/1442]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[103/1442]: training_loss: tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[104/1442]: training_loss: tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[105/1442]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[106/1442]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[107/1442]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[108/1442]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[109/1442]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[110/1442]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[111/1442]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[112/1442]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[113/1442]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[114/1442]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[115/1442]: training_loss: tensor(0.5067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[116/1442]: training_loss: tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[117/1442]: training_loss: tensor(0.4341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[118/1442]: training_loss: tensor(0.3726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[119/1442]: training_loss: tensor(0.6103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[120/1442]: training_loss: tensor(0.4482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[121/1442]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[122/1442]: training_loss: tensor(0.5601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[123/1442]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[124/1442]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[125/1442]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[126/1442]: training_loss: tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[127/1442]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[128/1442]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[129/1442]: training_loss: tensor(0.3444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[130/1442]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[131/1442]: training_loss: tensor(0.4299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[132/1442]: training_loss: tensor(0.8780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[133/1442]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[134/1442]: training_loss: tensor(0.5004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[135/1442]: training_loss: tensor(0.4737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[136/1442]: training_loss: tensor(1.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[137/1442]: training_loss: tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[138/1442]: training_loss: tensor(0.8878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[139/1442]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[140/1442]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[141/1442]: training_loss: tensor(0.5326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[142/1442]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[143/1442]: training_loss: tensor(0.8320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[144/1442]: training_loss: tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[145/1442]: training_loss: tensor(0.4387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[146/1442]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[147/1442]: training_loss: tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[148/1442]: training_loss: tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[149/1442]: training_loss: tensor(0.4364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[150/1442]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[151/1442]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[152/1442]: training_loss: tensor(0.4688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[153/1442]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[154/1442]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[155/1442]: training_loss: tensor(0.5104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[156/1442]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[157/1442]: training_loss: tensor(0.6504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[158/1442]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[159/1442]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[160/1442]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[161/1442]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[162/1442]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[163/1442]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[164/1442]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[165/1442]: training_loss: tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[166/1442]: training_loss: tensor(0.7075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[167/1442]: training_loss: tensor(0.7435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[168/1442]: training_loss: tensor(0.5345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[169/1442]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[170/1442]: training_loss: tensor(0.4669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[171/1442]: training_loss: tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[172/1442]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[173/1442]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[174/1442]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[175/1442]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[176/1442]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[177/1442]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[178/1442]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[179/1442]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[180/1442]: training_loss: tensor(0.7548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[181/1442]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[182/1442]: training_loss: tensor(0.6531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[183/1442]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[184/1442]: training_loss: tensor(0.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[185/1442]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[186/1442]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[187/1442]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[188/1442]: training_loss: tensor(0.4282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[189/1442]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[190/1442]: training_loss: tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[191/1442]: training_loss: tensor(0.4428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[192/1442]: training_loss: tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[193/1442]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[194/1442]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[195/1442]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[196/1442]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[197/1442]: training_loss: tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[198/1442]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[199/1442]: training_loss: tensor(0.4771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[200/1442]: training_loss: tensor(0.4150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[201/1442]: training_loss: tensor(0.6009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[202/1442]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[203/1442]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[204/1442]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[205/1442]: training_loss: tensor(0.4316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[206/1442]: training_loss: tensor(0.6552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[207/1442]: training_loss: tensor(0.4777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[208/1442]: training_loss: tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[209/1442]: training_loss: tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[210/1442]: training_loss: tensor(0.4819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[211/1442]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[212/1442]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[213/1442]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[214/1442]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[215/1442]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[216/1442]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[217/1442]: training_loss: tensor(0.2099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[218/1442]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[219/1442]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[220/1442]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[221/1442]: training_loss: tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[222/1442]: training_loss: tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[223/1442]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[224/1442]: training_loss: tensor(0.4440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[225/1442]: training_loss: tensor(1.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[226/1442]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[227/1442]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[228/1442]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[229/1442]: training_loss: tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[230/1442]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[231/1442]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[232/1442]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[233/1442]: training_loss: tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[234/1442]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[235/1442]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[236/1442]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[237/1442]: training_loss: tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[238/1442]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[239/1442]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[240/1442]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[241/1442]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[242/1442]: training_loss: tensor(0.4680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[243/1442]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[244/1442]: training_loss: tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[245/1442]: training_loss: tensor(0.4328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[246/1442]: training_loss: tensor(0.4549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[247/1442]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[248/1442]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[249/1442]: training_loss: tensor(0.8140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[250/1442]: training_loss: tensor(0.4692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[251/1442]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[252/1442]: training_loss: tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[253/1442]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[254/1442]: training_loss: tensor(0.4207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[255/1442]: training_loss: tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[256/1442]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[257/1442]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[258/1442]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[259/1442]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[260/1442]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[261/1442]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[262/1442]: training_loss: tensor(0.5469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[263/1442]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[264/1442]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[265/1442]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[266/1442]: training_loss: tensor(0.3158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[267/1442]: training_loss: tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[268/1442]: training_loss: tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[269/1442]: training_loss: tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[270/1442]: training_loss: tensor(0.4438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[271/1442]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[272/1442]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[273/1442]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[274/1442]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[275/1442]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[276/1442]: training_loss: tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[277/1442]: training_loss: tensor(0.6015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[278/1442]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[279/1442]: training_loss: tensor(0.7235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[280/1442]: training_loss: tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[281/1442]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[282/1442]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[283/1442]: training_loss: tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[284/1442]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[285/1442]: training_loss: tensor(0.1702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[286/1442]: training_loss: tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[287/1442]: training_loss: tensor(0.6202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[288/1442]: training_loss: tensor(0.4912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[289/1442]: training_loss: tensor(0.1864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[290/1442]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[291/1442]: training_loss: tensor(0.5282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[292/1442]: training_loss: tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[293/1442]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[294/1442]: training_loss: tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[295/1442]: training_loss: tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[296/1442]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[297/1442]: training_loss: tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[298/1442]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[299/1442]: training_loss: tensor(0.2766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[300/1442]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[301/1442]: training_loss: tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[302/1442]: training_loss: tensor(0.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[303/1442]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[304/1442]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[305/1442]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[306/1442]: training_loss: tensor(0.4409, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[307/1442]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[308/1442]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[309/1442]: training_loss: tensor(0.3656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[310/1442]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[311/1442]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[312/1442]: training_loss: tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[313/1442]: training_loss: tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[314/1442]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[315/1442]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[316/1442]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[317/1442]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[318/1442]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[319/1442]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[320/1442]: training_loss: tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[321/1442]: training_loss: tensor(0.7214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[322/1442]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[323/1442]: training_loss: tensor(1.3352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[324/1442]: training_loss: tensor(0.9355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[325/1442]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[326/1442]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[327/1442]: training_loss: tensor(0.4234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[328/1442]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[329/1442]: training_loss: tensor(0.4145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[330/1442]: training_loss: tensor(1.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[331/1442]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[332/1442]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[333/1442]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[334/1442]: training_loss: tensor(0.4579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[335/1442]: training_loss: tensor(0.4264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[336/1442]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[337/1442]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[338/1442]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[339/1442]: training_loss: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[340/1442]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[341/1442]: training_loss: tensor(0.7411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[342/1442]: training_loss: tensor(0.4241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[343/1442]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[344/1442]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[345/1442]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[346/1442]: training_loss: tensor(1.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[347/1442]: training_loss: tensor(0.5966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[348/1442]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[349/1442]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[350/1442]: training_loss: tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[351/1442]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[352/1442]: training_loss: tensor(0.5877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[353/1442]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[354/1442]: training_loss: tensor(0.4284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[355/1442]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[356/1442]: training_loss: tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[357/1442]: training_loss: tensor(0.2945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[358/1442]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[359/1442]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[360/1442]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[361/1442]: training_loss: tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[362/1442]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[363/1442]: training_loss: tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[364/1442]: training_loss: tensor(0.7445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[365/1442]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[366/1442]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[367/1442]: training_loss: tensor(0.3348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[368/1442]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[369/1442]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[370/1442]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[371/1442]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[372/1442]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[373/1442]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[374/1442]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[375/1442]: training_loss: tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[376/1442]: training_loss: tensor(0.4527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[377/1442]: training_loss: tensor(0.7870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[378/1442]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[379/1442]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[380/1442]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[381/1442]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[382/1442]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[383/1442]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[384/1442]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[385/1442]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[386/1442]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[387/1442]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[388/1442]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[389/1442]: training_loss: tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[390/1442]: training_loss: tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[391/1442]: training_loss: tensor(0.8408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[392/1442]: training_loss: tensor(0.5732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[393/1442]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[394/1442]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[395/1442]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[396/1442]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[397/1442]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[398/1442]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[399/1442]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[400/1442]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[401/1442]: training_loss: tensor(0.5100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[402/1442]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[403/1442]: training_loss: tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[404/1442]: training_loss: tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[405/1442]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[406/1442]: training_loss: tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[407/1442]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[408/1442]: training_loss: tensor(0.2617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[409/1442]: training_loss: tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[410/1442]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[411/1442]: training_loss: tensor(0.5894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[412/1442]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[413/1442]: training_loss: tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[414/1442]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[415/1442]: training_loss: tensor(0.7790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[416/1442]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[417/1442]: training_loss: tensor(0.9788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[418/1442]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[419/1442]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[420/1442]: training_loss: tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[421/1442]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[422/1442]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[423/1442]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[424/1442]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[425/1442]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[426/1442]: training_loss: tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[427/1442]: training_loss: tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[428/1442]: training_loss: tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[429/1442]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[430/1442]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[431/1442]: training_loss: tensor(0.5608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[432/1442]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[433/1442]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[434/1442]: training_loss: tensor(0.4768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[435/1442]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[436/1442]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[437/1442]: training_loss: tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[438/1442]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[439/1442]: training_loss: tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[440/1442]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[441/1442]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[442/1442]: training_loss: tensor(0.5367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[443/1442]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[444/1442]: training_loss: tensor(0.5023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[445/1442]: training_loss: tensor(0.2962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[446/1442]: training_loss: tensor(0.4800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[447/1442]: training_loss: tensor(0.3112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[448/1442]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[449/1442]: training_loss: tensor(0.7692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[450/1442]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[451/1442]: training_loss: tensor(0.3347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[452/1442]: training_loss: tensor(0.7092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[453/1442]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[454/1442]: training_loss: tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[455/1442]: training_loss: tensor(0.7840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[456/1442]: training_loss: tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[457/1442]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[458/1442]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[459/1442]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[460/1442]: training_loss: tensor(0.2900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[461/1442]: training_loss: tensor(0.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[462/1442]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[463/1442]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[464/1442]: training_loss: tensor(0.3871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[465/1442]: training_loss: tensor(0.5118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[466/1442]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[467/1442]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[468/1442]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[469/1442]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[470/1442]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[471/1442]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[472/1442]: training_loss: tensor(0.9334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[473/1442]: training_loss: tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[474/1442]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[475/1442]: training_loss: tensor(0.3581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[476/1442]: training_loss: tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[477/1442]: training_loss: tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[478/1442]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[479/1442]: training_loss: tensor(0.4903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[480/1442]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[481/1442]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[482/1442]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[483/1442]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[484/1442]: training_loss: tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[485/1442]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[486/1442]: training_loss: tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[487/1442]: training_loss: tensor(0.8806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[488/1442]: training_loss: tensor(0.4524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[489/1442]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[490/1442]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[491/1442]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[492/1442]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[493/1442]: training_loss: tensor(0.8203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[494/1442]: training_loss: tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[495/1442]: training_loss: tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[496/1442]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[497/1442]: training_loss: tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[498/1442]: training_loss: tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[499/1442]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[500/1442]: training_loss: tensor(0.3234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[501/1442]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[502/1442]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[503/1442]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[504/1442]: training_loss: tensor(0.4346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[505/1442]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[506/1442]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[507/1442]: training_loss: tensor(0.2547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[508/1442]: training_loss: tensor(0.7682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[509/1442]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[510/1442]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[511/1442]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[512/1442]: training_loss: tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[513/1442]: training_loss: tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[514/1442]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[515/1442]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[516/1442]: training_loss: tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[517/1442]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[518/1442]: training_loss: tensor(0.5661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[519/1442]: training_loss: tensor(0.5035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[520/1442]: training_loss: tensor(0.4252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[521/1442]: training_loss: tensor(0.4971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[522/1442]: training_loss: tensor(0.8431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[523/1442]: training_loss: tensor(0.5636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[524/1442]: training_loss: tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[525/1442]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[526/1442]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[527/1442]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[528/1442]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[529/1442]: training_loss: tensor(0.5836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[530/1442]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[531/1442]: training_loss: tensor(0.5149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[532/1442]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[533/1442]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[534/1442]: training_loss: tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[535/1442]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[536/1442]: training_loss: tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[537/1442]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[538/1442]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[539/1442]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[540/1442]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[541/1442]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[542/1442]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[543/1442]: training_loss: tensor(0.5869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[544/1442]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[545/1442]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[546/1442]: training_loss: tensor(0.4248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[547/1442]: training_loss: tensor(0.6045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[548/1442]: training_loss: tensor(0.3379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[549/1442]: training_loss: tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[550/1442]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[551/1442]: training_loss: tensor(0.4218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[552/1442]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[553/1442]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[554/1442]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[555/1442]: training_loss: tensor(0.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[556/1442]: training_loss: tensor(0.8798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[557/1442]: training_loss: tensor(0.3219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[558/1442]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[559/1442]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[560/1442]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[561/1442]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[562/1442]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[563/1442]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[564/1442]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[565/1442]: training_loss: tensor(0.4209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[566/1442]: training_loss: tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[567/1442]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[568/1442]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[569/1442]: training_loss: tensor(0.2633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[570/1442]: training_loss: tensor(0.3304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[571/1442]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[572/1442]: training_loss: tensor(0.2842, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[573/1442]: training_loss: tensor(0.4489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[574/1442]: training_loss: tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[575/1442]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[576/1442]: training_loss: tensor(0.3452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[577/1442]: training_loss: tensor(0.5136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[578/1442]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[579/1442]: training_loss: tensor(0.5375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[580/1442]: training_loss: tensor(0.4205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[581/1442]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[582/1442]: training_loss: tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[583/1442]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[584/1442]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[585/1442]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[586/1442]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[587/1442]: training_loss: tensor(0.2793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[588/1442]: training_loss: tensor(0.4153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[589/1442]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[590/1442]: training_loss: tensor(0.4376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[591/1442]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[592/1442]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[593/1442]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[594/1442]: training_loss: tensor(0.5218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[595/1442]: training_loss: tensor(0.5149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[596/1442]: training_loss: tensor(0.4608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[597/1442]: training_loss: tensor(1.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[598/1442]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[599/1442]: training_loss: tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[600/1442]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[601/1442]: training_loss: tensor(0.6113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[602/1442]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[603/1442]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[604/1442]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[605/1442]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[606/1442]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[607/1442]: training_loss: tensor(0.1534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[608/1442]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[609/1442]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[610/1442]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[611/1442]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[612/1442]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[613/1442]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[614/1442]: training_loss: tensor(0.3489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[615/1442]: training_loss: tensor(0.4897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[616/1442]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[617/1442]: training_loss: tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[618/1442]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[619/1442]: training_loss: tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[620/1442]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[621/1442]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[622/1442]: training_loss: tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[623/1442]: training_loss: tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[624/1442]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[625/1442]: training_loss: tensor(0.4812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[626/1442]: training_loss: tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[627/1442]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[628/1442]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[629/1442]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[630/1442]: training_loss: tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[631/1442]: training_loss: tensor(0.6530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[632/1442]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[633/1442]: training_loss: tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[634/1442]: training_loss: tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[635/1442]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[636/1442]: training_loss: tensor(0.3396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[637/1442]: training_loss: tensor(0.5305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[638/1442]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[639/1442]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[640/1442]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[641/1442]: training_loss: tensor(0.4742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[642/1442]: training_loss: tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[643/1442]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[644/1442]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[645/1442]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[646/1442]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[647/1442]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[648/1442]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[649/1442]: training_loss: tensor(0.6012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[650/1442]: training_loss: tensor(0.3320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[651/1442]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[652/1442]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[653/1442]: training_loss: tensor(0.5276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[654/1442]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[655/1442]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[656/1442]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[657/1442]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[658/1442]: training_loss: tensor(0.5703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[659/1442]: training_loss: tensor(0.3588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[660/1442]: training_loss: tensor(0.4485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[661/1442]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[662/1442]: training_loss: tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[663/1442]: training_loss: tensor(0.4703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[664/1442]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[665/1442]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[666/1442]: training_loss: tensor(0.5363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[667/1442]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[668/1442]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[669/1442]: training_loss: tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[670/1442]: training_loss: tensor(0.2790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[671/1442]: training_loss: tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[672/1442]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[673/1442]: training_loss: tensor(0.5199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[674/1442]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[675/1442]: training_loss: tensor(0.4894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[676/1442]: training_loss: tensor(0.4857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[677/1442]: training_loss: tensor(0.5998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[678/1442]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[679/1442]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[680/1442]: training_loss: tensor(0.4796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[681/1442]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[682/1442]: training_loss: tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[683/1442]: training_loss: tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[684/1442]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[685/1442]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[686/1442]: training_loss: tensor(0.6052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[687/1442]: training_loss: tensor(0.5023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[688/1442]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[689/1442]: training_loss: tensor(0.6138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[690/1442]: training_loss: tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[691/1442]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[692/1442]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[693/1442]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[694/1442]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[695/1442]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[696/1442]: training_loss: tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[697/1442]: training_loss: tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[698/1442]: training_loss: tensor(0.8164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[699/1442]: training_loss: tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[700/1442]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[701/1442]: training_loss: tensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[702/1442]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[703/1442]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[704/1442]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[705/1442]: training_loss: tensor(0.2919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[706/1442]: training_loss: tensor(0.4000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[707/1442]: training_loss: tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[708/1442]: training_loss: tensor(0.9459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[709/1442]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[710/1442]: training_loss: tensor(0.6699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[711/1442]: training_loss: tensor(0.4577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[712/1442]: training_loss: tensor(0.3441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[713/1442]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[714/1442]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[715/1442]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[716/1442]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[717/1442]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[718/1442]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[719/1442]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[720/1442]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[721/1442]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[722/1442]: training_loss: tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[723/1442]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[724/1442]: training_loss: tensor(0.5366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[725/1442]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[726/1442]: training_loss: tensor(0.8109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[727/1442]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[728/1442]: training_loss: tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[729/1442]: training_loss: tensor(0.8643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[730/1442]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[731/1442]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[732/1442]: training_loss: tensor(0.4202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[733/1442]: training_loss: tensor(0.5934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[734/1442]: training_loss: tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[735/1442]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[736/1442]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[737/1442]: training_loss: tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[738/1442]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[739/1442]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[740/1442]: training_loss: tensor(0.4749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[741/1442]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[742/1442]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[743/1442]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[744/1442]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[745/1442]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[746/1442]: training_loss: tensor(0.4885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[747/1442]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[748/1442]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[749/1442]: training_loss: tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[750/1442]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[751/1442]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[752/1442]: training_loss: tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[753/1442]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[754/1442]: training_loss: tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[755/1442]: training_loss: tensor(0.2719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[756/1442]: training_loss: tensor(0.4575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[757/1442]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[758/1442]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[759/1442]: training_loss: tensor(0.4985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[760/1442]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[761/1442]: training_loss: tensor(0.2100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[762/1442]: training_loss: tensor(0.5186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[763/1442]: training_loss: tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[764/1442]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[765/1442]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[766/1442]: training_loss: tensor(0.8419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[767/1442]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[768/1442]: training_loss: tensor(0.4063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[769/1442]: training_loss: tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[770/1442]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[771/1442]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[772/1442]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[773/1442]: training_loss: tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[774/1442]: training_loss: tensor(0.5048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[775/1442]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[776/1442]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[777/1442]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[778/1442]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[779/1442]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[780/1442]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[781/1442]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[782/1442]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[783/1442]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[784/1442]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[785/1442]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[786/1442]: training_loss: tensor(0.3697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[787/1442]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[788/1442]: training_loss: tensor(0.4921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[789/1442]: training_loss: tensor(0.6007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[790/1442]: training_loss: tensor(0.5425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[791/1442]: training_loss: tensor(0.4105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[792/1442]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[793/1442]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[794/1442]: training_loss: tensor(0.5732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[795/1442]: training_loss: tensor(0.7478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[796/1442]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[797/1442]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[798/1442]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[799/1442]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[800/1442]: training_loss: tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[801/1442]: training_loss: tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[802/1442]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[803/1442]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[804/1442]: training_loss: tensor(0.5425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[805/1442]: training_loss: tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[806/1442]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[807/1442]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[808/1442]: training_loss: tensor(0.4107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[809/1442]: training_loss: tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[810/1442]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[811/1442]: training_loss: tensor(0.4553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[812/1442]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[813/1442]: training_loss: tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[814/1442]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[815/1442]: training_loss: tensor(0.4213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[816/1442]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[817/1442]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[818/1442]: training_loss: tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[819/1442]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[820/1442]: training_loss: tensor(0.4514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[821/1442]: training_loss: tensor(0.4688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[822/1442]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[823/1442]: training_loss: tensor(0.4342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[824/1442]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[825/1442]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[826/1442]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[827/1442]: training_loss: tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[828/1442]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[829/1442]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[830/1442]: training_loss: tensor(0.4244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[831/1442]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[832/1442]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[833/1442]: training_loss: tensor(0.3590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[834/1442]: training_loss: tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[835/1442]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[836/1442]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[837/1442]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[838/1442]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[839/1442]: training_loss: tensor(0.1742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[840/1442]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[841/1442]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[842/1442]: training_loss: tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[843/1442]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[844/1442]: training_loss: tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[845/1442]: training_loss: tensor(0.5007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[846/1442]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[847/1442]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[848/1442]: training_loss: tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[849/1442]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[850/1442]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[851/1442]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[852/1442]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[853/1442]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[854/1442]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[855/1442]: training_loss: tensor(0.5074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[856/1442]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[857/1442]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[858/1442]: training_loss: tensor(0.8212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[859/1442]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[860/1442]: training_loss: tensor(0.3538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[861/1442]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[862/1442]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[863/1442]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[864/1442]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[865/1442]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[866/1442]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[867/1442]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[868/1442]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[869/1442]: training_loss: tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[870/1442]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[871/1442]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[872/1442]: training_loss: tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[873/1442]: training_loss: tensor(0.4797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[874/1442]: training_loss: tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[875/1442]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[876/1442]: training_loss: tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[877/1442]: training_loss: tensor(0.5760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[878/1442]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[879/1442]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[880/1442]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[881/1442]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[882/1442]: training_loss: tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[883/1442]: training_loss: tensor(0.3379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[884/1442]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[885/1442]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[886/1442]: training_loss: tensor(0.7334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[887/1442]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[888/1442]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[889/1442]: training_loss: tensor(0.8108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[890/1442]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[891/1442]: training_loss: tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[892/1442]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[893/1442]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[894/1442]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[895/1442]: training_loss: tensor(0.7815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[896/1442]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[897/1442]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[898/1442]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[899/1442]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[900/1442]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[901/1442]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[902/1442]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[903/1442]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[904/1442]: training_loss: tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[905/1442]: training_loss: tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[906/1442]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[907/1442]: training_loss: tensor(0.9460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[908/1442]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[909/1442]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[910/1442]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[911/1442]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[912/1442]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[913/1442]: training_loss: tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[914/1442]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[915/1442]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[916/1442]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[917/1442]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[918/1442]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[919/1442]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[920/1442]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[921/1442]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[922/1442]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[923/1442]: training_loss: tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[924/1442]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[925/1442]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[926/1442]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[927/1442]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[928/1442]: training_loss: tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[929/1442]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[930/1442]: training_loss: tensor(0.4329, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[931/1442]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[932/1442]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[933/1442]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[934/1442]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[935/1442]: training_loss: tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[936/1442]: training_loss: tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[937/1442]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[938/1442]: training_loss: tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[939/1442]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[940/1442]: training_loss: tensor(0.4863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[941/1442]: training_loss: tensor(0.4618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[942/1442]: training_loss: tensor(0.3842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[943/1442]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[944/1442]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[945/1442]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[946/1442]: training_loss: tensor(0.9982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[947/1442]: training_loss: tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[948/1442]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[949/1442]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[950/1442]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[951/1442]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[952/1442]: training_loss: tensor(0.8364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[953/1442]: training_loss: tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[954/1442]: training_loss: tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[955/1442]: training_loss: tensor(0.5094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[956/1442]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[957/1442]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[958/1442]: training_loss: tensor(0.4937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[959/1442]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[960/1442]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[961/1442]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[962/1442]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[963/1442]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[964/1442]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[965/1442]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[966/1442]: training_loss: tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[967/1442]: training_loss: tensor(0.6112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[968/1442]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[969/1442]: training_loss: tensor(0.2869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[970/1442]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[971/1442]: training_loss: tensor(0.4625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[972/1442]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[973/1442]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[974/1442]: training_loss: tensor(0.4144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[975/1442]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[976/1442]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[977/1442]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[978/1442]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[979/1442]: training_loss: tensor(0.3870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[980/1442]: training_loss: tensor(0.3266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[981/1442]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[982/1442]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[983/1442]: training_loss: tensor(0.5177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[984/1442]: training_loss: tensor(0.6105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[985/1442]: training_loss: tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[986/1442]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[987/1442]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[988/1442]: training_loss: tensor(0.5362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[989/1442]: training_loss: tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[990/1442]: training_loss: tensor(0.4486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[991/1442]: training_loss: tensor(0.2765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[992/1442]: training_loss: tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[993/1442]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[994/1442]: training_loss: tensor(0.9684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[995/1442]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[996/1442]: training_loss: tensor(0.5113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[997/1442]: training_loss: tensor(0.2889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[998/1442]: training_loss: tensor(0.4238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[999/1442]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1000/1442]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1001/1442]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1002/1442]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1003/1442]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1004/1442]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1005/1442]: training_loss: tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1006/1442]: training_loss: tensor(0.5135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1007/1442]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1008/1442]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1009/1442]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1010/1442]: training_loss: tensor(0.1917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1011/1442]: training_loss: tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1012/1442]: training_loss: tensor(0.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1013/1442]: training_loss: tensor(0.5550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1014/1442]: training_loss: tensor(0.4545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1015/1442]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1016/1442]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1017/1442]: training_loss: tensor(0.4162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1018/1442]: training_loss: tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1019/1442]: training_loss: tensor(0.6468, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1020/1442]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1021/1442]: training_loss: tensor(0.4723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1022/1442]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1023/1442]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1024/1442]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1025/1442]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1026/1442]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1027/1442]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1028/1442]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1029/1442]: training_loss: tensor(0.7481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1030/1442]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1031/1442]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1032/1442]: training_loss: tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1033/1442]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1034/1442]: training_loss: tensor(0.4668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1035/1442]: training_loss: tensor(1.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1036/1442]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1037/1442]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1038/1442]: training_loss: tensor(0.5498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1039/1442]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1040/1442]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1041/1442]: training_loss: tensor(0.2026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1042/1442]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1043/1442]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1044/1442]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1045/1442]: training_loss: tensor(0.7665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1046/1442]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1047/1442]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1048/1442]: training_loss: tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1049/1442]: training_loss: tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1050/1442]: training_loss: tensor(0.1915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1051/1442]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1052/1442]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1053/1442]: training_loss: tensor(1.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1054/1442]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1055/1442]: training_loss: tensor(0.8108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1056/1442]: training_loss: tensor(0.4946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1057/1442]: training_loss: tensor(0.2194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1058/1442]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1059/1442]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1060/1442]: training_loss: tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1061/1442]: training_loss: tensor(0.4159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1062/1442]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1063/1442]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1064/1442]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1065/1442]: training_loss: tensor(0.6067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1066/1442]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1067/1442]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1068/1442]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1069/1442]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1070/1442]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1071/1442]: training_loss: tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1072/1442]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1073/1442]: training_loss: tensor(0.3406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1074/1442]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1075/1442]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1076/1442]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1077/1442]: training_loss: tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1078/1442]: training_loss: tensor(0.4445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1079/1442]: training_loss: tensor(0.6012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1080/1442]: training_loss: tensor(0.4652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1081/1442]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1082/1442]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1083/1442]: training_loss: tensor(0.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1084/1442]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1085/1442]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1086/1442]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1087/1442]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1088/1442]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1089/1442]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1090/1442]: training_loss: tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1091/1442]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1092/1442]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1093/1442]: training_loss: tensor(0.4335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1094/1442]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1095/1442]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1096/1442]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1097/1442]: training_loss: tensor(0.5736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1098/1442]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1099/1442]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1100/1442]: training_loss: tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1101/1442]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1102/1442]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1103/1442]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1104/1442]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1105/1442]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1106/1442]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1107/1442]: training_loss: tensor(0.4707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1108/1442]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1109/1442]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1110/1442]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1111/1442]: training_loss: tensor(0.4096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1112/1442]: training_loss: tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1113/1442]: training_loss: tensor(0.4964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1114/1442]: training_loss: tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1115/1442]: training_loss: tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1116/1442]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1117/1442]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1118/1442]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1119/1442]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1120/1442]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1121/1442]: training_loss: tensor(0.7442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1122/1442]: training_loss: tensor(0.5993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1123/1442]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1124/1442]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1125/1442]: training_loss: tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1126/1442]: training_loss: tensor(0.4107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1127/1442]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1128/1442]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1129/1442]: training_loss: tensor(0.4118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1130/1442]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1131/1442]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1132/1442]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1133/1442]: training_loss: tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1134/1442]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1135/1442]: training_loss: tensor(0.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1136/1442]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1137/1442]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1138/1442]: training_loss: tensor(0.4474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1139/1442]: training_loss: tensor(0.4159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1140/1442]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1141/1442]: training_loss: tensor(0.5163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1142/1442]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1143/1442]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1144/1442]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1145/1442]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1146/1442]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1147/1442]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1148/1442]: training_loss: tensor(0.4658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1149/1442]: training_loss: tensor(0.7767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1150/1442]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1151/1442]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1152/1442]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1153/1442]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1154/1442]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1155/1442]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1156/1442]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1157/1442]: training_loss: tensor(0.8465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1158/1442]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1159/1442]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1160/1442]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1161/1442]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1162/1442]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1163/1442]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1164/1442]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1165/1442]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1166/1442]: training_loss: tensor(0.3618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1167/1442]: training_loss: tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1168/1442]: training_loss: tensor(0.3297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1169/1442]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1170/1442]: training_loss: tensor(0.4584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1171/1442]: training_loss: tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1172/1442]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1173/1442]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1174/1442]: training_loss: tensor(0.1908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1175/1442]: training_loss: tensor(0.8794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1176/1442]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1177/1442]: training_loss: tensor(0.1841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1178/1442]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1179/1442]: training_loss: tensor(0.7940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1180/1442]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1181/1442]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1182/1442]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1183/1442]: training_loss: tensor(0.4752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1184/1442]: training_loss: tensor(0.7252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1185/1442]: training_loss: tensor(0.4131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1186/1442]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1187/1442]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1188/1442]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1189/1442]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1190/1442]: training_loss: tensor(0.1740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1191/1442]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1192/1442]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1193/1442]: training_loss: tensor(0.5472, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1194/1442]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1195/1442]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1196/1442]: training_loss: tensor(0.2856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1197/1442]: training_loss: tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1198/1442]: training_loss: tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1199/1442]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1200/1442]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1201/1442]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1202/1442]: training_loss: tensor(0.2758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1203/1442]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1204/1442]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1205/1442]: training_loss: tensor(0.7885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1206/1442]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1207/1442]: training_loss: tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1208/1442]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1209/1442]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1210/1442]: training_loss: tensor(0.5158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1211/1442]: training_loss: tensor(0.5091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1212/1442]: training_loss: tensor(0.5110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1213/1442]: training_loss: tensor(0.6847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1214/1442]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1215/1442]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1216/1442]: training_loss: tensor(0.2104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1217/1442]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1218/1442]: training_loss: tensor(0.2600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1219/1442]: training_loss: tensor(0.9394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1220/1442]: training_loss: tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1221/1442]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1222/1442]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1223/1442]: training_loss: tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1224/1442]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1225/1442]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1226/1442]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1227/1442]: training_loss: tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1228/1442]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1229/1442]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1230/1442]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1231/1442]: training_loss: tensor(0.3560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1232/1442]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1233/1442]: training_loss: tensor(0.3479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1234/1442]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1235/1442]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1236/1442]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1237/1442]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1238/1442]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1239/1442]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1240/1442]: training_loss: tensor(0.4850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1241/1442]: training_loss: tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1242/1442]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1243/1442]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1244/1442]: training_loss: tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1245/1442]: training_loss: tensor(0.4390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1246/1442]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1247/1442]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1248/1442]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1249/1442]: training_loss: tensor(0.5090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1250/1442]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1251/1442]: training_loss: tensor(0.4720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1252/1442]: training_loss: tensor(0.4538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1253/1442]: training_loss: tensor(0.3241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1254/1442]: training_loss: tensor(0.5810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1255/1442]: training_loss: tensor(0.5022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1256/1442]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1257/1442]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1258/1442]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1259/1442]: training_loss: tensor(0.4298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1260/1442]: training_loss: tensor(0.7106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1261/1442]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1262/1442]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1263/1442]: training_loss: tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1264/1442]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1265/1442]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1266/1442]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1267/1442]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1268/1442]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1269/1442]: training_loss: tensor(0.4447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1270/1442]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1271/1442]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1272/1442]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1273/1442]: training_loss: tensor(0.5234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1274/1442]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1275/1442]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1276/1442]: training_loss: tensor(0.9059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1277/1442]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1278/1442]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1279/1442]: training_loss: tensor(0.4431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1280/1442]: training_loss: tensor(0.4901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1281/1442]: training_loss: tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1282/1442]: training_loss: tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1283/1442]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1284/1442]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1285/1442]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1286/1442]: training_loss: tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1287/1442]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1288/1442]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1289/1442]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1290/1442]: training_loss: tensor(0.4190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1291/1442]: training_loss: tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1292/1442]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1293/1442]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1294/1442]: training_loss: tensor(0.6195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1295/1442]: training_loss: tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1296/1442]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1297/1442]: training_loss: tensor(0.6316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1298/1442]: training_loss: tensor(0.6667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1299/1442]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1300/1442]: training_loss: tensor(0.6275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1301/1442]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1302/1442]: training_loss: tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1303/1442]: training_loss: tensor(0.4940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1304/1442]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1305/1442]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1306/1442]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1307/1442]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1308/1442]: training_loss: tensor(0.4777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1309/1442]: training_loss: tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1310/1442]: training_loss: tensor(0.4614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1311/1442]: training_loss: tensor(0.7982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1312/1442]: training_loss: tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1313/1442]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1314/1442]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1315/1442]: training_loss: tensor(1.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1316/1442]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1317/1442]: training_loss: tensor(0.5604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1318/1442]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1319/1442]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1320/1442]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1321/1442]: training_loss: tensor(0.6063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1322/1442]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1323/1442]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1324/1442]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1325/1442]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1326/1442]: training_loss: tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1327/1442]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1328/1442]: training_loss: tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1329/1442]: training_loss: tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1330/1442]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1331/1442]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1332/1442]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1333/1442]: training_loss: tensor(0.2709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1334/1442]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1335/1442]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1336/1442]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1337/1442]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1338/1442]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1339/1442]: training_loss: tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1340/1442]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1341/1442]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1342/1442]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1343/1442]: training_loss: tensor(0.5074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1344/1442]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1345/1442]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1346/1442]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1347/1442]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1348/1442]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1349/1442]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1350/1442]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1351/1442]: training_loss: tensor(0.5979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1352/1442]: training_loss: tensor(0.5109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1353/1442]: training_loss: tensor(0.4151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1354/1442]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1355/1442]: training_loss: tensor(0.4328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1356/1442]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1357/1442]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1358/1442]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1359/1442]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1360/1442]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1361/1442]: training_loss: tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1362/1442]: training_loss: tensor(0.4777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1363/1442]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1364/1442]: training_loss: tensor(0.4504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1365/1442]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1366/1442]: training_loss: tensor(0.5763, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1367/1442]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1368/1442]: training_loss: tensor(1.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1369/1442]: training_loss: tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1370/1442]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1371/1442]: training_loss: tensor(0.6337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1372/1442]: training_loss: tensor(0.3931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1373/1442]: training_loss: tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1374/1442]: training_loss: tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1375/1442]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1376/1442]: training_loss: tensor(0.4179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1377/1442]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1378/1442]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1379/1442]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1380/1442]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1381/1442]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1382/1442]: training_loss: tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1383/1442]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1384/1442]: training_loss: tensor(0.5258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1385/1442]: training_loss: tensor(0.4302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1386/1442]: training_loss: tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1387/1442]: training_loss: tensor(0.4722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1388/1442]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1389/1442]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1390/1442]: training_loss: tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1391/1442]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1392/1442]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1393/1442]: training_loss: tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1394/1442]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1395/1442]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1396/1442]: training_loss: tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1397/1442]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1398/1442]: training_loss: tensor(0.4430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1399/1442]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1400/1442]: training_loss: tensor(0.4455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1401/1442]: training_loss: tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1402/1442]: training_loss: tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1403/1442]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1404/1442]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1405/1442]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1406/1442]: training_loss: tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1407/1442]: training_loss: tensor(0.5098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1408/1442]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1409/1442]: training_loss: tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1410/1442]: training_loss: tensor(0.4114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1411/1442]: training_loss: tensor(0.4985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1412/1442]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1413/1442]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1414/1442]: training_loss: tensor(0.4770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1415/1442]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1416/1442]: training_loss: tensor(0.5066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1417/1442]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1418/1442]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1419/1442]: training_loss: tensor(0.4186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1420/1442]: training_loss: tensor(0.4151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1421/1442]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1422/1442]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1423/1442]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1424/1442]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1425/1442]: training_loss: tensor(0.5422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1426/1442]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1427/1442]: training_loss: tensor(0.7702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1428/1442]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1429/1442]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1430/1442]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1431/1442]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1432/1442]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1433/1442]: training_loss: tensor(0.6592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1434/1442]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1435/1442]: training_loss: tensor(0.2707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1436/1442]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1437/1442]: training_loss: tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1438/1442]: training_loss: tensor(0.5302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1439/1442]: training_loss: tensor(0.8044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1440/1442]: training_loss: tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1441/1442]: training_loss: tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [4/5],global step [5768/7210], pt loss:0.0000, dev loss:0.5448\n",
      "batch_no[0/1442]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1/1442]: training_loss: tensor(0.5847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[2/1442]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[3/1442]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[4/1442]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[5/1442]: training_loss: tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[6/1442]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[7/1442]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[8/1442]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[9/1442]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[10/1442]: training_loss: tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[11/1442]: training_loss: tensor(0.5062, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[12/1442]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[13/1442]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[14/1442]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[15/1442]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[16/1442]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[17/1442]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[18/1442]: training_loss: tensor(0.8778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[19/1442]: training_loss: tensor(0.6205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[20/1442]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[21/1442]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[22/1442]: training_loss: tensor(0.5183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[23/1442]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[24/1442]: training_loss: tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[25/1442]: training_loss: tensor(0.4646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[26/1442]: training_loss: tensor(0.4633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[27/1442]: training_loss: tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[28/1442]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[29/1442]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[30/1442]: training_loss: tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[31/1442]: training_loss: tensor(0.4342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[32/1442]: training_loss: tensor(0.3465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[33/1442]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[34/1442]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[35/1442]: training_loss: tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[36/1442]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[37/1442]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[38/1442]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[39/1442]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[40/1442]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[41/1442]: training_loss: tensor(0.5220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[42/1442]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[43/1442]: training_loss: tensor(0.4373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[44/1442]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[45/1442]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[46/1442]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[47/1442]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[48/1442]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[49/1442]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[50/1442]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[51/1442]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[52/1442]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[53/1442]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[54/1442]: training_loss: tensor(0.2759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[55/1442]: training_loss: tensor(0.6619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[56/1442]: training_loss: tensor(1.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[57/1442]: training_loss: tensor(0.6032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[58/1442]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[59/1442]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[60/1442]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[61/1442]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[62/1442]: training_loss: tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[63/1442]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[64/1442]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[65/1442]: training_loss: tensor(0.4400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[66/1442]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[67/1442]: training_loss: tensor(0.4726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[68/1442]: training_loss: tensor(0.5092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[69/1442]: training_loss: tensor(0.5512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[70/1442]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[71/1442]: training_loss: tensor(0.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[72/1442]: training_loss: tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[73/1442]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[74/1442]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[75/1442]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[76/1442]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[77/1442]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[78/1442]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[79/1442]: training_loss: tensor(0.3065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[80/1442]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[81/1442]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[82/1442]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[83/1442]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[84/1442]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[85/1442]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[86/1442]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[87/1442]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[88/1442]: training_loss: tensor(0.5443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[89/1442]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[90/1442]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[91/1442]: training_loss: tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[92/1442]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[93/1442]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[94/1442]: training_loss: tensor(0.4363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[95/1442]: training_loss: tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[96/1442]: training_loss: tensor(0.4855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[97/1442]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[98/1442]: training_loss: tensor(0.3500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[99/1442]: training_loss: tensor(0.5048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[100/1442]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[101/1442]: training_loss: tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[102/1442]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[103/1442]: training_loss: tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[104/1442]: training_loss: tensor(0.6246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[105/1442]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[106/1442]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[107/1442]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[108/1442]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[109/1442]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[110/1442]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[111/1442]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[112/1442]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[113/1442]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[114/1442]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[115/1442]: training_loss: tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[116/1442]: training_loss: tensor(0.5017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[117/1442]: training_loss: tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[118/1442]: training_loss: tensor(0.3242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[119/1442]: training_loss: tensor(0.4940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[120/1442]: training_loss: tensor(0.4409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[121/1442]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[122/1442]: training_loss: tensor(0.5757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[123/1442]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[124/1442]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[125/1442]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[126/1442]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[127/1442]: training_loss: tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[128/1442]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[129/1442]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[130/1442]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[131/1442]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[132/1442]: training_loss: tensor(0.9628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[133/1442]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[134/1442]: training_loss: tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[135/1442]: training_loss: tensor(0.4894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[136/1442]: training_loss: tensor(0.7555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[137/1442]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[138/1442]: training_loss: tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[139/1442]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[140/1442]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[141/1442]: training_loss: tensor(0.6455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[142/1442]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[143/1442]: training_loss: tensor(0.8066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[144/1442]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[145/1442]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[146/1442]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[147/1442]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[148/1442]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[149/1442]: training_loss: tensor(0.4729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[150/1442]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[151/1442]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[152/1442]: training_loss: tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[153/1442]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[154/1442]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[155/1442]: training_loss: tensor(0.4950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[156/1442]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[157/1442]: training_loss: tensor(0.7743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[158/1442]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[159/1442]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[160/1442]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[161/1442]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[162/1442]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[163/1442]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[164/1442]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[165/1442]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[166/1442]: training_loss: tensor(0.6560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[167/1442]: training_loss: tensor(0.4494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[168/1442]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[169/1442]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[170/1442]: training_loss: tensor(0.4372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[171/1442]: training_loss: tensor(0.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[172/1442]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[173/1442]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[174/1442]: training_loss: tensor(0.4405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[175/1442]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[176/1442]: training_loss: tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[177/1442]: training_loss: tensor(0.4412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[178/1442]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[179/1442]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[180/1442]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[181/1442]: training_loss: tensor(0.5304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[182/1442]: training_loss: tensor(0.7702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[183/1442]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[184/1442]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[185/1442]: training_loss: tensor(0.5507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[186/1442]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[187/1442]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[188/1442]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[189/1442]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[190/1442]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[191/1442]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[192/1442]: training_loss: tensor(0.5736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[193/1442]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[194/1442]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[195/1442]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[196/1442]: training_loss: tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[197/1442]: training_loss: tensor(0.8516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[198/1442]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[199/1442]: training_loss: tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[200/1442]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[201/1442]: training_loss: tensor(0.5042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[202/1442]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[203/1442]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[204/1442]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[205/1442]: training_loss: tensor(0.2900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[206/1442]: training_loss: tensor(0.4279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[207/1442]: training_loss: tensor(0.2945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[208/1442]: training_loss: tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[209/1442]: training_loss: tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[210/1442]: training_loss: tensor(0.4588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[211/1442]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[212/1442]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[213/1442]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[214/1442]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[215/1442]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[216/1442]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[217/1442]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[218/1442]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[219/1442]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[220/1442]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[221/1442]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[222/1442]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[223/1442]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[224/1442]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[225/1442]: training_loss: tensor(0.9056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[226/1442]: training_loss: tensor(0.3352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[227/1442]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[228/1442]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[229/1442]: training_loss: tensor(0.5328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[230/1442]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[231/1442]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[232/1442]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[233/1442]: training_loss: tensor(0.3612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[234/1442]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[235/1442]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[236/1442]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[237/1442]: training_loss: tensor(0.3156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[238/1442]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[239/1442]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[240/1442]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[241/1442]: training_loss: tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[242/1442]: training_loss: tensor(0.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[243/1442]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[244/1442]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[245/1442]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[246/1442]: training_loss: tensor(0.4040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[247/1442]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[248/1442]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[249/1442]: training_loss: tensor(0.8330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[250/1442]: training_loss: tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[251/1442]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[252/1442]: training_loss: tensor(0.4959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[253/1442]: training_loss: tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[254/1442]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[255/1442]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[256/1442]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[257/1442]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[258/1442]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[259/1442]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[260/1442]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[261/1442]: training_loss: tensor(0.4481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[262/1442]: training_loss: tensor(0.4454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[263/1442]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[264/1442]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[265/1442]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[266/1442]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[267/1442]: training_loss: tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[268/1442]: training_loss: tensor(0.7247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[269/1442]: training_loss: tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[270/1442]: training_loss: tensor(0.5227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[271/1442]: training_loss: tensor(0.5128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[272/1442]: training_loss: tensor(0.2904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[273/1442]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[274/1442]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[275/1442]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[276/1442]: training_loss: tensor(0.5067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[277/1442]: training_loss: tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[278/1442]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[279/1442]: training_loss: tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[280/1442]: training_loss: tensor(0.4627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[281/1442]: training_loss: tensor(0.4329, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[282/1442]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[283/1442]: training_loss: tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[284/1442]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[285/1442]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[286/1442]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[287/1442]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[288/1442]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[289/1442]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[290/1442]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[291/1442]: training_loss: tensor(0.6858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[292/1442]: training_loss: tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[293/1442]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[294/1442]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[295/1442]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[296/1442]: training_loss: tensor(0.3562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[297/1442]: training_loss: tensor(0.5240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[298/1442]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[299/1442]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[300/1442]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[301/1442]: training_loss: tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[302/1442]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[303/1442]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[304/1442]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[305/1442]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[306/1442]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[307/1442]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[308/1442]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[309/1442]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[310/1442]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[311/1442]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[312/1442]: training_loss: tensor(0.2572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[313/1442]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[314/1442]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[315/1442]: training_loss: tensor(0.4087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[316/1442]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[317/1442]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[318/1442]: training_loss: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[319/1442]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[320/1442]: training_loss: tensor(0.5943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[321/1442]: training_loss: tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[322/1442]: training_loss: tensor(0.1995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[323/1442]: training_loss: tensor(1.4632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[324/1442]: training_loss: tensor(0.8004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[325/1442]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[326/1442]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[327/1442]: training_loss: tensor(0.4640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[328/1442]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[329/1442]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[330/1442]: training_loss: tensor(1.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[331/1442]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[332/1442]: training_loss: tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[333/1442]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[334/1442]: training_loss: tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[335/1442]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[336/1442]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[337/1442]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[338/1442]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[339/1442]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[340/1442]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[341/1442]: training_loss: tensor(0.7901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[342/1442]: training_loss: tensor(0.4973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[343/1442]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[344/1442]: training_loss: tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[345/1442]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[346/1442]: training_loss: tensor(1.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[347/1442]: training_loss: tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[348/1442]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[349/1442]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[350/1442]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[351/1442]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[352/1442]: training_loss: tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[353/1442]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[354/1442]: training_loss: tensor(0.4592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[355/1442]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[356/1442]: training_loss: tensor(0.2915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[357/1442]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[358/1442]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[359/1442]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[360/1442]: training_loss: tensor(0.8365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[361/1442]: training_loss: tensor(0.7010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[362/1442]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[363/1442]: training_loss: tensor(0.4431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[364/1442]: training_loss: tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[365/1442]: training_loss: tensor(0.2975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[366/1442]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[367/1442]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[368/1442]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[369/1442]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[370/1442]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[371/1442]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[372/1442]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[373/1442]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[374/1442]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[375/1442]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[376/1442]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[377/1442]: training_loss: tensor(0.7291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[378/1442]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[379/1442]: training_loss: tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[380/1442]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[381/1442]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[382/1442]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[383/1442]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[384/1442]: training_loss: tensor(0.3461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[385/1442]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[386/1442]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[387/1442]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[388/1442]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[389/1442]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[390/1442]: training_loss: tensor(0.2834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[391/1442]: training_loss: tensor(0.7242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[392/1442]: training_loss: tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[393/1442]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[394/1442]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[395/1442]: training_loss: tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[396/1442]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[397/1442]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[398/1442]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[399/1442]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[400/1442]: training_loss: tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[401/1442]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[402/1442]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[403/1442]: training_loss: tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[404/1442]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[405/1442]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[406/1442]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[407/1442]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[408/1442]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[409/1442]: training_loss: tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[410/1442]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[411/1442]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[412/1442]: training_loss: tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[413/1442]: training_loss: tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[414/1442]: training_loss: tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[415/1442]: training_loss: tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[416/1442]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[417/1442]: training_loss: tensor(1.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[418/1442]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[419/1442]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[420/1442]: training_loss: tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[421/1442]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[422/1442]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[423/1442]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[424/1442]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[425/1442]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[426/1442]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[427/1442]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[428/1442]: training_loss: tensor(0.4998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[429/1442]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[430/1442]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[431/1442]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[432/1442]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[433/1442]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[434/1442]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[435/1442]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[436/1442]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[437/1442]: training_loss: tensor(0.4829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[438/1442]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[439/1442]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[440/1442]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[441/1442]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[442/1442]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[443/1442]: training_loss: tensor(0.4120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[444/1442]: training_loss: tensor(0.4595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[445/1442]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[446/1442]: training_loss: tensor(0.5776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[447/1442]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[448/1442]: training_loss: tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[449/1442]: training_loss: tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[450/1442]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[451/1442]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[452/1442]: training_loss: tensor(0.8199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[453/1442]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[454/1442]: training_loss: tensor(0.8612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[455/1442]: training_loss: tensor(0.5276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[456/1442]: training_loss: tensor(0.6272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[457/1442]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[458/1442]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[459/1442]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[460/1442]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[461/1442]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[462/1442]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[463/1442]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[464/1442]: training_loss: tensor(0.4852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[465/1442]: training_loss: tensor(0.5025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[466/1442]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[467/1442]: training_loss: tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[468/1442]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[469/1442]: training_loss: tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[470/1442]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[471/1442]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[472/1442]: training_loss: tensor(0.7745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[473/1442]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[474/1442]: training_loss: tensor(0.3163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[475/1442]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[476/1442]: training_loss: tensor(0.2759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[477/1442]: training_loss: tensor(0.4568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[478/1442]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[479/1442]: training_loss: tensor(0.6002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[480/1442]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[481/1442]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[482/1442]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[483/1442]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[484/1442]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[485/1442]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[486/1442]: training_loss: tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[487/1442]: training_loss: tensor(0.4970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[488/1442]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[489/1442]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[490/1442]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[491/1442]: training_loss: tensor(0.4310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[492/1442]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[493/1442]: training_loss: tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[494/1442]: training_loss: tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[495/1442]: training_loss: tensor(0.4185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[496/1442]: training_loss: tensor(0.3648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[497/1442]: training_loss: tensor(0.4259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[498/1442]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[499/1442]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[500/1442]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[501/1442]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[502/1442]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[503/1442]: training_loss: tensor(0.3443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[504/1442]: training_loss: tensor(0.4065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[505/1442]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[506/1442]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[507/1442]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[508/1442]: training_loss: tensor(0.7817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[509/1442]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[510/1442]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[511/1442]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[512/1442]: training_loss: tensor(0.3581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[513/1442]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[514/1442]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[515/1442]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[516/1442]: training_loss: tensor(0.8314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[517/1442]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[518/1442]: training_loss: tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[519/1442]: training_loss: tensor(0.4632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[520/1442]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[521/1442]: training_loss: tensor(0.5024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[522/1442]: training_loss: tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[523/1442]: training_loss: tensor(0.4910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[524/1442]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[525/1442]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[526/1442]: training_loss: tensor(0.4345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[527/1442]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[528/1442]: training_loss: tensor(0.4084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[529/1442]: training_loss: tensor(0.4262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[530/1442]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[531/1442]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[532/1442]: training_loss: tensor(0.4556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[533/1442]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[534/1442]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[535/1442]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[536/1442]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[537/1442]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[538/1442]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[539/1442]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[540/1442]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[541/1442]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[542/1442]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[543/1442]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[544/1442]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[545/1442]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[546/1442]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[547/1442]: training_loss: tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[548/1442]: training_loss: tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[549/1442]: training_loss: tensor(0.5414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[550/1442]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[551/1442]: training_loss: tensor(0.4875, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[552/1442]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[553/1442]: training_loss: tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[554/1442]: training_loss: tensor(0.6108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[555/1442]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[556/1442]: training_loss: tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[557/1442]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[558/1442]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[559/1442]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[560/1442]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[561/1442]: training_loss: tensor(0.2619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[562/1442]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[563/1442]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[564/1442]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[565/1442]: training_loss: tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[566/1442]: training_loss: tensor(0.7997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[567/1442]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[568/1442]: training_loss: tensor(0.2764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[569/1442]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[570/1442]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[571/1442]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[572/1442]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[573/1442]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[574/1442]: training_loss: tensor(0.4157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[575/1442]: training_loss: tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[576/1442]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[577/1442]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[578/1442]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[579/1442]: training_loss: tensor(0.4458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[580/1442]: training_loss: tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[581/1442]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[582/1442]: training_loss: tensor(0.8009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[583/1442]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[584/1442]: training_loss: tensor(0.4506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[585/1442]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[586/1442]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[587/1442]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[588/1442]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[589/1442]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[590/1442]: training_loss: tensor(0.6061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[591/1442]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[592/1442]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[593/1442]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[594/1442]: training_loss: tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[595/1442]: training_loss: tensor(0.4040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[596/1442]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[597/1442]: training_loss: tensor(0.5069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[598/1442]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[599/1442]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[600/1442]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[601/1442]: training_loss: tensor(0.4802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[602/1442]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[603/1442]: training_loss: tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[604/1442]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[605/1442]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[606/1442]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[607/1442]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[608/1442]: training_loss: tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[609/1442]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[610/1442]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[611/1442]: training_loss: tensor(0.4402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[612/1442]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[613/1442]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[614/1442]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[615/1442]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[616/1442]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[617/1442]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[618/1442]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[619/1442]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[620/1442]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[621/1442]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[622/1442]: training_loss: tensor(0.5925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[623/1442]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[624/1442]: training_loss: tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[625/1442]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[626/1442]: training_loss: tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[627/1442]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[628/1442]: training_loss: tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[629/1442]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[630/1442]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[631/1442]: training_loss: tensor(0.4655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[632/1442]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[633/1442]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[634/1442]: training_loss: tensor(0.4290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[635/1442]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[636/1442]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[637/1442]: training_loss: tensor(0.4339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[638/1442]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[639/1442]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[640/1442]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[641/1442]: training_loss: tensor(0.4437, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[642/1442]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[643/1442]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[644/1442]: training_loss: tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[645/1442]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[646/1442]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[647/1442]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[648/1442]: training_loss: tensor(0.6054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[649/1442]: training_loss: tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[650/1442]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[651/1442]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[652/1442]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[653/1442]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[654/1442]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[655/1442]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[656/1442]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[657/1442]: training_loss: tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[658/1442]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[659/1442]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[660/1442]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[661/1442]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[662/1442]: training_loss: tensor(0.4931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[663/1442]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[664/1442]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[665/1442]: training_loss: tensor(0.3404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[666/1442]: training_loss: tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[667/1442]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[668/1442]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[669/1442]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[670/1442]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[671/1442]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[672/1442]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[673/1442]: training_loss: tensor(0.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[674/1442]: training_loss: tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[675/1442]: training_loss: tensor(0.4740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[676/1442]: training_loss: tensor(0.4271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[677/1442]: training_loss: tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[678/1442]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[679/1442]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[680/1442]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[681/1442]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[682/1442]: training_loss: tensor(0.3578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[683/1442]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[684/1442]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[685/1442]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[686/1442]: training_loss: tensor(0.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[687/1442]: training_loss: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[688/1442]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[689/1442]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[690/1442]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[691/1442]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[692/1442]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[693/1442]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[694/1442]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[695/1442]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[696/1442]: training_loss: tensor(0.4300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[697/1442]: training_loss: tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[698/1442]: training_loss: tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[699/1442]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[700/1442]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[701/1442]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[702/1442]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[703/1442]: training_loss: tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[704/1442]: training_loss: tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[705/1442]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[706/1442]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[707/1442]: training_loss: tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[708/1442]: training_loss: tensor(1.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[709/1442]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[710/1442]: training_loss: tensor(0.4432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[711/1442]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[712/1442]: training_loss: tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[713/1442]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[714/1442]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[715/1442]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[716/1442]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[717/1442]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[718/1442]: training_loss: tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[719/1442]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[720/1442]: training_loss: tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[721/1442]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[722/1442]: training_loss: tensor(0.5362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[723/1442]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[724/1442]: training_loss: tensor(0.4600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[725/1442]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[726/1442]: training_loss: tensor(0.8941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[727/1442]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[728/1442]: training_loss: tensor(0.5122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[729/1442]: training_loss: tensor(0.7767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[730/1442]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[731/1442]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[732/1442]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[733/1442]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[734/1442]: training_loss: tensor(0.6355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[735/1442]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[736/1442]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[737/1442]: training_loss: tensor(0.5717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[738/1442]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[739/1442]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[740/1442]: training_loss: tensor(0.4516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[741/1442]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[742/1442]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[743/1442]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[744/1442]: training_loss: tensor(0.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[745/1442]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[746/1442]: training_loss: tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[747/1442]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[748/1442]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[749/1442]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[750/1442]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[751/1442]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[752/1442]: training_loss: tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[753/1442]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[754/1442]: training_loss: tensor(0.4794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[755/1442]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[756/1442]: training_loss: tensor(0.5596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[757/1442]: training_loss: tensor(0.2673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[758/1442]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[759/1442]: training_loss: tensor(0.5569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[760/1442]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[761/1442]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[762/1442]: training_loss: tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[763/1442]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[764/1442]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[765/1442]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[766/1442]: training_loss: tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[767/1442]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[768/1442]: training_loss: tensor(0.3470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[769/1442]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[770/1442]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[771/1442]: training_loss: tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[772/1442]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[773/1442]: training_loss: tensor(0.6055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[774/1442]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[775/1442]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[776/1442]: training_loss: tensor(0.3689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[777/1442]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[778/1442]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[779/1442]: training_loss: tensor(0.3239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[780/1442]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[781/1442]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[782/1442]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[783/1442]: training_loss: tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[784/1442]: training_loss: tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[785/1442]: training_loss: tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[786/1442]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[787/1442]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[788/1442]: training_loss: tensor(0.3132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[789/1442]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[790/1442]: training_loss: tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[791/1442]: training_loss: tensor(0.4928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[792/1442]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[793/1442]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[794/1442]: training_loss: tensor(0.5045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[795/1442]: training_loss: tensor(0.5289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[796/1442]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[797/1442]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[798/1442]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[799/1442]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[800/1442]: training_loss: tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[801/1442]: training_loss: tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[802/1442]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[803/1442]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[804/1442]: training_loss: tensor(0.7733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[805/1442]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[806/1442]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[807/1442]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[808/1442]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[809/1442]: training_loss: tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[810/1442]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[811/1442]: training_loss: tensor(0.5816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[812/1442]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[813/1442]: training_loss: tensor(0.4287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[814/1442]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[815/1442]: training_loss: tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[816/1442]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[817/1442]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[818/1442]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[819/1442]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[820/1442]: training_loss: tensor(0.5003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[821/1442]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[822/1442]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[823/1442]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[824/1442]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[825/1442]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[826/1442]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[827/1442]: training_loss: tensor(0.4412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[828/1442]: training_loss: tensor(0.4812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[829/1442]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[830/1442]: training_loss: tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[831/1442]: training_loss: tensor(0.4657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[832/1442]: training_loss: tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[833/1442]: training_loss: tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[834/1442]: training_loss: tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[835/1442]: training_loss: tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[836/1442]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[837/1442]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[838/1442]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[839/1442]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[840/1442]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[841/1442]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[842/1442]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[843/1442]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[844/1442]: training_loss: tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[845/1442]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[846/1442]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[847/1442]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[848/1442]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[849/1442]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[850/1442]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[851/1442]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[852/1442]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[853/1442]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[854/1442]: training_loss: tensor(0.5327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[855/1442]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[856/1442]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[857/1442]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[858/1442]: training_loss: tensor(0.8833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[859/1442]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[860/1442]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[861/1442]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[862/1442]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[863/1442]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[864/1442]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[865/1442]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[866/1442]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[867/1442]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[868/1442]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[869/1442]: training_loss: tensor(0.6455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[870/1442]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[871/1442]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[872/1442]: training_loss: tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[873/1442]: training_loss: tensor(0.4687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[874/1442]: training_loss: tensor(0.4888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[875/1442]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[876/1442]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[877/1442]: training_loss: tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[878/1442]: training_loss: tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[879/1442]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[880/1442]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[881/1442]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[882/1442]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[883/1442]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[884/1442]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[885/1442]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[886/1442]: training_loss: tensor(0.8530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[887/1442]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[888/1442]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[889/1442]: training_loss: tensor(1.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[890/1442]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[891/1442]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[892/1442]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[893/1442]: training_loss: tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[894/1442]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[895/1442]: training_loss: tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[896/1442]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[897/1442]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[898/1442]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[899/1442]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[900/1442]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[901/1442]: training_loss: tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[902/1442]: training_loss: tensor(0.2641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[903/1442]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[904/1442]: training_loss: tensor(0.4529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[905/1442]: training_loss: tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[906/1442]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[907/1442]: training_loss: tensor(0.8463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[908/1442]: training_loss: tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[909/1442]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[910/1442]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[911/1442]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[912/1442]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[913/1442]: training_loss: tensor(0.6202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[914/1442]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[915/1442]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[916/1442]: training_loss: tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[917/1442]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[918/1442]: training_loss: tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[919/1442]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[920/1442]: training_loss: tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[921/1442]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[922/1442]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[923/1442]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[924/1442]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[925/1442]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[926/1442]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[927/1442]: training_loss: tensor(0.4690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[928/1442]: training_loss: tensor(0.5525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[929/1442]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[930/1442]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[931/1442]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[932/1442]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[933/1442]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[934/1442]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[935/1442]: training_loss: tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[936/1442]: training_loss: tensor(0.5104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[937/1442]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[938/1442]: training_loss: tensor(0.4526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[939/1442]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[940/1442]: training_loss: tensor(0.5254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[941/1442]: training_loss: tensor(0.5398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[942/1442]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[943/1442]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[944/1442]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[945/1442]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[946/1442]: training_loss: tensor(0.8925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[947/1442]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[948/1442]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[949/1442]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[950/1442]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[951/1442]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[952/1442]: training_loss: tensor(0.4988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[953/1442]: training_loss: tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[954/1442]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[955/1442]: training_loss: tensor(0.4518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[956/1442]: training_loss: tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[957/1442]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[958/1442]: training_loss: tensor(0.5507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[959/1442]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[960/1442]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[961/1442]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[962/1442]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[963/1442]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[964/1442]: training_loss: tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[965/1442]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[966/1442]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[967/1442]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[968/1442]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[969/1442]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[970/1442]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[971/1442]: training_loss: tensor(0.2635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[972/1442]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[973/1442]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[974/1442]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[975/1442]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[976/1442]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[977/1442]: training_loss: tensor(0.2870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[978/1442]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[979/1442]: training_loss: tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[980/1442]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[981/1442]: training_loss: tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[982/1442]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[983/1442]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[984/1442]: training_loss: tensor(0.4838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[985/1442]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[986/1442]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[987/1442]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[988/1442]: training_loss: tensor(0.7762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[989/1442]: training_loss: tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[990/1442]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[991/1442]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[992/1442]: training_loss: tensor(0.2638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[993/1442]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[994/1442]: training_loss: tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[995/1442]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[996/1442]: training_loss: tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[997/1442]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[998/1442]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[999/1442]: training_loss: tensor(0.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1000/1442]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1001/1442]: training_loss: tensor(0.1977, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1002/1442]: training_loss: tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1003/1442]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1004/1442]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1005/1442]: training_loss: tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1006/1442]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1007/1442]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1008/1442]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1009/1442]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1010/1442]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1011/1442]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1012/1442]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1013/1442]: training_loss: tensor(0.3740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1014/1442]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1015/1442]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1016/1442]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1017/1442]: training_loss: tensor(0.4102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1018/1442]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1019/1442]: training_loss: tensor(0.5033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1020/1442]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1021/1442]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1022/1442]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1023/1442]: training_loss: tensor(0.4470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1024/1442]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1025/1442]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1026/1442]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1027/1442]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1028/1442]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1029/1442]: training_loss: tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1030/1442]: training_loss: tensor(0.4686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1031/1442]: training_loss: tensor(0.4249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1032/1442]: training_loss: tensor(0.3305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1033/1442]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1034/1442]: training_loss: tensor(0.5882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1035/1442]: training_loss: tensor(1.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1036/1442]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1037/1442]: training_loss: tensor(0.2068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1038/1442]: training_loss: tensor(0.5355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1039/1442]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1040/1442]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1041/1442]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1042/1442]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1043/1442]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1044/1442]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1045/1442]: training_loss: tensor(0.4726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1046/1442]: training_loss: tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1047/1442]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1048/1442]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1049/1442]: training_loss: tensor(0.7170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1050/1442]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1051/1442]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1052/1442]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1053/1442]: training_loss: tensor(0.5453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1054/1442]: training_loss: tensor(0.3398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1055/1442]: training_loss: tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1056/1442]: training_loss: tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1057/1442]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1058/1442]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1059/1442]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1060/1442]: training_loss: tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1061/1442]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1062/1442]: training_loss: tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1063/1442]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1064/1442]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1065/1442]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1066/1442]: training_loss: tensor(0.5007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1067/1442]: training_loss: tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1068/1442]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1069/1442]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1070/1442]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1071/1442]: training_loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1072/1442]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1073/1442]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1074/1442]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1075/1442]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1076/1442]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1077/1442]: training_loss: tensor(0.5459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1078/1442]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1079/1442]: training_loss: tensor(0.4882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1080/1442]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1081/1442]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1082/1442]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1083/1442]: training_loss: tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1084/1442]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1085/1442]: training_loss: tensor(0.4188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1086/1442]: training_loss: tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1087/1442]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1088/1442]: training_loss: tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1089/1442]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1090/1442]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1091/1442]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1092/1442]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1093/1442]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1094/1442]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1095/1442]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1096/1442]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1097/1442]: training_loss: tensor(0.4135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1098/1442]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1099/1442]: training_loss: tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1100/1442]: training_loss: tensor(0.4967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1101/1442]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1102/1442]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1103/1442]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1104/1442]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1105/1442]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1106/1442]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1107/1442]: training_loss: tensor(0.4831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1108/1442]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1109/1442]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1110/1442]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1111/1442]: training_loss: tensor(0.2884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1112/1442]: training_loss: tensor(0.5701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1113/1442]: training_loss: tensor(0.4324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1114/1442]: training_loss: tensor(0.5220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1115/1442]: training_loss: tensor(0.4651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1116/1442]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1117/1442]: training_loss: tensor(0.4363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1118/1442]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1119/1442]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1120/1442]: training_loss: tensor(0.2120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1121/1442]: training_loss: tensor(0.5432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1122/1442]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1123/1442]: training_loss: tensor(0.3350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1124/1442]: training_loss: tensor(0.4107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1125/1442]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1126/1442]: training_loss: tensor(0.4052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1127/1442]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1128/1442]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1129/1442]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1130/1442]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1131/1442]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1132/1442]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1133/1442]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1134/1442]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1135/1442]: training_loss: tensor(0.4265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1136/1442]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1137/1442]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1138/1442]: training_loss: tensor(0.7045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1139/1442]: training_loss: tensor(0.2593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1140/1442]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1141/1442]: training_loss: tensor(0.4825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1142/1442]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1143/1442]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1144/1442]: training_loss: tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1145/1442]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1146/1442]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1147/1442]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1148/1442]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1149/1442]: training_loss: tensor(0.8420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1150/1442]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1151/1442]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1152/1442]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1153/1442]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1154/1442]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1155/1442]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1156/1442]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1157/1442]: training_loss: tensor(0.8973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1158/1442]: training_loss: tensor(0.2123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1159/1442]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1160/1442]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1161/1442]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1162/1442]: training_loss: tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1163/1442]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1164/1442]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1165/1442]: training_loss: tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1166/1442]: training_loss: tensor(0.4593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1167/1442]: training_loss: tensor(0.5062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1168/1442]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1169/1442]: training_loss: tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1170/1442]: training_loss: tensor(0.3146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1171/1442]: training_loss: tensor(0.4844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1172/1442]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1173/1442]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1174/1442]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1175/1442]: training_loss: tensor(0.7754, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1176/1442]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1177/1442]: training_loss: tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1178/1442]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1179/1442]: training_loss: tensor(0.5660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1180/1442]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1181/1442]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1182/1442]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1183/1442]: training_loss: tensor(0.4359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1184/1442]: training_loss: tensor(0.5033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1185/1442]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1186/1442]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1187/1442]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1188/1442]: training_loss: tensor(0.3453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1189/1442]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1190/1442]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1191/1442]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1192/1442]: training_loss: tensor(0.3550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1193/1442]: training_loss: tensor(0.4665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1194/1442]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1195/1442]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1196/1442]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1197/1442]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1198/1442]: training_loss: tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1199/1442]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1200/1442]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1201/1442]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1202/1442]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1203/1442]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1204/1442]: training_loss: tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1205/1442]: training_loss: tensor(0.4595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1206/1442]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1207/1442]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1208/1442]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1209/1442]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1210/1442]: training_loss: tensor(0.4194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1211/1442]: training_loss: tensor(0.7554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1212/1442]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1213/1442]: training_loss: tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1214/1442]: training_loss: tensor(0.4329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1215/1442]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1216/1442]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1217/1442]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1218/1442]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1219/1442]: training_loss: tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1220/1442]: training_loss: tensor(0.4910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1221/1442]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1222/1442]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1223/1442]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1224/1442]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1225/1442]: training_loss: tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1226/1442]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1227/1442]: training_loss: tensor(0.5523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1228/1442]: training_loss: tensor(0.5046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1229/1442]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1230/1442]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1231/1442]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1232/1442]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1233/1442]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1234/1442]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1235/1442]: training_loss: tensor(0.3627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1236/1442]: training_loss: tensor(0.4417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1237/1442]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1238/1442]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1239/1442]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1240/1442]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1241/1442]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1242/1442]: training_loss: tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1243/1442]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1244/1442]: training_loss: tensor(0.4724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1245/1442]: training_loss: tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1246/1442]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1247/1442]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1248/1442]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1249/1442]: training_loss: tensor(0.4525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1250/1442]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1251/1442]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1252/1442]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1253/1442]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1254/1442]: training_loss: tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1255/1442]: training_loss: tensor(0.5285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1256/1442]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1257/1442]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1258/1442]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1259/1442]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1260/1442]: training_loss: tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1261/1442]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1262/1442]: training_loss: tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1263/1442]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1264/1442]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1265/1442]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1266/1442]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1267/1442]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1268/1442]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1269/1442]: training_loss: tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1270/1442]: training_loss: tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1271/1442]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1272/1442]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1273/1442]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1274/1442]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1275/1442]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1276/1442]: training_loss: tensor(0.5608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1277/1442]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1278/1442]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1279/1442]: training_loss: tensor(0.4582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1280/1442]: training_loss: tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1281/1442]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1282/1442]: training_loss: tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1283/1442]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1284/1442]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1285/1442]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1286/1442]: training_loss: tensor(0.4502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1287/1442]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1288/1442]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1289/1442]: training_loss: tensor(0.2218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1290/1442]: training_loss: tensor(0.4645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1291/1442]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1292/1442]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1293/1442]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1294/1442]: training_loss: tensor(0.5508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1295/1442]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1296/1442]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1297/1442]: training_loss: tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1298/1442]: training_loss: tensor(0.6107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1299/1442]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1300/1442]: training_loss: tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1301/1442]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1302/1442]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1303/1442]: training_loss: tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1304/1442]: training_loss: tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1305/1442]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1306/1442]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1307/1442]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1308/1442]: training_loss: tensor(0.2912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1309/1442]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1310/1442]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1311/1442]: training_loss: tensor(0.7712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1312/1442]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1313/1442]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1314/1442]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1315/1442]: training_loss: tensor(0.9624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1316/1442]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1317/1442]: training_loss: tensor(0.5798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1318/1442]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1319/1442]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1320/1442]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1321/1442]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1322/1442]: training_loss: tensor(0.4114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1323/1442]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1324/1442]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1325/1442]: training_loss: tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1326/1442]: training_loss: tensor(0.5693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1327/1442]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1328/1442]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1329/1442]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1330/1442]: training_loss: tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1331/1442]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1332/1442]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1333/1442]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1334/1442]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1335/1442]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1336/1442]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1337/1442]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1338/1442]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1339/1442]: training_loss: tensor(0.5566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1340/1442]: training_loss: tensor(0.3320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1341/1442]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1342/1442]: training_loss: tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1343/1442]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1344/1442]: training_loss: tensor(0.4232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1345/1442]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1346/1442]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1347/1442]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1348/1442]: training_loss: tensor(0.2842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1349/1442]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1350/1442]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1351/1442]: training_loss: tensor(0.4864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1352/1442]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1353/1442]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1354/1442]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1355/1442]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1356/1442]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1357/1442]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1358/1442]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1359/1442]: training_loss: tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1360/1442]: training_loss: tensor(0.1907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1361/1442]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1362/1442]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1363/1442]: training_loss: tensor(0.5115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1364/1442]: training_loss: tensor(0.3468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1365/1442]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1366/1442]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1367/1442]: training_loss: tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1368/1442]: training_loss: tensor(0.9928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1369/1442]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1370/1442]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1371/1442]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1372/1442]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1373/1442]: training_loss: tensor(0.8852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1374/1442]: training_loss: tensor(0.7777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1375/1442]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1376/1442]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1377/1442]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1378/1442]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1379/1442]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1380/1442]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1381/1442]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1382/1442]: training_loss: tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1383/1442]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1384/1442]: training_loss: tensor(0.4513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1385/1442]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1386/1442]: training_loss: tensor(0.4338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1387/1442]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1388/1442]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1389/1442]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1390/1442]: training_loss: tensor(0.3184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1391/1442]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1392/1442]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1393/1442]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1394/1442]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1395/1442]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1396/1442]: training_loss: tensor(0.4490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1397/1442]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1398/1442]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1399/1442]: training_loss: tensor(0.3289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1400/1442]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1401/1442]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1402/1442]: training_loss: tensor(0.4768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1403/1442]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1404/1442]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1405/1442]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1406/1442]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1407/1442]: training_loss: tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1408/1442]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1409/1442]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1410/1442]: training_loss: tensor(0.3258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1411/1442]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1412/1442]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1413/1442]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1414/1442]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1415/1442]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1416/1442]: training_loss: tensor(0.4930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1417/1442]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1418/1442]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1419/1442]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1420/1442]: training_loss: tensor(0.4091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1421/1442]: training_loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1422/1442]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1423/1442]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1424/1442]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1425/1442]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1426/1442]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1427/1442]: training_loss: tensor(0.8045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1428/1442]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1429/1442]: training_loss: tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1430/1442]: training_loss: tensor(0.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1431/1442]: training_loss: tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1432/1442]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1433/1442]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1434/1442]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1435/1442]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1436/1442]: training_loss: tensor(0.3371, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1437/1442]: training_loss: tensor(0.2557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1438/1442]: training_loss: tensor(0.4134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1439/1442]: training_loss: tensor(0.4588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1440/1442]: training_loss: tensor(0.6639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1441/1442]: training_loss: tensor(0.7207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/5],global step [7210/7210], pt loss:0.0000, dev loss:0.5641\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier().to(device)  \n",
    "NUM_EPOCHS = 5\n",
    "steps_per_epoch = len(train_iter)\n",
    "optimizer = AdamW(model.parameters(),lr=2e-6,weight_decay=0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*4, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "\n",
    "train(model = model, train_iter=train_iter, dev_iter=dev_iter,optimizer=optimizer,\n",
    "      epochs = NUM_EPOCHS,scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "# Evaluation Function\n",
    "import seaborn as sns\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (source, target), _ in test_loader:\n",
    "                mask = (source != pad_index).type(torch.uint8)\n",
    "                \n",
    "                output = model(source, attention_mask=mask)[0]\n",
    "                y_prob.extend(output.tolist())\n",
    "                y_pred.extend(torch.argmax(output, axis=-1).tolist())\n",
    "                y_true.extend(target.tolist())\n",
    "    \n",
    "    label_true = []\n",
    "    for i in y_true:\n",
    "        if i == 1:\n",
    "            label_true.append([1,0])\n",
    "        else:\n",
    "            label_true.append([0,1])\n",
    "    y_prob_final = []\n",
    "    for i in range(len(y_prob)):\n",
    "        tempA = abs(y_prob[i][0])\n",
    "        tempB = abs(y_prob[i][1])\n",
    "        y_prob_final.append(tempA/(tempA+tempB))\n",
    "\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    plt.figure(1, figsize=(20,8))\n",
    "\n",
    "    ax= plt.subplot(121)\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "    ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "    fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "    plt.subplot(122)\n",
    "    lw = 2\n",
    "    plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "             lw=lw, label='roc curve')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid()\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    print('Accuracy is:', accuracy_score(y_true, y_pred))\n",
    "    print('Precision is:', precision_score(y_true, y_pred))\n",
    "    print('Recall is:', recall_score(y_true, y_pred))\n",
    "    print('F1 score is:', f1_score(y_true, y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7023    0.5279    0.6027      1470\n",
      "           0     0.7715    0.8769    0.8208      2672\n",
      "\n",
      "    accuracy                         0.7530      4142\n",
      "   macro avg     0.7369    0.7024    0.7118      4142\n",
      "weighted avg     0.7469    0.7530    0.7434      4142\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7530178657653308\n",
      "Precision is: 0.702262443438914\n",
      "Recall is: 0.527891156462585\n",
      "F1 score is: 0.6027184466019417\n"
     ]
    }
   ],
   "source": [
    "pred = evaluate(model,dev_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6471    0.6914    0.6685       175\n",
      "           0     0.9171    0.9005    0.9087       663\n",
      "\n",
      "    accuracy                         0.8568       838\n",
      "   macro avg     0.7821    0.7959    0.7886       838\n",
      "weighted avg     0.8607    0.8568    0.8585       838\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJEAAAHwCAYAAAAW3v7qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAB7QklEQVR4nOzdd5hU9fWA8ffs0quIgAqo2LD3XrEXFARRMLbYiFGj+WliSYxGE5OYRJOYGI29C6goKJbYsFfsXSMWFBsWet3v748ZdF0XtjCzd3b3/fDMM3Pv3HJ2L7N799zzPTdSSkiSJEmSJEmLU5Z1AJIkSZIkSSp9JpEkSZIkSZJUI5NIkiRJkiRJqpFJJEmSJEmSJNXIJJIkSZIkSZJqZBJJkiRJkiRJNTKJJBVBRLSNiNsj4puIuGkJtnNgRPy3kLFlISLuiohDs45DkiRJklR/JpHUrEXEjyLi2YiYHhGT88mObQqw6SFAD6BrSmm/+m4kpXR9SmnXAsTzPRHRLyJSRIyuMn/9/PzxtdzObyPiupqWSyntkVK6up7hSpIkNXkR8V5EzMqfl34SEVdFRIcqy2wVEQ9ExLT8xcrbI2KtKst0ioi/R8QH+W29k59epmG/IklNkUkkNVsRcSLwd+AP5BI+KwD/BgYWYPMrAm+llOYXYFvF8jmwVUR0rTTvUOCtQu0gcvw5I0mSVDt7p5Q6ABsAGwKnLXwjIrYE/guMAZYH+gAvAo9FxMr5ZVoB9wNrA7sDnYCtgCnAZsUKOiJaFGvbkkqLf9ypWYqIzsDZwLEppdEppRkppXkppdtTSr/ML9M6f9Xm4/zj7xHROv9ev4iYFBEnRcRn+Sqmw/LvnQWcAQzNX/05omrFTkSslK/4aZGf/nFEvJu/qjQxIg6sNP/RSuttFRHP5K88PRMRW1V6b3xE/C4iHstv5781XHGaC9wGDMuvXw7sD1xf5Xv1j4j4MCKmRsSEiNg2P3934FeVvs4XK8VxTkQ8BswEVs7POzL//kURcXOl7Z8bEfdHRNT2+EmSJDVlKaVPgHvIJZMW+jNwTUrpHymlaSmlL1NKpwNPAr/NL3MIuQujg1JKr6WUKlJKn6WUfpdSurO6fUXE2hFxb0R8GRGfRsSv8vOviojfV1quX0RMqjT9XkScEhEvATMi4vTK53j5Zf4RERfkX3eOiMvz580fRcTv8+efkhoRk0hqrrYE2gC3LmaZXwNbkPvlvT65qzenV3p/WaAz0BM4ArgwIrqklM4kV900MqXUIaV0+eICiYj2wAXAHimljuSuFr1QzXJLA+Pyy3YFzgfGVakk+hFwGNAdaAX8YnH7Bq4hd7IBsBvwKvBxlWWeIfc9WBq4AbgpItqklO6u8nWuX2mdg4HhQEfg/SrbOwlYL58g25bc9+7QlFKqIVZJkqRmISJ6AXsA7+Sn25E7R6yu1+YoYJf8652Bu1NK02u5n47AfcDd5KqbViVXyVRbBwD9gaWAa4E9I6JTftsLL1DekF/2amB+fh8bArsCR9ZhX5JKgEkkNVddgS9qGG52IHB2/urN58BZ5JIjC83Lvz8vf2VnOtC3nvFUAOtERNuU0uSU0qvVLNMfeDuldG1KaX5K6UbgDWDvSstcmVJ6K6U0i9wJxQaL22lK6XFg6YjoSy6ZdE01y1yXUpqS3+d5QGtq/jqvSim9ml9nXpXtzQQOIpcEuw74WUppUnUbkSRJamZui4hpwIfAZ8CZ+flLk/vbbXI160wGFlafd13EMouyF/BJSum8lNLsfIXTU3VY/4KU0ocppVkppfeB54B98u/tCMxMKT0ZET3IJcV+nh8B8BnwN/IV8ZIaD5NIaq6mAMvUMH57eb5fRfN+ft6326iShJoJfK/5YW2klGYAQ4GjgckRMS4i1qhFPAtj6llp+pN6xHMtcBywA9VUZuWH7L2eH0L3Nbnqq5oaM364uDdTSk8D7wJBLtklSZIk2Cdfmd4PWIPvzrm+InfRcblq1lkO+CL/esoillmU3sD/6hVpTtVzvhvIVSdBrkJ+YRXSikBLcue6X+fPKf9DrnpeUiNiEknN1RPAbL67UlKdj8n9wltoBX441Ku2ZgDtKk0vW/nNlNI9KaVdyP3SfwO4tBbxLIzpo3rGtNC1wDHAnfkqoW/lh5udQq4UuUtKaSngG3LJH4BFDUFb7NC0iDiWXEXTx8DJ9Y5ckiSpCUopPQRcBfw1Pz2D3PlrdXf93Z/vhqDdB+yWb5dQGx8CqyzivcWevy4Mtcr0TUC//HC8QXyXRPoQmAMsk1JaKv/olFJau5ZxSioRJpHULKWUviHX/PrCiNgnItpFRMuI2CMi/pxf7Ebg9Ijolm9QfQa54Vf18QKwXUSskG/qXflOGz0iYkD+l/0ccsPiFlSzjTuB1SPiRxHRIiKGAmsBd9QzJgBSShOB7cn1gKqqI7mx658DLSLiDHJ3+VjoU2ClqMMd2CJideD35Ia0HQycHBEb1C96SZKkJuvvwC6VzpNOBQ6NiOMjomNEdMk3vt6SXNsFyF0c/BC4JSLWiIiyiOgaEb+KiD2r2ccdwLIR8fPI3VSmY0Rsnn/vBXI9jpaOiGWBn9cUcL4FxHjgSmBiSun1/PzJ5O4sd15EdMrHtUpEbF/H74mkjJlEUrOVUjofOJFcs+zPyf3CPY7cHcsgl+h4FngJeJncGO/f/2BDtdvXvcDI/LYm8P3ETxm5ZtMfA1+SS+gcU802ppAbt34SuVLlk4G9UkpfVF22HvE9mlKqrsrqHuAu4C1yQ+dm8/2y5YXNHadExHM17Sc/fPA64NyU0osppbfJ3eHt2sjf+U6SJEnfJmSuAX6Tn36U3I1QBpPre/Q+uQbV2+TPqUgpzSHXXPsN4F5gKvA0uWFxP+h1lFKaRq4p997k2iK8Ta7FAeQSUi8C75FLAI2sZeg35GO4ocr8Q8jd+OU1csPzbqZuQ+8klYDwhkiSJEmSJEmqiZVIkiRJkiRJqpFJJEmSJEmSJNXIJJIkSZIkSZJqZBJJkiRJkiRJNTKJJEmSJEmSpBq1yDqARXnvi9neNk6qhWWXapN1CFKj0KYFUex9tN3wuKL87pr1/L+KHru00FJLLZVWXXXVrMNQJTNmzKB9+/ZZh6EqPC6lx2NSmjwupWfChAlfpJS61Wfdkk0iSZIkqeH16NGDZ599NuswVMn48ePp169f1mGoCo9L6fGYlCaPS+mJiPfru65JJEmSCiUcJS5JkqSmy7NdSZIkSZIk1chKJEmSCiVsXSRJkqSmy0okSZIkSZIk1chKJEmSCsWeSJIkSWrCTCJJklQoDmeTJElSE+YlU0mSJEmSJNXISiRJkgrF4WySJElqwjzblSRJkiRJUo2sRJIkqVDsiSRJkqQmzCSSJEmF4nA2SZIkNWGe7UqSJEmSJKlGViJJklQoDmeTJElSE2YlkiRJkiRJkmpkJZIkSYViTyRJkiQ1YZ7tSpJUKBHFeUjViIgrIuKziHhlEe9HRFwQEe9ExEsRsVFDxyhJkpoWk0iSJEmN01XA7ot5fw9gtfxjOHBRA8QkSZKaMIezSZJUKA5nUwNKKT0cESstZpGBwDUppQQ8GRFLRcRyKaXJDROhJElqakwiSZIkNU09gQ8rTU/KzzOJJElSczK6P0y8k/++uQobLP/JEm3KJJIkSYVi/yKVlur+Q6ZqF4wYTm7IG926dWP8+PFFDEt1NX36dI9JCfK4lB6PSWnyuGRv+3fv5C/jt+bUO3dmm5U+AK6s97ZMIkmSJDVNk4DelaZ7AR9Xt2BK6RLgEoC+ffumfv36FT041d748ePxmJQej0vp8ZiUJo9LtmbNmsdBJw7mhufXA2CXww7lkTPqn0SyeYMkSYUSZcV5SPUzFjgkf5e2LYBv7IckSVLzMWnSVLbb7ipueH49OrSew623DuU3v9l+ibZpJZIkSYViwkcNKCJuBPoBy0TEJOBMoCVASuli4E5gT+AdYCZwWDaRSpKkhvbEH4cy6I8r8Om0DvRZ+ivGHnYj6+zzhyXerkkkSZKkRiildEAN7yfg2AYKR5IklZC7H/yMT6etxY6rvsuog2+i6zo7FGS7JpEkSSqUMhtrS5IkKXtn7vIQvZeayqHXP03LllcXbLvW3UuSJEmSJDViX345i4MPvpWPP54GQFlZ4sjNn6Nly/KC7sdKJEmSCsWeSJIkSWpgr776GQMHjuB///uKqVPnMGbMsKLtyySSJEmFEg5nkyRJUsMZO/ZNDjxwNNOnz2WjjZbjX//ao6j785KpJEmSJElSI5JS4pxzHmaffUYwffpchg1bh0ceOYzevTsXdb9WIkmSVCgOZ5MkSVKRpZT40Y9GM2LEK0TAH/+4E6ecsjWxsCp+dP+i7dskkiRJkiRJUiMREayzTjc6dmzFDTfsy157rf79BSbemXvus2fB920SSZKkQrEnkiRJkopk1qx5tG3bEoBf/WpbDj54fVZYYTHD1waPK3gM1t1LklQoUVachyRJkpq1//znWVZf/V988ME3QK4aabEJpCLxzFSSJEmSJKkEzZu3gGOOGcfRR49j0qSp3Hrr65nG43A2SZIKxeFskiRJKpDPP5/BfvvdxEMPvU+rVuVccsleHHroBpnGZBJJkiRJkiSphLz00qcMGHAj77//Dcsu24Fbbx3KFlv0yjosk0iSJBWM/YskSZK0hKZMmcm2217J1Klz2HTT5bn11qH07Nkp67AAk0iSJBWOw9kkSZK0hLp2bccZZ2zHCy98yiWX7PXtHdlqZXT/4gWGSSRJkiRJkqRMTZ8+l7ffnsKGGy4HwIknbgnk7sJWJxPvzD332bOQ4X3LuntJkgolyorzkCRJUpM1ceJXbLXV5ey887W8++5XQC55VOcEUmWDxxUouu/zzFSSJEmSJCkDDz44kU03vZSXX/6Mbt3asWBBRdYhLZbD2SRJKhR7IkmSJKkWUkr8+9/PcMIJd7NgQWKPPVblxhv3pXPnNvXb4Oj+3w1lKyIrkSRJkiRJkhrI3LkL+MlP7uC44+5iwYLEySdvxe23H1D/BBJ8P4FUpH5IYCWSJEmFY/8iSZIk1eCFFz7hiiuep02bFlx22d4ceOB6hdv4Salw26qGSSRJkgrFJJIkSZJqsNlmPbn88gGsvXZ3Ntlk+SXf4Oj+S76NWjKJJEmSJEmSVEQjR75C585t2H33VQE49NANCrfxhUPZijiMbSGTSJIkFYqNtSVJklRJRUXiN795gD/84VE6d27N668fy3LLdSzOzgaPK852KzGJJEmSJEmSVGBTp87hoINGc/vtb1FeHpx99g4su2yHrMNaIiaRJEkqFHsiSZIkCXjnnS8ZMOBGXn/9C7p0acOoUfux884rF24Ho/t//45sDcQkkiRJheJwNkmSpGbvgQcmMmTIKL76ajZrrdWNMWOGseqqSxd2J1UTSA3QDwlMIkmSJEmSJBVM69blTJ8+l733Xp3rrhtMp06ti7ezk1Lxtl0Nk0iSJBWKw9kkSZKapYqKRFlZrip9661X4PHHj2CjjZb7dl5T4dmuJEmSJElSPX3yyXS22+5Kxox549t5m2yyfJNLIIGVSJIkFY49kSRJkpqVZ5/9mH32GcFHH03j668fYK+9Vqe8vMj1OqP7F3f7i2ESSZKkAgmTSJIkSc3G9de/xJFH3s7s2fPZdtsVuPnm/YufQILvmmo3UDPtyhzOJkmSJEmSVEsLFlRwyin3ctBBtzJ79nyGD9+I++47hO7d2xd3x6P7w3mVLloOHlfc/VXDSiRJkgrESiRJkqSm7yc/uYPLL3+eFi3KuOCC3fnpTzdtmB0vrECCTKqQwCSSJEmSJElSrf3kJxtzzz3/49prB9Gv30oNH8BJqeH3mWcSSZKkQrEQSZIkqUl6550vWXXVpQHYdNOevPPOz2jdugFTKhk2067MnkiSJEmSJEnVSCnx178+Tt++/2LUqFe/nd+gCSTItJl2ZVYiSZJUIPZEkiRJajpmzZrH8OF3cN11LwEwceJXDR/E6P7f74WUQTPtykwiSZJUICaRJEmSmoaPPprKoEEjeeaZj2nfviXXXjuIQYPWbPhASqCZdmUmkSRJkiRJkvKefHISgwaN5JNPptOnz1KMGTOMddftkW1QGTbTrswkkiRJBWIlkiRJUuM2f34Fhx56G598Mp0ddliJUaP2Y5ll2jVsEFWHsJUQk0iSJEmSJElAixZljBo1hKuvfpFzz92Zli3LGz6IqgmkEhjGtpBJJEmSCsRKJEmSpMbnq69mMXr06xxxxEYArL/+spx//rIZR0XJDGGrzCSSJEmFYg5JkiSpUXnttc8ZOHAE77zzJW3btuRHP1o365BKmkkkSZIkSZLU7Nx++5sceOBopk2bywYbLMs226yQdUglzySSJEkF4nA2SZKk0pdS4o9/fJTTT3+AlGD//dfmiisG0L59q6xDK3kmkSRJkiRJUrMwc+Y8Dj98DCNHvgrAOefsyGmnbePFwFoyiSRJUoF48iFJklTaZs+ez7PPfkyHDq24/vrBDBjQN+uQGhWTSJIkFYhJJEmSpNK29NJtGTv2AFJKrL1296zDaXTKsg5AkiQtuYh4LyJejogXIuLZ/LylI+LeiHg7/9yl0vKnRcQ7EfFmROyWXeSSJEnFddllz3Hiifd8O73WWt1KM4E0uj+cV9oXJa1EkiSpQEqgEmmHlNIXlaZPBe5PKf0pIk7NT58SEWsBw4C1geWB+yJi9ZTSgoYPWZIkqTjmzVvA//3fPVx44TMA7LffWmy5Ze+Mo1qMiXd+97rPntnFsRgmkSRJaroGAv3yr68GxgOn5OePSCnNASZGxDvAZsATGcQoSZJUcF98MZP997+JBx98j1atyrn44v6lm0Aa3f/7CaSTUnax1MAkkiRJhZJtIVIC/hsRCfhPSukSoEdKaTJASmlyRCys2+4JPFlp3Un5eZIkSY3eyy9/ysCBI5g48WuWXbYDo0fvX7oJJGgUFUgLmUSSJKnERcRwYHilWZfkk0SVbZ1S+jifKLo3It5Y3CarmVe6l7wkSZJq6dFHP2D33a9jxox5bLLJ8tx661B69eqUdVi1U8IVSAuZRJIkqUCK1RMpnzCqmjSquszH+efPIuJWcsPTPo2I5fJVSMsBn+UXnwRUvhzXC/i48JFLkiQ1rHXX7U7v3p3ZeOPluPTSvWnbtmXWITUp3p1NkqQCiYiiPGqx3/YR0XHha2BX4BVgLHBofrFDgTH512OBYRHROiL6AKsBTxf42yFJktQgZsyYy5w58wHo3LkNjz56GNdeO8gEUhFYiSRJUuPXA7g1n3BqAdyQUro7Ip4BRkXEEcAHwH4AKaVXI2IU8BowHzjWO7NJkqTG6L33vmbgwBFsuunyXHrp3kQEXbu2yzqsJsskkiRJBVKs4Ww1SSm9C6xfzfwpwE6LWOcc4JwihyZJklQ048e/x5Aho5gyZRazZ8/n669n06VL26zDatIcziZJkiRJkhqViy56hl12uZYpU2ax++6r8tRTR5pAagBWIkmSVCjZFCJJkiQ1G3PnLuD44+/iP/+ZAMAvf7kVf/zjTpSXWyPTEEwiSZJUIFkNZ5MkSWou/vCHR/jPfybQunU5l102gIMOWi/rkJoVk0iSJEmSJKlR+MUvtuKppz7i7LP7semmPbMOp9kxiSRJUoFYiSRJklR4d9/9DttvvyJt27akQ4dW3HXXgVmH1Gw5aFCSJEmSJJWciorEGWc8yB57XM9RR91OSinrkApvdP+sI6gTK5EkSSoQK5EkSZIKY9q0ORx88K2MGfMmZWXBJpssn3VIxTHxztxznz2zjaOWTCJJklQgJpEkSZKW3P/+9yUDB47g1Vc/p0uXNowcOYRddlkl67CKa/C4rCOoFZNIkiRJkiSpJNx//7vsv//NfPnlLNZccxnGjj2AVVddOuuwlGcSSZKkQrEQSZIkaYlcddWLfPnlLPbee3Wuu24wnTq1zjokVWISSZIkSZIklYT//GcvNt+8J8ccsyllZV6hKzXenU2SpAKJiKI8JEmSmqpPP53OUUeNZcaMuQC0a9eS447bzARSibISSZIkSZIkNbgJEz5mn31GMmnSVFq1KufCCxvX7e6bI5NIkiQViFVDkiRJtXPjjS9z+OFjmT17Pltv3Zszztg+65BUCyaRJEkqEJNIkiRJi7dgQQW//vUDnHvuYwAceeSGXHhhf1q1Ks84MtWGSSRJkiRJklR0c+bMZ999RzFu3NuUlwf/+MfuHHPMpl6Ia0RMIkmSVCie/0iSJC1Sq1bldOvWnq5d23LTTfuxww59sg5JdeTd2SRJkhqpiNg9It6MiHci4tRq3u8cEbdHxIsR8WpEHJZFnJKk5m3u3AVAbuj/xRf3Z8KE4SaQRveH8xrfFUiTSJIkFUhEFOUhVSciyoELgT2AtYADImKtKosdC7yWUlof6AecFxGtGjRQSVKzlVJi1KgP2WSTS5g6dQ4ArVu3YMUVl8o2sFIw8c7vXvfZM7s46sjhbJIkFYgJHzWwzYB3UkrvAkTECGAg8FqlZRLQMXL/OTsAXwLzGzpQSVLzM3v2fIYPv51rr30XgDvvfJthw9bJOKoSdFLKOoI6MYkkSZLUOPUEPqw0PQnYvMoy/wLGAh8DHYGhKaWKqhuKiOHAcIBu3boxfvz4YsSrepo+fbrHpAR5XEqPx6R0fPHFHH7zm1d5441ptG5dxmmnrcGyy37h8amkX/65sX1PTCLpW+f94QyeeuxhluqyNJdcNxqAS/91Pk8+9hAtW7ZkuZ69OOlXZ9OhYyemfvM1v/v1Sbz1xqvssscAjjvpVxlHL2Vnj112pF379pSXlVHeopwbR43+9r2rr7yc8//6Z8Y/+gRduiydYZRqCFYiqYFV9x+u6uXM3YAXgB2BVYB7I+KRlNLU762U0iXAJQB9+/ZN/fr1K3iwqr/x48fjMSk9HpfS4zEpDU89NYnjjx/J5MnTWXHFzpx++qoceeReWYdVeibknhrb/1l7Iulbu+45kHPOv+h78zbadAsuufYWLr7mZnr2XpER114OQKtWrTj0qGM56tgTswhVKjmXXXk1o0aP+V4C6ZPJk3ni8cdZbrnlM4xMUhM2CehdaboXuYqjyg4DRqecd4CJwBoNFJ8kqZl5++0pbL/9VUyePJ3tt1+RZ545ilVX7ZB1WCogk0j61robbEzHTp2+N2/jzbeivEWuYG3Ntdfji88+A6BN23ass/5GtGrVusHjlBqLv5z7R/7vpF9andKM2FhbDewZYLWI6JNvlj2M3NC1yj4AdgKIiB5AX+DdBo1SktRsrLZaVw47bAN++tNNuPfeg+nWrX3WIanAijacLSL2SCndVWXe0Smli4u1TxXXPeNuY/uddss6DKn0BBx91BFEBEP2G8qQ/Ycy/oH76d6jO33X8IJ/s2K+Rw0opTQ/Io4D7gHKgStSSq9GxNH59y8GfgdcFREvk/sfekpK6YvMgpYkNTlffTWLL7+cxSqr5Fo3XHhhf8rKPClqqorZE+k3ETEnpfQAQEScQq53lEmkRuiGqy+lvLycHXftn3UoUsm5+rob6d69B1OmTOHoIw+jz8orc+klF3PxpVdkHZqkJi6ldCdwZ5V5F1d6/TGwa0PHJUlqHl5//XMGDhxBSvD000fSpUtbE0hNXDGHsw0A/hAR20bEOeRuQztgcStExPCIeDYinr3hmsuLGJrq4t47x/L0Yw9zypl/dFiFVI3u3XsA0LVrV3bceReefeZpPvpoEvsPHsgeu+zIp59+wrAhg/ni888zjlTF5nA2SZLUXIwb9xabb34Zb7/9Je3bt2T69LlZh6QGULRKpJTSFxExALiPXN/xISmlqncMqbrOt3cGee+L2YtdVg3jmScfY9T1V/KXf11OmzZtsw5HKjkzZ84kpQrat+/AzJkzeeLxx/jJ0ccw/pEnvl1mj1125IZRN3t3NkmSJDV6KSXOPfcxfvWr+0kJ9ttvLa68ciDt27fKOjQ1gIInkSJiGrnby0b+uRWwMjAkIlJKqdPi1ld2/njmKbz0/LN88/XXHLjPLhx8xE8Zce0VzJs3l9N+fjQAa6y9Liec/BsADtl3D2bMmM78+fN44pEH+cPfLmbFPqtk+SVIDe7LKVP4v+OPBWD+ggXs2X8vtt52u4yjUlasGpIkSU3ZzJnzOPLIsdx44ysA/O53O/DrX2/rOVAzUvAkUkqpY6G3qYZx2lnn/mDe7nsPXuTy19xy1yLfk5qLXr17c9OtVW+G9H133ftAA0UjSZIkFc8997zDjTe+QocOrbjuukEMHOhNZOpldOPtNVzMu7MNAh5IKX2Tn14K6JdSuq1Y+5QkKUtehJMkSU3ZoEFr8qc/7UT//quzzjrdsw6n8ZqYvydGnz2zjaMeitlY+8yFCSSAlNLXwJlF3J8kSZmysbYkSWpqrrzyeV566dNvp085ZRsTSEuichXS4HHZxVFPxUwiVbftolU+SZIkSZKkwpg3bwHHH38Xhx8+loEDRzBjhndfK4hGXIUExU3qPBsR5wMXkmuw/TNyd2mTJKlJsmhIkiQ1BVOmzGT//W/mgQcm0rJlGaefvq13Xyu0RliFBMVNIv0M+A0wktyd2v4LHFvE/UmSJEmSpCXwyiufMXDgCN599yt69GjP6NFD2Wqr3lmHpRJRtCRSSmkGcGqxti9JUqmxf5EkSWrMxox5g4MOupXp0+ey8cbLceutQ+ndu3PWYamEFPPubN2Ak4G1gTYL56eUdizWPiVJypI5JEmS1JjNnDmP6dPncsAB63DZZQNo165l1iGpxBRzONv15Iay7QUcDRwKfF7E/UmSJEmSpDpIKX1bTX3AAeuy/PId2W67Fa2wVrWKeXe2rimly4F5KaWHUkqHA1sUcX+SJGWqrCyK8pAkSSqG99//mq22uoJnn/3423nbb7+SCSQtUjGTSPPyz5Mjon9EbAj0KuL+JEmSJElSLTz88PtsssmlPPnkJE4++d6sw1EjUczhbL+PiM7AScA/gU7Az4u4P0mSMuVFO0mS1BhcfPGz/OxndzF/fgW77roKI0bsm3VIaiSKmUT6KqX0DfANsANARGxdxP1JkpQpS78lSVIpmzdvASeccDcXXfQsACedtCV/+tPOtGhRzEFKakqKmUT6J7BRLeZJkiRJkqQiSikxaNBIxo17m9aty7nkkr055JD1sw5LjUzBk0gRsSWwFdAtIk6s9FYnoLzQ+5MkqVRYiCRJkkpVRHD44RvywgufMHr0UDbbrGfWIakRKkYlUiugQ37bHSvNnwoMKcL+JEmSJElSNT788Bt69+4MwODBa7L77qvSrl3LjKNSY1XwJFJK6SHgoYiYlVL6c+X3ImI/4O1C71OSpFJgTyRJklQqKioSZ501nnPPfYwHHzyULbfsDWACKUuj+2cdwRIrZvesYdXMO62I+5MkSZIkqdmbNm0OQ4aM4uyzH2bevApefvmzrEMSwMQ7c8999sw2jiVQjJ5IewB7Aj0j4oJKb3UE5hV6f5IklQorkSRJUtbeffcrBg4cwSuvfMZSS7VhxIh92W23VbMOS5UNHpd1BPVWjJ5IHwMTgAH554VWBGYWYX+SJJUEc0iSJClLDzwwkf32u4kvv5zFGmssw5gxw1h99a5ZhyVoEkPZoAjD2VJKL6aUrgJWBV4E1gbOAnYAXi/0/iRJkiRJau6mTZvzbQKpf//VePLJI0wglZImMJQNijOcbXVy/ZAOAKYAI4FIKe1Q6H1JklRKHM4mSZKy0rFja665Zh8effQDfv/7HSkvL2YLZNVJ5SqkRjyUDYoznO0N4BFg75TSOwAR8X9F2I8kSZIkSc3Wp59O54knJrHPPmsA0L//6vTvv3rGUekHmkgVEhTn7mz7Ap8AD0bEpRGxE+ClWUlSkxdRnIckSVJVzz03mU02uZT99ruJhx9+P+twVBuNvAoJitMT6daU0lBgDWA88H9Aj4i4KCJ2LfT+JEkqFRFRlIckSVJlI0a8wjbbXMGkSVPZbLOe9j5SgynaIMmU0oyU0vUppb2AXsALwKnF2p8kSZIkSU3ZggUVnHbafRxwwC3MmjWfww/fgAceOIRll+2QdWhqJorRE+kHUkpfAv/JPyRJapIsGpIkScUydeocfvSjWxg37m3Ky4O//W03jjtuM6uW1aAaJIkkSZIkSZLq7/PPZ/D44x/SpUsbbrppP3baaeWsQ1IzZBJJkqQC8UqgJEkqllVWWZrbbhtGz54dWWWVpbMOR82USSRJkgrEHJIkSSqUlBJ///uTlJeXcfzxmwOw3XYrZhyVmjuTSJIkSZIklZDZs+dz9NF3cPXVL1JeHuy11+qsvHKXrMOSTCJJklQoDmeTJElLavLkaQwaNJKnnvqIdu1actVVA00gNWaj+2cdQUGZRJIkSZIkqQQ8/fRHDBo0ko8/nsYKK3RmzJhhbLDBslmHpSUx8c7cc589s42jQEwiSZJUIBYiSZKk+rr99jfZb7+bmDNnAdtuuwI337w/3bu3zzosFcrgcVlHUBAmkSRJkiRJyth66/WgY8fW/PjHa3LBBXvQqlV51iFJP2ASSZKkArEnkiRJqovp0+fSvn1LIoIVV1yKl146muWW65h1WCqUJtYPCaAs6wAkSWoqIorzkCRJTc8bb3zBxhtfwrnnPvbtPBNITUwT64cEJpEkSZIkSWpQd975NptvfhlvvTWFkSNfZe7cBVmHpGJqIv2QwCSSJEkFExFFeUiSpKYhpcS55z7KXnvdwNSpc9h33zV55JHD7H+kRsOeSJIkSZIkFdmsWfM48sjbueGGlwE466x+nH76dpSVecFIjYdJJEmSCsSiIUmStCjHH38XN9zwMu3bt+TaawcxaNCaWYck1ZlJJEmSCsShZ5IkaVF++9t+vP76F1x0UX/WXbdH1uFI9WJPJEmSmoiIKI+I5yPijvz00hFxb0S8nX/uUmnZ0yLinYh4MyJ2yy5qSZKarnvv/R8VFQmAnj078cgjh5lAUqNmEkmSpAIpgcbaJwCvV5o+Fbg/pbQacH9+mohYCxgGrA3sDvw7IuzoKUlSgcyfX8HPf343u+56HWef/dC3861aVmNnEkmSpCYgInoB/YHLKs0eCFydf301sE+l+SNSSnNSShOBd4DNGihUSZKatC+/nMUee1zPP/7xFC1bltGrV6esQ5IKxp5IkiQVSMYXF/8OnAx0rDSvR0ppMkBKaXJEdM/P7wk8WWm5Sfl5kiRpCbz66mcMHDiC//3vK7p3b88tt+zPNtuskHVYUsFYiSRJUoEUazhbRAyPiGcrPYZX2e9ewGcppQm1DbWaeWmJvwGSJDVjY8e+yRZbXM7//vcVG220HM88c5QJJDU5ViJJklTiUkqXAJcsZpGtgQERsSfQBugUEdcBn0bEcvkqpOWAz/LLTwJ6V1q/F/BxEUKXJKlZqKhI/OUvjzN9+lyGDVuHyy8fQLt2LbMOS1ka3T/rCIrCSiRJkgokojiPmqSUTksp9UoprUSuYfYDKaWDgLHAofnFDgXG5F+PBYZFROuI6AOsBjxd4G+HJEnNRllZcPPN+/GPf+zODTcMNoEkmHhn7rnPntnGUWAmkSRJarr+BOwSEW8Du+SnSSm9CowCXgPuBo5NKS3ILEpJkhqhDz74hl/84r8sWFABQI8eHTj++M29A5u+b/C4rCMoKIezSZJUIKVw0phSGg+Mz7+eAuy0iOXOAc5psMAkSWpCHn30AwYPHsnnn8+ke/f2nHzy1lmHJDUIk0iSJBVICeSQJElSkV166QSOPfZO5s2rYJddVuaoozbKOiSpwTicTZIkSZKkGsybt4DjjruT4cPvYN68Cv7v/7bgzjsPpEuXtlmHplLTRJtqg5VIkiQVTJmlSJIkNUnffDObQYNG8uCD79GqVTmXXLIXhx66QdZhqVQ10abaYBJJkiRJkqTFWni3tWWX7cCttw5liy16ZRyRGoUm1lQbTCJJklQwFiJJktS0LFhQQXl5GS1bljNq1H7MmTOfnj07ZR2WlBl7IkmSJEmSVElFReK3vx3PHntcz/z5FQAss0w7E0hq9qxEkiSpQMJSJEmSGr3p0+dy6KG3MXr065SVBQ8//D477tgn67DUWDThptpgEkmSpIIpM4ckSVKjNnHiVwwcOIKXX/6Mzp1bM2LEEBNIqpsm3FQbTCJJkiRJksT48e8xZMgopkyZRd++XRkzZhh9+y6TdVhqLEb3/y6BBE2yqTaYRJIkqWAcziZJUuP0+OMfsssu1zJ/fgV77LEqN9ywL0st1SbrsNSYVE4gNdEqJDCJJEmSJElq5jbfvCc77dSH9dfvwR/+sBPl5d6DSotQteKoqpNSw8WSAZNIkiQViIVIWhIR0T6lNCPrOCSpufjssxlEQLdu7SkvL+P22w+gZcvyrMNSqVtcAqkJVyAtZBJJkqQCCcwiqe4iYivgMqADsEJErA/8JKV0TLaRSVLT9fzzkxk4cAQrrbQU9913CK1alZtAUt008YqjRbFGT5IkKVt/A3YDpgCklF4Etss0IklqwkaOfIWtt76CDz+cyrx5FUybNifrkNQYjO4P53nB0EokSZIKpMzzCtVTSunDKo3ZF2QViyQ1VRUViTPOeJBzznkEgB//eAMuvrg/rVv7Z7FqoZk0zq6JnxZJkqRsfZgf0pYiohVwPPB6xjFJUpMydeocDjpoNLff/hZlZcF55+3KCSds7p1VVXfNdBjbQiaRJEkqEE9EVU9HA/8AegKTgP8C9kOSpAK68srnuf32t+jSpQ2jRu3HzjuvnHVIUqNkEkmSpAIxh6R66ptSOrDyjIjYGngso3gkqcn52c825/33v+GYYzZl1VWXzjocqdGysbYkSVK2/lnLeZKkWkopcfHFz/Lxx9MAKCsLzj9/NxNIqpuFzbRtqP0tK5EkSSqQMkuRVAcRsSWwFdAtIk6s9FYnwPtMS1I9zZkzn6OPHsdVV73A1Ve/yKOPHkZ5ufUTqofKzbShWTfUXsgkkiRJUjZaAR3InY91rDR/KjCkNhuIiN3J9VMqBy5LKf2pmmX6AX8HWgJfpJS2X5KgJamUTZ48jcGDR/Hkk5No27YFP//55iaQVHej+38/gdTMm2lXZhJJkqQCsRBJdZFSegh4KCKuSim9X9f1I6IcuBDYhVxD7mciYmxK6bVKyywF/BvYPaX0QUR0L0z0klR63nhjKgcddCkffTSN3r07MWbMMDbccLmsw1JjVDmBZPXR95hEkiRJytbMiPgLsDbQZuHMlNKONay3GfBOSuldgIgYAQwEXqu0zI+A0SmlD/Lb/KyQgUtSqbjhhpc54YQXmTu3gm22WYGbb96PHj06ZB2WGqPR/b97bQXSD5hEkiSpQMJSJNXP9cBIYC/gaOBQ4PNarNcT+LDS9CRg8yrLrA60jIjx5IbM/SOldE3VDUXEcGA4QLdu3Rg/fnzdvgIV1fTp0z0mJcjjUloeeOB95s6tYK+9luP441fk9def5fXXs45K0Pg+K/3yVUhTOm3Oy40o7oZiEkmSpAIxh6R66ppSujwiTqg0xO2hWqxX3f+4qpdMWwAbAzsBbYEnIuLJlNJb31sppUuASwD69u2b+vXrV9evQUU0fvx4PCalx+NSWrbfPrHaaqM5+eTBXtQpMY3uszIh99T1qCfpl2kgpckOY5IkSdmal3+eHBH9I2JDoFct1psE9K403Qv4uJpl7k4pzUgpfQE8DKy/pAFLUtbeemsK229/FR988A2QqwbefPOuJpBUf6P7w3n+/6mJSSRJkgqkLKIoDzV5v4+IzsBJwC+Ay4Cf12K9Z4DVIqJPRLQChgFjqywzBtg2IlpERDtyw90c4CGpUbv77nfYbLNLefjh9/n1rx/IOhw1FTbTrhWHs0mSJGUopXRH/uU3wA4AEbF1LdabHxHHAfcA5cAVKaVXI+Lo/PsXp5Rej4i7gZeACuCylNIrxfg6JKnYUkqcd94TnHLKfVRUJAYNWoOLLupf84pSTWymXWs1JpEi4gTgSmAauStjGwKnppT+W+TYJElqVKwZUl1ERDmwP7kG2XenlF6JiL2AX5HrX7RhTdtIKd0J3Fll3sVVpv8C/KVQcUtSFmbPns9RR93Odde9BMBvf7s9v/nN9pSV+dtXBbCwCskKpBrVphLp8JTSPyJiN6AbcBi5pJJJJEmSKrEPg+rocnI9jZ4GLoiI94EtyV2suy3LwCSplMyfX8GOO17NE09Mon37llxzzSAGD14z67DUFA0el3UEJa82SaSFZ8R7AlemlF4Mz5IlSZKW1CbAeimliohoA3wBrJpS+iTjuCSppLRoUcbgwWsyefJ0xowZxnrr9cg6JKnZqk0SaUJE/BfoA5wWER3JjamXJEmVWFGvOpqbUqoASCnNjoi3TCBJ0nc++2wG3bu3B+Ckk7Zk+PCN6dSpdcZRSc1bbe7OdgRwKrBpSmkm0IrckDZJkiTV3xoR8VL+8XKl6Zcj4qWsg5OkrMyfX8GJJ97DWmtdyLvvfgXkhoybQFJRjLY5e10sshIpIjaqMmtlR7FJkrRo/p5UHdnQQ5Kq+OqrWQwdejP33vsuLVqUMWHCx6y8cpesw1JTZlPtOlnccLbzFvNeAnYscCySJDVq5pBUFyml97OOQZJKyeuvf86AASN4550v6datHbfcsj/bbrti1mGpKatchWRT7VpZZBIppbRDQwYiSZIkSWqe7rjjLX70o1uYNm0uG264LLfdNowVVuicdVhq6qxCqrMaG2tHRDvgRGCFlNLwiFgN6JtSuqPo0UmS1Ig4nE2SpLqbNGkq++47irlzF7D//mtz5ZUDadeuZdZhqSkb3f+7BBJYhVQHtbk725XABGCr/PQk4CbAJJIkSVIBRERbchfs3sw6FklqaL16deL883flm2/mcNpp23hRRsVXOYFkFVKd1CaJtEpKaWhEHACQUpoVfqolSfqBMn87qh4iYm/gr+TugNsnIjYAzk4pDcg0MEkqog8//IaJE79mu+1yPY+OPXazjCNSs3RSyjqCRqesFsvMzV8dSwARsQowp6hRSZIkNR+/BTYDvgZIKb0ArJRZNJJUZI899gGbbHIpAwbcyFtvTck6HDU3lZtpq85qU4l0JnA30Dsirge2Bn5czKAkSWqMLNRVPc1PKX3j/x9JzcFllz3HMceMY968CnbeeWWWWaZd1iGpubGZ9hKpMYmUUro3Ip4DtgACOCGl9EXRI5MkqZExBaB6eiUifgSU529gcjzweMYxSVJBzZu3gBNPvId//esZAH7+8835y192pUWL2gyOkYrAZtr1UptKJIDtgW3IDWlrCdxatIgkSZKal58BvybXLuAG4B7g95lGJEkFNGXKTPbb7yYefPA9WrUq5+KL+3PYYRtmHZakeqgxiRQR/wZWBW7Mz/pJROycUjq2qJFJktTIlDkcSfXTN6X0a3KJJElqct56awqPPvoBPXq059Zbh7Lllr2zDklSPdWmEml7YJ2U0sLG2lcDLxc1KkmSpObj/IhYDrgJGJFSejXrgCSpkLbcsjcjRw5h00170qtXp6zDUXM1uv93/ZBUb7UZgPomsEKl6d7AS8UJR5KkxiuiOA81bSmlHYB+wOfAJRHxckScnm1UklR/FRWJs89+iDFj3vh23qBBa5pAUrYqJ5Bsql1vi6xEiojbyfVA6gy8HhFP56c3x2aPkiT9gHfXUn2llD4BLoiIB4GTgTOwL5KkRmj69Ln8+Me3ccstr9O5c2smTjyBLl3aZh2W9J2TUtYRNGqLG8721waLQpIkqZmKiDWBocAQYAowAjgp06AkqR7ee+9rBg4cwUsvfUqnTq254YZ9TSBJTcwik0gppYcaMhBJkho7C5FUT1eSu4HJrimlj7MORpLq46GH3mPIkJv44ouZrL56V8aMGcYaayyTdVhq7uyDVHC1uTvbFsA/gTWBVkA5MCOl5IBWSZKkJZRS2iLrGCRpSVxzzYscccRY5s+vYPfdV+XGG/dlqaXaZB2W9MMEkr2Qllht7s72L2AYuTuGbAIcAqxWzKAkSWqMyixFUh1ExKiU0v4R8TK5vpPfvgWklNJ6GYUmSXWyxhrLUF4e/PznW/KnP+1MeXlt7t8kNSD7IBVMbZJIpJTeiYjylNIC4MqIsLG2JElVmENSHZ2Qf94r0ygkqR5mzZpH27YtAdhss5688cZxrLTSUtkGJVU2un/WETRJtUkRz4yIVsALEfHniPg/oH2R45IkSWrSUkqT8y+PSSm9X/kBHJNlbJK0OC+88AlrrnkhN9306rfzTCCp5CwcyuYQtoKqTRLp4PxyxwEzgN7A4GIGJUlSYxQRRXmoydulmnl7NHgUklQLN930KltvfQXvv/8NF188gZQcJqQSM7o/nFfp/GnwuOxiaYJqHM6WvxoGMBs4CyAiRpK7FW3RLNOxdTE3LzUZXTY9LusQpEZh1vP/yjoE6Xsi4qfkKo5WjoiXKr3VEXgsm6gkqXoVFYkzz3yQ3//+EQAOPXR9Lr54Ly92qPRUbqZtFVLB1aonUjW2LGgUkiQ1AbYRVR3dANwF/BE4tdL8aSmlL7MJSZJ+aNq0ORx88K2MGfMmZWXBX/+6Cz//+RYmkFR6KvdBspl2UdQ3iSRJkqQlk1JK70XEsVXfiIilTSRJKhUHHHAL48a9zVJLtWHkyCHsuusqWYckVc8+SEW3yCRSRGy0qLeAlsUJR5KkxssrsqqjG8jdmW0CkMidYy2UgJWzCEqSqvr973fk009ncMMNg1ltta5ZhyPVzD5IRbO4SqTzFvPeG4UORJKkxq7MHJLqIKW0V/65T9axSFJlKSUee+xDttlmBQA22GBZnn76SC+WqHSM7v/93kdqMItMIqWUdmjIQCRJkpqjiNgaeCGlNCMiDgI2Av6eUvog49AkNUNz5sznmGPGccUVL3DNNftw8MHrA1bbqsQsLoHkULaisieSJEkFYiWS6ukiYP2IWB84GbgcuBbYPtOoJDU7n3wynX33HcXjj39I27YtaNWqPOuQpMWzeXaDM4kkSZKUrfkppRQRA4F/pJQuj4hDsw5KUvMyYcLH7LPPSCZNmkqvXp0YM2YYG220XNZhSSoxJpEkSSoQS/1VT9Mi4jTgYGDbiCjHm5hIakA33vgyhx8+ltmz57P11r255Zb96dGjQ9ZhSSpBNSaRIndGfCCwckrp7IhYAVg2pfR00aOTJKkRcTib6mko8CPg8JTSJ/lzrb9kHJOkZmL27PmcccZ4Zs+ezxFHbMiFF+5J69bWGqhE2EC75JTVYpl/A1sCB+SnpwEXFi0iSZKkZiSl9AlwPdA5IvYCZqeUrsk4LEnNRJs2LbjttqH86197cOmle5tAUmmxgXbJqc1PiM1TShtFxPMAKaWvIqJVkeOSJKnRcTSb6iMi9idXeTQeCOCfEfHLlNLNmQYmqcl6++0pjB79Oqecsg0Aa6/dnbXX7p5xVNJi2EC7ZNQmiTQvPzY/AUREN6CiqFFJkiQ1H78GNk0pfQbfnmvdB5hEklRw//3v/xg69Ga+/no2K620FEOHrpN1SJIakdokkS4AbgW6R8Q5wBDg9KJGJUlSI1RmKZLqp2xhAilvCrVrOSBJtZZS4m9/e5Jf/vJeKioS++yzBnvuuVrWYUnVsxdSyaoxiZRSuj4iJgA7kSux3iel9HrRI5MkqZHxr37V090RcQ9wY356KOCZs6SCmT17Pj/5yR1cc82LAJxxxnaceWY/yrwjhEpV5QSSvY9KSm3uzrYCMBO4vfK8lNIHxQxMkiSpOUgp/TIiBgPbkLtgd0lK6daMw5LURHzyyXT22WcETz31Ee3ateTqq/dhyJC1sg5Lqh17IZWc2gxnG0euH1IAbYA+wJvA2kWMS5KkRsfRbKqLiFgN+CuwCvAy8IuU0kfZRiWpqWnbtgVffz2bFVfszJgxw1h//WWzDklavNH9s45Ai1Gb4WzrVp6OiI2AnxQtIkmSVCcR0QZ4GGhN7nf7zSmlMyNiaWAksBLwHrB/Sumr/DqnAUcAC4DjU0r3ZBB6c3cFcA25Y7c38E9gcKYRSWoyKioSZWVB585tuPPOA+nYsRXdurXPOiypZguHsjmMrSTVphLpe1JKz0XEpsUIRpKkxizDxtpzgB1TStMjoiXwaETcRS4hcX9K6U8RcSpwKnBKRKwFDCNXVbw8cF9ErJ5SWpDVF9BMdUwpXZp//WZEPJdpNJKahPnzKzj11PuYOnUO//nPXkQEK6/cJeuwpJpVbaY9eFx2sWiRatMT6cRKk2XARsDnRYtIkiTVSUopAdPzky3zjwQMBPrl518NjAdOyc8fkVKaA0yMiHeAzYAnGi5qAW0iYkNyLQMA2laeTimZVJJUJ199NYsDDriFe+75Hy1alHHCCZuz9trdsw5Lqh2baTcKtalE6ljp9XxyPZJuKU44kiQ1Xln2RIqIcmACsCpwYUrpqYjokVKaDJBSmhwRC/+S6Ak8WWn1Sfl5aliTgfMrTX9SaToBOzZ4RJIarTfe+IIBA27k7be/ZJll2nHLLfubQFJpqlpxVJXNtEvaYpNI+RPSDimlXzZQPJIkNVrFulNyRAwHhleadUlK6ZLKy+SHom0QEUsBt0bEOovbZDXzPGNrYCmlHbKOQVLTcOedb3PAAbcwdeoc1l+/B2PGDGPFFZfKOiypeotLIFmBVPIWmUSKiBYppfn5RtqSJCkj+YTRJTUumFv264gYD+wOfBoRy+WrkJYDPssvNgnoXWm1XsDHBQxZktRAbrvtDQYPHklKsN9+a3HllQNp375V1mFJNbPiqFEqW8x7T+efX4iIsRFxcEQMXvhoiOAkSWpMyiKK8qhJRHTLVyAREW2BnYE3gLHAofnFDgXG5F+PBYZFROuI6AOsxne/9yVJjcjOO6/Meuv14He/24GRI4eYQFJpG90/6wi0hGrTE2lpYAq5cfmJXAl8AkYXMS5JklR7ywFX54ehlwGjUkp3RMQTwKiIOAL4ANgPIKX0akSMAl4j1+/wWO/MJkmNx0cfTaVr13a0adOCDh1a8dRTR9K6dZ1vvC01vIVD2Ry21mgt7idN9/yd2V7hu+TRQtadSZJURVaNtVNKLwEbVjN/CrDTItY5BzinyKGpFiIigAOBlVNKZ0fECsCyKSWrwyT9wOOPf8jgwSPZdddVuPrqfYgIE0gqfVWbaQ8el10sWiKL+2lTDnTA5puSJNVKsRprq8n7N1BBrur7bGAauTvhbpplUJJKzxVXPM/RR9/BvHkVfPTRNGbNmk+7di2zDkuqWeUEklVIjdrikkiTU0pnN1gkkiRJzdPmKaWNIuJ5gJTSVxFhUxNJ35o/v4KTTrqHCy7IFSj+7Gebcd55u9KyZXnGkUl1ZDPtRm9xSSSvp0qSVAfhr07Vz7x8P6sEuUbp5CqTJIkpU2YydOjN3H//RFq2LOOii/pzxBHeQFtSNhaXRKq2h4IkSZIK6gLgVnL9KM8BhgCnZxuSpFLxhz88wv33T6R79/aMHr0/W2+9QtYhSWrGFplESil92ZCBSJLU2NkTSfWRUro+IiaQu4AXwD4ppdczDktSifjd73bkq69mc9ZZ/ejdu3PW4Ug/bJJdg37Fi0QZKMs6AEmSmoqyKM5DTVv+bmwzgduBscCM/DxJzVBKif/851lmzJgLQLt2LbniioEmkFQ66pBA+h4bajcJ3gtSkiQpW+PI9UMKoA3QB3gTWDvLoCQ1vBkz5nLYYWO46abXGD/+fW68cd+sQ1JzVlPFUS2bZI8fP55+/foVJiZlziSSJEkFEmHZkOoupbRu5emI2Aj4SUbhSMrI++9/zcCBI3jxxU/p2LEVBx64bs0rScW0uASSVUXNlkkkSZKkEpJSei4iNs06DkkN5+GH32fIkFF8/vlMVl11acaOHcaaa3bLOiwpp5YVR2oeTCJJklQg9i9SfUTEiZUmy4CNgM8zCkdSA/vPf57luOPuYv78CnbddRVGjNiXLl3aZh2Wmos6NsmWbKwtSZKUrY6VHq3J9UgamGlEkhpESoknn/yI+fMrOPHELRg37kcmkNSwakogOWxNVViJJElSgdgSSXUVEeVAh5TSL7OORVLDiwguuqg/gwatwYABfbMOR82ZQ9ZUS1YiSZJUIGURRXmoaYqIFimlBeSGr0lqJl588RP697+BadPmANCmTQsTSJIaDZNIkiRJ2Xg6//xCRIyNiIMjYvDCR6aRSSqKm29+ja22uoI773ybP/zhkazDUXM3un/WEagRcjibJEkFYmNt1dPSwBRgRyABkX8enWVQkgqnoiJx1lnjOfvshwE4+OD1OPPMftkGJS3sh2TfI9WBSSRJkqRsdM/fme0VvkseLWRzCqmJmDZtDoceehu33voGZWXBn/+8MyeeuCXhcGWVisHjso5AjYhJJEmSCsS/B1RH5UAHvp88WsgkktQETJ06h623voJXXvmMzp1bM3LkEHbbbdWsw1JzN7p/zXdlkxbBJJIkSQVSVm0uQFqkySmls7MOQlLxdOrUmq237s28eQsYO/YAVl+9a9YhSd9PIDmUTXVkEkmSJCkbZh2lJiilxNdfz6ZLl7YAXHDBHsyaNY/OndtkHJmavLpWGJ1k0avqzruzSZJUIBHFeajJ2inrACQV1ty5Cxg+/Ha22OJyvv56NgCtWpWbQFLDqEsCyQok1ZOVSJIkSRlIKX2ZdQySCufTT6ez776jeOyxD2nTpgUTJnzMTjutnHVYai5G9//utRVGKiKTSJIkFUiZVUOS1Cw999xk9tlnBB9+OJWePTty223D2GST5bMOS83JwiokK4xUZA5nkySpQMoiivKQFiUido+INyPinYg4dTHLbRoRCyJiSEPGJzUHI0e+wjbbXMGHH05lyy178eyzw00gKTuDx2UdgZo4K5EkSZIaoYgoBy4EdgEmAc9ExNiU0mvVLHcucE/DRyk1bc89N5lhw24B4PDDN+Df/+5P69b+iaUGUtdG2lIB+BNOkqQCsWhIDWwz4J2U0rsAETECGAi8VmW5nwG3AJs2bHhS07fRRstx0klbssIKnfnZzzYj/EWghlQ1geRQNjUAk0iSJEmNU0/gw0rTk4DNKy8QET2BQcCOmESSCuLtt6cwd+6Cb6f/+tddM4xGwkbaalAmkSRJKhD7F6mBVfcfrupfEn8HTkkpLVhchUREDAeGA3Tr1o3x48cXKEQVwvTp0z0mJeLZZ7/krLNep2PHFvzlL309LiWmuX1W+uWfS/1rbm7HpakziSRJktQ4TQJ6V5ruBXxcZZlNgBH5BNIywJ4RMT+ldFvlhVJKlwCXAPTt2zf169evSCGrPsaPH4/HJFspJf7+9yc55ZRXqKhI7LTTKnTp0sHjUmKaxWelmj5Ipf41N4vj0ox4dzZJkgokojgPaRGeAVaLiD4R0QoYBoytvEBKqU9KaaWU0krAzcAxVRNIkhZvzpz5HH74WE488b9UVCROP31bRo8eSrt2Xo9XBuyDpIz5k0+SpALxyowaUkppfkQcR+6ua+XAFSmlVyPi6Pz7F2caoNQETJ48jcGDR/Hkk5No164lV101kP32WzvrsCT7ICkzJpEkSZIaqZTSncCdVeZVmzxKKf24IWKSmpJHH/2AJ5+cxAordGbMmGFssMGyWYek5qKaYWtSKTCJJElSgXhrZ0lqWvbbb20uvXQOAwb0pXv39lmHo+ZkcQkkh7ApQyaRJEmSJAlYsKCC3/zmQYYMWYuNNloOgCOP3CjjqNRsVFd95LA1lRjbN0iSVCBRpIckqfi+/no2e+11I3/846MMGTKKuXMXZB2SmhubZqsRsBJJkqQCKXM4myQ1Sm+++QUDBozgrbem0LVrW664YiCtWpVnHZaaK6uPVMJMIkmSJElqtu66622GDbuFqVPnsO663RkzZhh9+nTJOiw1FTbIVhPjcDZJkgrE4WyS1LhccMFT9O9/A1OnzmHw4DV5/PEjTCCpsOqaQHIIm0qclUiSJEmSmqVevToB8Nvfbs9vfrM9ZWWm7rUEFld15BA1NREmkSRJKhBbIklS6Zs7d8G3/Y4GD16T1147ljXWWCbjqNQkLCqBZHWRmhCTSJIkFUiYRZKkkvbEEx9ywAG3MGLEELbYoheACSQtmeqqj6w6UhNmTyRJkiRJTd6VVz5Pv35X8/7733DBBU9lHY6aiqoJJKuO1MRZiSRJUoF4ZUaSSs/8+RX84hf/5R//yCWOjjtuU84/f7eMo1KTY/WRmgmTSJIkSZKapC+/nMXQoTdz333v0rJlGRdeuCdHHbVx1mGpMVtc82ypGTCJJElSgdgTSZJKR0VFYpddruW55ybTrVs7Ro8eyjbbrJB1WGrsqksgOYRNzYhJJEmSJElNTllZcPbZ/TjzzPGMHj2UFVbonHVIakxqqjhy+JqaKds3SJJUIFGkhySpdlJKPPfc5G+n+/dfnaeeOtIEkupucQkkK4/UjFmJJElSgTicTZKyM2PGXA4/fCyjR7/O/fcfwnbbrQhAebnXzVVL1VUfWXEkfY9JJEmSJEmN2gcffMPAgSN44YVP6NixFdOnz806JDVGVRNIVhxJP2ASSZKkAvFatyQ1vEceeZ999x3F55/PZJVVujB27AGstVa3rMNSY2b1kbRIJpEkSZIkNUqXXDKB4467k3nzKth555UZOXIISy/dNuuwVIpqapQtqVZMIkmSVCD2RJKkhvP55zM49dT7mDevgp//fHP+8pddadHCmlAtQm0TSA5hkxbLJJIkSQViCkmSGk63bu0ZOXIIH300jR//eIOsw1GpqKniyKFq0hIxiSRJkiSpUXj55U+ZMGHyt0mjXXZZJduAVHoWl0CyykhaYiaRJEkqEEezSVLx3Hrr6xx88K3Mnj2f1VZbmq23XiHrkFTKrDiSisJBw5IkSZJKVkVF4qyzxjN48ChmzJjHAQesy0YbLZd1WJLULFmJJElSgZTZFUmSCmr69Ln8+Me3ccstr1NWFpx77s6cdNKW3shAkjJiEkmSpALxbxpJKpz33vuagQNH8NJLn9K5c2tGjBjC7ruvmnVYKmHrvn1q1iFITZ5JJEmSJEkl6eOPp9G3b1fGjBlG377LZB2OSlzXqU/lXthAWyoak0iSJBVIOJxNkpZISrlmyBHBSistxX//exB9+nRhqaXaZByZSt7o/t+9HjwuuzikJs7G2pIkSZIyN3fuAo4++g7+8pfHv5234YbLmUBS7Uy8M/dsFZJUVFYiSZJUIPZEkqT6+eyzGey77ygeffQD2rVryaGHrk+PHh2yDkuNkVVIUlGZRJIkqUC8O5sk1d3zz09mn31G8sEH39CzZ0duu22YCSRJKlEmkSRJkiRlYtSoV/nxj29j1qz5bLFFL0aP3p/lluuYdViSpEUwiSRJUoE4nE2Sau+SSybwk5/cAcBhh23ARRf1p3Vr/zxRHYzu/10vJEkNwp/SkiRJkhrcnnuuRq9enfjFL7bk+OM3J8zEq66qJJCmdNqcrhmFIjUXJpEkSSoQ//6RpMX76KOpLLdcR8rKgl69OvHGG8fSvn2rrMNSqahvZdFJCYCXx4+nX2EjklRFWdYBSJIkSWr67rvvXdZd9yJ+//uHv51nAknfU58EUp89Cx+HpEWyEkmSpAIJ784mST+QUuKCC57ixBP/S0VFYsKEyVRUJMrK/JmpRchXFkkqPSaRJEkqEP8ekqTvmzNnPj/96TiuvPIFAH71q2343e92NIFUymxWLWkxTCJJkiRJKrhPPpnO4MEjeeKJSbRt24IrrxzI0KHrZB2WapJ1AsnhaVJJM4kkSVKBOJxNkr7zs5/dxRNPTKJ3707cdtswNtpouaxDUk1G9//utUPKJFXDxtqSJDVyEdE7Ih6MiNcj4tWIOCE/f+mIuDci3s4/d6m0zmkR8U5EvBkRu2UXvaSm6p//3IP991+bZ545ygRSY7GwCslqIEmLYCWSJEkFEtkVIs0HTkopPRcRHYEJEXEv8GPg/pTSnyLiVOBU4JSIWAsYBqwNLA/cFxGrp5QWZBS/pCZgwYIKrrnmRQ45ZH3Ky8tYdtkOjBw5JOuwmp9C9DQaPK4wsUhqcqxEkiSpQKJI/2qSUpqcUnou/3oa8DrQExgIXJ1f7Gpgn/zrgcCIlNKclNJE4B1gs8J+NyQ1J19/PZu9976Rww8fyxlnPJh1OM3bkiaQrEKStBhWIkmS1IRExErAhsBTQI+U0mTIJZoiont+sZ7Ak5VWm5SfJ0l19tZbUxgw4EbefHMKXbu2ZeedV846JIE9jSQVhUkkSZIKpFh3rI6I4cDwSrMuSSldUs1yHYBbgJ+nlKbGosfXVfeGf21IqrO7736HYcNu5ptv5rDuut0ZM2YYffp0qXlFLblCDFuTpDoyiSRJUonLJ4x+kDSqLCJakksgXZ9SGp2f/WlELJevQloO+Cw/fxLQu9LqvYCPCxy2pCYspcR55z3BKafcR0VFYtCgNbjmmkF06NAq69Caj8UlkBySJqlITCJJklQgtelfVJT95kqOLgdeTymdX+mtscChwJ/yz2Mqzb8hIs4n11h7NeDphotYUmO3YEFi3Li3qahInHnm9pxxxvaUFascszmpT3WRw9YkNSCTSFqkaVOn8rvfns4777xNRHDm2eew3vobMuKGaxl14/WUt2jBNttuzwkn/jLrUKUG98a4s5g2Yw4LKiqYv6CCbQ78M+uu3pN//noY7du25v2Pp3DYr69m2ozZDNtjE35+6M7frrvuasuz5QHn8tJbH2X4FagYMrw729bAwcDLEfFCft6vyCWPRkXEEcAHwH4AKaVXI2IU8Bq5O7sd653ZJNVFixZl3HTTfjz++IcMGNA363CajromkKw4ktTATCJpkf5y7jlsufW2/Pn8C5g3by6zZ83mmaef5KEHH2DELWNp1aoVX06ZknWYUmZ2H/4Ppnw949vpi874Eaf+7VYenfAOhwzcgv87dCfO/vc4Rtz1LCPuehaAtVddnpv+NtwEkgoqpfQo1fc5AthpEeucA5xTtKAkNTlPPTWJCy98hiuuGEiLFmUss0w7E0jFYnWRpBJVtCRSRGycUppQZd7eKaXbi7VPFc706dN5fsKznPX7PwHQsmUrWrZsxc2jRvDjI46iVavcePelu3bNMkyppKy2YncenfAOAA88+QZj/30sZ/973PeW2X/3jRl194TqVlcT4EAOSU3V1Ve/wPDhdzB37gI237wnxx67WdYhNT42wpbUBJQVcduXRsS6Cyci4gDg9CLuTwX00aQP6bL00vz2N6fxo/0HcfaZpzNr5kw+eP89np/wLIf8aH+OOuwgXn3l5axDlTKRUuL2fx/HY9efzOGDtwbgtf9NZq9+uR97g3fZiF49fnh3miG7bsSou59t0FglSaqv+fMrOPHEe/jxj8cwd+4CjjlmE4YP3zjrsBqn2iaQHKImqYQVczjbEODmiDgQ2AY4BNi1iPtTAS1YMJ83Xn+NX556Ouuutz5/+dM5XHnFpSyYv4Cp06Zy9fUjefWVlzn1Fz9n7F33sZjbSEtN0o6H/Y3Jn39Dty4duOPi43jzvU/4yW+v57yTh3DaUXsw7qGXmTvv+y1mNl1nRWbOnsdr/5ucUdQqtjJ/FkpqQr76ahZDh97Mvfe+S4sWZVx44Z7NI4FUy4qhfgD1KS52qJqkRqxolUgppXeBYeRuNzwE2DWl9M3i1omI4RHxbEQ8e8Vli72TsYqse49l6d6jB+uutz4AO++yG2+8/hrde/Rgx512ISJYZ931iLIyvv7qq4yjlRre5M9zP84+/2o6Yx94iU3XXom33vuUvY+5kK0P/DOj7p7AxEmff2+d/Xbb2CokSVKj8NFHU9lss8u499536datHQ88cEjzSCBBcYecWWUkqZEreCVSRLwMVE6vLw2UA09FBCml9Ra1bkrpEuASgOlzkin6DC2zTDd69FiO9ya+y0p9Vubpp55g5ZVXoVfvFXjm6afYZNPNef+9icyfN4+luvxwyI7UlLVr04qysmD6zDm0a9OKnbdcgz9cchfdunTg86+mExGcetRuXHrzo9+uExEM3mVDdj7i79kFrqKzDklSU9GjRwdWXrkLHTq04rbbhrLiiktlHVJh1abaqIaKofHjx9OvX7/CxSRJjUAxhrPtVYRtKgMnn3Y6p5/2S+bNm0fPXr357e/+QNu2bTnrjF+z/6C9adGyJb/9/Z8cyqZmp3vXjow8/ygAWpSXM/KuZ7n38dc59oB+/GTodgCMeeAFrhnz5LfrbLPRqnz06de895F3NGzS/HEoqRFLKTFjxjw6dGhFixZljBw5hJYty2jfvlXWoRVeTQkkK4YkqVoFTyKllN4HiIgtgFdTStPy0x2BtYD3C71PFUffNdbkuhG3/GD+7//4lwyikUrHex9NYfOhf/rB/AtvHM+FN46vdp1HJrzN9oeeV+TIJEmqn5kz53HEEWP5+ONp3HvvwbRqVc5SS7XJOqzisz+RJNVJMe/OdhEwvdL0jPw8SZKapCjSP0kqpg8//IZtt72SESNe4bnnJvPqq59lHZIkqUQV8+5skdJ3fY1SShURUcz9SZIkSaqDxx77gMGDR/HZZzNYZZUujBkzjLXX7p51WJKkElXMSqR3I+L4iGiZf5wAvFvE/UmSlKmI4jwkqRguu+w5dtjhaj77bAY777wyTz99lAkkSdJiFTOJdDSwFfARMAnYHBhexP1JkpSpKNJDkgrtjjve4qijbmfevApOOGFz7rrrQJZeum3WYUmSSlzRhpellD4DhhVr+5IkSZLqZ889V2PIkLXYc89VOeywDbMOp3hG96/5TmySpForWhIpItoARwBrA9/e2iGldHix9ilJUqYsG5JUwl555TO6dm3Lcst1pKwsGDVqCNHUx8wuLoHUZ8+Gi0OSmohiNrq+FngD2A04GzgQeL2I+5MkSZJUjTFj3uCgg25lnXW6M378obRu3aLpJ5AqOynVvIwkqUbFTCKtmlLaLyIGppSujogbgHuKuD9JkjIVliJJKjEpJX7/+4c544zxAKy8chcqKppoQsWha5JUdMVMIs3LP38dEesAnwArFXF/kiRlqjld1JdU+mbMmMuPfzyGm29+jQj405925pe/3KrpViAtKoHksDVJKphiJpEuiYguwOnAWKAD8Jsi7k+SJEkS8N57XzNw4AheeulTOnVqzY037suee66WdViFUVPFkUPXJKloiplEuj+l9BXwMLAyQET0KeL+JEnKVBO9ti+pEbrppld56aVPWX31rowZM4w11lgm65AKx2bZkpSZYiaRbgE2qjLvZmDjIu5TkiRJavZ+8YutSAmGD9+YpZZqU/MKpaIufY2sOJKkBlfwJFJErAGsDXSOiMGV3uoENKLfYJIk1ZGlSJIyMnfuAn7zmwc47rjN6N27MxHBySdvnXVYdVfbBJIVR5KUiWJUIvUF9gKWAvauNH8acFQR9idJkiQ1W59/PoMhQ27i4Yff57HHPuSRRw5r/M2zrTKSpJJU8CRSSmkMMCYitkspPVz5vYhohJdDJEmqnbAUSVIDe/HFTxg4cATvv/8Nyy/fkfPP361hE0h1GX4mSWr0yoq47b9XM++fRdyfJEmZiijOQ5Kqc/PNr7HVVlfw/vvfsPnmPXnmmaPYbLOeDRtEMRJIDlWTpJJVjJ5IWwJbAd0i4sRKb3UCygu9P0mSJKm5Oeus8fz2tw8BcOih63PxxXvRpk0BTu3rW1nk8DNJahaK0ROpFdAhv+2OleZPBYYUYX+SJJUEi4YkNZR27VpSVhb89a+78POfb1G4IWz1SSBZOSRJzUYxeiI9BDwUEVellN4v9PYlSZKUExG7A/8gV+19WUrpT1XePxA4JT85HfhpSunFho1ShbJgQQXl5bluFL/4xVbsuusqrL/+ssXZmZVFkqRqFLMn0syI+EtE3BkRDyx8FHF/kiRlK4r0kKoREeXAhcAewFrAARGxVpXFJgLbp5TWA34HXNKwUapQHnhgImut9W8mTvwKgIgobAJpdH84zx84kqTFK2YS6XrgDaAPcBbwHvBMEfcnSVKmokj/pEXYDHgnpfRuSmkuMAIYWHmBlNLjKaWv8pNPAr0aOEYtoZQSo0d/xK67Xstbb03hn/98ujg7qjyMzeFpkqRFKEZPpIW6ppQuj4gTKg1xe6iI+5MkSWpOegIfVpqeBGy+mOWPAO4qakQqqDlz5nPssXdy+eXvAHDaadvwu9/tUPgdje7/3WuHsUmSFqOYSaR5+efJEdEf+BivfkmSmrBC9bWVaqm6/3HVZgAiYgdySaRtFvH+cGA4QLdu3Rg/fnyBQlR9ffnlXM4441VefXUqrVoFJ5+8BjvtVM4jjzxc8H31y1chTem0OS977Gtt+vTpflZKjMekNHlcmpZiJpF+HxGdgZOAfwKdgP8r4v4kSZKak0lA70rTvchdtPueiFgPuAzYI6U0pboNpZQuId8vqW/fvqlfv34FD1a1N2vWPNZa69+8995UevXqxOmnr8pPfrJ3cXZWqQqp61FP0q84e2mSxo8fj5+V0uIxKU0el6alaEmklNId+ZffAEWou5UkqbRYiKQG9gywWkT0AT4ChgE/qrxARKwAjAYOTim91fAhqj7atm3JiSduwYgRrzJ69P68/vqzxdvZwl5I9kGSJNVCMRtrS5LUvHh3NjWglNJ84DjgHuB1YFRK6dWIODoijs4vdgbQFfh3RLwQEUXMRmhJLFhQwRtvfPHt9HHHbcb48YfSo0eHhglg8LiG2Y8kqVEr5nA2SZIkFVFK6U7gzirzLq70+kjgyIaOS3XzzTezOfDA0Tz22Ic8/fSRrLZaVyKCli3Li7vjyg21JUmqhaJVIuVLq2ucJ0lSUxFF+iep6Xr77SlsscXljBv3NmVlwSefTG+4nTuUTZJUR8WsRLoF2KjKvJuBjYu4T0mSJKlRuOeedxg27Ba+/no2a6/djbFjD2DllbvUvOLo/t8lgArBoWySpFoqeBIpItYA1gY6R8TgSm91AtoUen+SJJWKsGhIUi2klPjb357kl7+8l4qKxD77rME11+xDx46ta7eBQiaQrEKSJNVBMSqR+gJ7AUsBle9FOg04qgj7kyRJkhqNt96awqmn3kdFReKMM7bjzDP7UVZWiyx01Qqkk1LxgpQkqRoFTyKllMYAYyJiy5TSE4XeviRJpcpCJEm10bfvMvznP3vRsWNrhgxZq/YrVk4gWUEkScpAMXsifRgRtwJbAwl4FDghpTSpiPuUJCk7ZpEkLcLTT3/ElCkz2WOP1QA47LAN678xK5AkSRkpZhLpSuAGYL/89EH5ebsUcZ+SJElSSbn22hc56qjbadWqnAkThrPaal0XvXA1TbP7AUwoZoSSJNVOWRG33T2ldGVKaX7+cRXQrYj7kyQpU1Gkf5IapwULKvjFL/7LIYfcxpw5CzjooPVYaaWlFr9STU2zHcYmScpQMSuRPo+Ig4Ab89MHAFOKuD9JkiSpJHz11SwOOOAW7rnnf7RoUcY//7kHRx+9Se03UGnI2vjx4+nXr1/hg5QkqY6KmUQ6HPgX8DdyPZEez8+TJKlJCouGJAFvvPEFAwbcyNtvf8kyy7Tjllv2Z7vtVsw6LEmSlljRkkgppQ+AAcXaviRJpcYckiSAb76ZzQcffMP66/fgttuG1TyETZKkRqLgSaSIOGMxb6eU0u8KvU9JkiSpVGy+eS/uuutANtusJ+3bt8o6HEmSCqYYjbVnVPMAOAI4pQj7kySpNESRHpJK2qxZ8zjooNHcdNOr387bYYc+JpAkSU1OwSuRUkrnLXwdER2BE4DDgBHAeYtaT5IkSWpsJk2ayj77jGDChMncd9+79O+/Ou3ataz7hkb3r/nObJIkZawoPZEiYmngROBA4Gpgo5TSV8XYlyRJpSIsG5Kalccf/5DBg0fy6aczWHnlLowZM6x+CST4fgKpz56FCVCSpAIrRk+kvwCDgUuAdVNK0wu9D0mSSpF3Z5OajyuueJ6f/nQcc+cuYMcd+zBq1BC6dm1Xt41UV310UipckJIkFVgxeiKdBCwPnA58HBFT849pETG1CPuTJEmSGsw55zzMEUeMZe7cBRx//Gbcc89BdU8gwQ8TSFYgSZJKXDF6IhUjMSVJUsmzEElqHvbcczX++tcn+Otfd+GIIzZa8g1afSRJaiSK0hNJkiRJako++2wG3bu3B2DDDZfjvfdOoHPnNhlHJUlSw7JqSJKkQokiPSRlasyYN1h11Qu47rqXvp1nAkmS1BxZiSRJkiRVI6XEH/7wCKef/iAA998/kYMOWq/2G6iucbYkSY2YSSRJkgokLBuSmowZM+Zy2GFjuOmm14iAP/xhJ045Zeu6baQ2CSSbaUuSGhGTSJIkFUiYQ5KahPff/5p99hnJCy98QseOrbjhhn3Za6/Va7+BqhVINs6WJDURJpEkSZKkvJQSw4bdwgsvfMKqqy7N2LHDWHPNbnXbSOUEkpVGkqQmxCSSJEkFYiGS1PhFBJdeujdnnjmeyy7bmy5d2tZ/Y1YgSZKaGO/OJkmSpGZt3rwF3Hzza99Or7NOd265Zf+6J5BG94fzTCdLkpouk0iSJBVKFOkhqWg+/3wGu+xyLfvtdxOXX/7ckm3MYWySpCbO4WySJBWId2eTGpeXXvqUAQNu5P33v2HZZTuw9trd67+x0f2/e+0wNklSE2USSZIkSc3OLbe8xiGH3MbMmfPYdNPlufXWofTs2an+G1xYhWQFkiSpCTOJJElSgYSFSFLJq6hInHXWeM4++2EADjpoPS65ZC/atm1Ztw2N7v/94WsLDR5XgCglSSpN9kSSJElSszFjxlxGjnyVsrLgL3/ZhWuu2afuCSSoPoFkFZIkqYmzEkmSpAKxEEkqfR07tmbMmGFMnPg1u+++6pJv0P5HkqRmxCSSJEkF4nA2qTQ9+OBE7rrrHc49d2cigr59l6Fv32Vqv4FFDV2TJKmZMYkkSZKkJimlxIUXPsPPf343CxYktt66NwMHrlH3DS0qgeTwNUlSM2MSSZKkgrEUSSoVc+cu4Nhjx3HZZc8DcPLJW7HXXqsv2UYduiZJauZMIkmSJKlJ+fTT6ey77ygee+xD2rRpwWWX7c2BB66XdViSJDV6JpEkSSoQeyJJ2XvzzS/YZZdr+fDDqfTs2ZHbbhvGJpssn3VYkiQ1CWVZByBJkpZMRFwREZ9FxCuV5i0dEfdGxNv55y6V3jstIt6JiDcjYrdsopaKY/nlO9KpU2u23LIXzz47fMkTSKP7FyYwSZKaAJNIkiQVSBTpUQtXAbtXmXcqcH9KaTXg/vw0EbEWMAxYO7/OvyOivM5frFRCKioSc+bMB6Bjx9b8978H8+CDh7Lssh2WfOMLm2rbRFuSJIezSZJUKFkNZ0spPRwRK1WZPRDol399NTAeOCU/f0RKaQ4wMSLeATYDnmiQYKUCmzp1DgceOJouXdpw9dX7EBEsv3zH7xYY3X/Rd1eri8HjlnwbkiQ1ciaRJElqmnqklCYDpJQmR0T3/PyewJOVlpuUnyc1Ou+88yUDBtzI669/QZcubXj//W9YaaWlvr9QIRJIViFJkgSYRJIkqWCitoPP6rrdiOHA8EqzLkkpXVLfzVUzz/uWq9G5997/MXTozXz11WzWWqsbY4+9j5VuOXXRK5zkf3NJkpaUPZEkSSpxKaVLUkqbVHrUJoH0aUQsB5B//iw/fxLQu9JyvYCPCxuxVDwpJf7+9yfZfffr+eqr2QwY0JcnnjiCVWbduuiVrCSSJKkgrESSJKlQMuqJtAhjgUOBP+Wfx1Saf0NEnA8sD6wGPJ1JhFI9XHrpc/zf/90DwOmnb8tZZ+1AWVmlD58VR5IkFY1JJEmSCiSrHFJE3EiuifYyETEJOJNc8mhURBwBfADsB5BSejUiRgGvAfOBY1NKCzIJXKqHgw5aj2uvfYnjj9+M/cpPhr/tlHVIkiQ1GyaRJElq5FJKByzirWr/uk4pnQOcU7yIpMJ64YVPWG21pWnfvhXt2rXk4Yd/TETAeVWaZjtsTZKkojKJJElSgURpDWeTmoTrr3+JI44Yy4ABfRk5cggRQdy61/fvuuYQNkmSGoSNtSVJklRyFiyo4OST7+Wgg25lzpwFdOnShgUL8smiygkkq48kSWowViJJklQgUWKdtaXG6uuvZ/OjH93CXXe9Q4sWZVxwwe789Kebwuj+ViBJkpQhk0iSJBWKOSRpib355hcMGDCCt96aQteubbn55v3p12+l3JtWIEmSlCmTSJIkSSoZF1zwFG+9NYX11uvBmDHDWGmlpX64kBVIkiRlwiSSJEkFYiGStIRG9+e83v9l6Z235ZQdHqPDLT/NOiJJklSJjbUlSZKUmVmz5vHrX9/PtGlzYOKdtGk5n9/t/iAdWs+tfgWHsUmSlBkrkSRJKpCwFEmqk48+mso++4zk2Wc/ZuJdF3HDgfk3HK4mSVJJMokkSZKkBvfkk5MYNGgkn3wynZW6fMVpOz6Se8NKI0mSSpZJJEmSCiTsiiTVylXHHcpPLl6BuQta0G+Vidx0yE0s036mFUiSpG/NmzePSZMmMXv27KxDabTatGlDr169aNmyZcG2aRJJkqQCcTibtHgVFYmTTrqHv1+4MgDHbvU0fxt4Ny3LK6xAkiR9z6RJk+jYsSMrrbQS4UlWnaWUmDJlCpMmTaJPnz4F265JJEmSJDWICJgxYx4tyxdw4aBxHHXThKxDkiSVqNmzZ5tAWgIRQdeuXfn8888Lul2TSJIkSVoyo/vDxDsX+XZFRVBWlgjgX6uU85OfdWfjXpMbLj5JUqNkAmnJFOP7V1bwLUqSJKl5WUwC6fZXV2fzC47k61ltAGjVYkEugeTwNUmSGh0rkSRJKhAvlqnZq9QYO6XEH//4KKdf9QApwaWt7+OXJ22dYXCSJNVPSomUEmVlDV+HM3/+fFq0KJ3UjZVIkiQVSBTpn9TYzJw5jwMOuIVf//oBAM45Z0d+8YutMo5KkqTae++991hzzTU55phj2Gijjfjwww/55S9/yTrrrMO6667LyJEjv132z3/+M+uuuy7rr78+p5566g+29emnnzJo0CDWX3991l9/fR5//HHee+891llnnW+X+etf/8pvf/tbAPr168evfvUrtt9+e8455xxWWmklKioqAJg5cya9e/dm3rx5/O9//2P33Xdn4403Ztttt+WNN94o7jcFK5EkSZJUQB988A377DOC55//hA4dWnHDDYPZe+++WYclSWrMzivSRbVKFbTVefPNN7nyyiv597//zS233MILL7zAiy++yBdffMGmm27KdtttxwsvvMBtt93GU089Rbt27fjyyy9/sJ3jjz+e7bffnltvvZUFCxYwffp0vvrqq8Xu++uvv+ahhx4C4LnnnuOhhx5ihx124Pbbb2e33XajZcuWDB8+nIsvvpjVVluNp556imOOOYYHHnig/t+PWjCJJElSgTicTc3dZ5/NYNNNL+Wzz2awyipdGDNmGGuv3T3rsCRJqpcVV1yRLbbYAoBHH32UAw44gPLycnr06MH222/PM888w0MPPcRhhx1Gu3btAFh66aV/sJ0HHniAa665BoDy8nI6d+5cYxJp6NCh33s9cuRIdthhB0aMGMExxxzD9OnTefzxx9lvv/2+XW7OnDlL/DXXxCSSJEmSCqJ79/YMHbo2r7/+BSNHDmHppdtmHZIkqSmooWKoWNq3b//t65SqjyGlVK+7oLVo0eLbIWoAs2fPXuS+BwwYwGmnncaXX37JhAkT2HHHHZkxYwZLLbUUL7zwQp33vSTsiSRJUoFEkR5SKZs3bwEffNX52+nzz9+Nu+460ASSJKlJ2W677Rg5ciQLFizg888/5+GHH2azzTZj11135YorrmDmzJkA1Q5n22mnnbjooosAWLBgAVOnTqVHjx589tlnTJkyhTlz5nDHHXcsct8dOnRgs80244QTTmCvvfaivLycTp060adPH2666SYgl8x68cUXi/CVf59JJEmSCsUskpqZL76YyW67XUe/i37MFzNyZfwtWpTRooWnmJKkpmXQoEGst956rL/++uy44478+c9/Ztlll2X33XdnwIABbLLJJmywwQb89a9//cG6//jHP3jwwQdZd9112XjjjXn11Vdp2bIlZ5xxBptvvjl77bUXa6yxxmL3P3ToUK677rrvDXO7/vrrufzyy1l//fVZe+21GTNmTMG/7qpiUSVZWZs+p0QDk0pMty1+lnUIUqMw6/l/FT0dM21ORVF+d3VsXWYqSQ2mb9++6c0336xxuZdf/pQBA0bw3ntf06PjdO4+8jo2OH9yA0TY/IwfP55+/fplHYaq8LiUHo9JaarvcXn99ddZc801Cx9QM1Pd9zEiJqSUNqnP9uyJJElSgYRlQ2omRo9+nUMOuZUZM+axySbLc+vuv6DXUlOzDkuSJBWZtcaSJEmqlYqKxFlnjWfffUcxY8Y8DjxwXR5++McmkCRJaiasRJIkqUDqcWMOqVF5+OH3+e1vHyICzj13Z37xi62IW/fKOixJktRATCJJkiSpVvr1W4mzzurHppsuzx57rJabOfHO3HOfPTOLS5LUNKWUCK/S1VsxemCbRJIkqUA8xVFTNH78e3Tq1JqN3jsSJt7JGe2B1/KPygaPyyA6SVJT1aZNG6ZMmULXrl1NJNVDSokpU6bQpk2bgm7XJJIkSYXi+Y2akJQSF130LCeccDfLLtuB548azzLtF7GwVUiSpALr1asXkyZN4vPPP886lEarTZs29OrVq6DbNIkkSZLUSEXE7sA/gHLgspTSn6q8H/n39wRmAj9OKT1X03bnzl3Az352J5dcklv0gFXvokvbWbk3Typ8abwkSVW1bNmSPn36ZB2GqjCJJElSgYSlSGpAEVEOXAjsAkwCnomIsSmlygPN9gBWyz82By7KPy/SggWJnXe+hkce+YDWLeZz2X5jOWjjl3JvWnEkSVKzZhJJkiSpcdoMeCel9C5ARIwABvL9bkUDgWtSrrPmkxGxVEQsl1KavKiNvv/+TP73vw9YfvmO3DbkPDZd4WOrjyRJEmASSZKkgrHnoxpYT+DDStOT+GGVUXXL9AQWmUSaP7+CLVb8kNGHjmS5TtMLFaskSWoCSjaJ1KG1p+KlKCKGp5QuyToOfWfW8//KOgRVw89K89SmhePZ1KCq+/9WtWSoNssQEcOB4fnJOU++f/kry59daYFf+F87Y8sAX2QdhH7A41J6PCalyeNSevrWd8WSTSKpZA0H/MNYqpmfFUnFNgnoXWm6F/BxPZYhn/S+BCAink0pbVLYULUkPCalyeNSejwmpcnjUnoi4tn6rltWyEAkSZLUYJ4BVouIPhHRChgGjK2yzFjgkMjZAvhmcf2QJEmSFsdKJEmSpEYopTQ/Io4D7gHKgStSSq9GxNH59y8G7gT2BN4BZgKHZRWvJElq/Ewiqa4cniPVjp8VSUWXUrqTXKKo8ryLK71OwLF13Kw/v0qPx6Q0eVxKj8ekNHlcSk+9j0nkzi0kSZIkSZKkRbMnkiRJkiRJkmpkEqmZiohBEZEiYo389AYRsWel9/tFxFZLsP3phYhTKob8//3zKk3/IiJ+W8M6+0TEWnXcz/c+R/XZRqV1V4qIV+qzriRVJyJ2j4g3I+KdiDi1mvcjIi7Iv/9SRGyURZzNSS2OyYH5Y/FSRDweEetnEWdzUtMxqbTcphGxICKGNGR8zVVtjkv+POyFiHg1Ih5q6Bibm1r8/OocEbdHxIv5Y2KPviKLiCsi4rNF/Q1R39/zJpGarwOAR8ndyQVgA3KNNxfqB9Q7iSSVuDnA4IhYpg7r7APUNQHUj+9/juqzDUkquIgoBy4E9iD3c+mAapLcewCr5R/DgYsaNMhmppbHZCKwfUppPeB32GekqGp5TBYudy65Jvcqstocl4hYCvg3MCCltDawX0PH2ZzU8rNyLPBaSml9cufI5+XvLKriuQrYfTHv1+v3vEmkZigiOgBbA0cAw/If3rOBofls/SnA0cD/5ae3jYi9I+KpiHg+Iu6LiB4LtxURV0bEy/ns5b5V9rVMRDwREf0b+MuUFmc+uRPv/6v6RkSsGBH35/8/3x8RK+SriQYAf8l/Jlapss4PPh8RsRLf/xxtX3UbEXFURDyTvyJzS0S0y2+vR0Tcmp//YlSpCoyIlfP72rQo3x1JzcFmwDsppXdTSnOBEcDAKssMBK5JOU8CS0XEcg0daDNS4zFJKT2eUvoqP/kk0KuBY2xuavM5AfgZcAvwWUMG14zV5rj8CBidUvoAIKXksSmu2hyTBHSMiAA6AF+SOydXkaSUHib3fV6Uev2eN4nUPO0D3J1Seovcf6p1gDOAkSmlDVJK5wIXA3/LTz9Crmppi5TShuR+KJyc39ZvgG9SSuvmr4o9sHAn+UTTOOCMlNK4BvrapNq6EDgwIjpXmf8vcj9M1wOuBy5IKT0OjAV+mf9M/K/KOj/4fKSU3uP7n6OHqtnG6JTSpvkrMq+TS+wCXAA8lJ+/EfDqwh1FRF9yJ6qHpZSeKdD3QlLz0xP4sNL0pPy8ui6jwqnr9/sI4K6iRqQaj0lE9AQGkfudr4ZRm8/K6kCXiBgfERMi4pAGi655qs0x+RewJvAx8DJwQkqpomHC0yLU6/d8i6KFo1J2APD3/OsR+elXF7l0Ti9gZD4z2YpcOTXAznw3JI5KV8daAvcDx+b/eJZKSkppakRcAxwPzKr01pbA4Pzra4E/12Jzi/p81GSdiPg9sBS5KzILy+B3BA7Jx7kA+CYiugDdgDHAvimlmj6zkrQ4Uc28qrfsrc0yKpxaf78jYgdySaRtihqRanNM/g6cklJakCuwUAOozXFpAWwM7AS0BZ6IiCfzF9FVeLU5JrsBL5A7z10FuDciHkkpTS1ybFq0ev2etxKpmYmIruQ+uJdFxHvAL4GhVP8fqLJ/Av9KKa0L/ARos3CTVP8fbT4wgdwPC6lU/Z3cSXj7xSxTmz+YFvX5qMlVwHH59c6qxXrfkLtasHUtty9JizIJ6F1puhe5q8N1XUaFU6vvd0SsB1wGDEwpTWmg2Jqr2hyTTYAR+fPqIcC/I2KfBomu+artz6+7U0ozUkpfAA8DNqIvntock8PIVeGnlNI75C66rtFA8al69fo9bxKp+RlCbqjOiimllVJKvcl9gFcAOlZablqV6c7AR/nXh1aa/1/guIUT+WoJyP3hfTiwxqLumCBlLaX0JTCK74aRATzOd9V1B5IbqgY//ExUtqjPR9V1qk53BCZHRMv8vha6H/gp5BoVRkSn/Py55IajHhIRP1rc1yZJNXgGWC0i+uR7Iw4jN+S2srHkft5ERGxBbvj65IYOtBmp8ZhExArAaOBgKyoaRI3HJKXUJ39OvRJwM3BMSum2Bo+0eanNz68xwLYR0SLfc3Jzcq0DVBy1OSYfkKsMW9j2pC/wboNGqarq9XveJFLzcwBwa5V5twDLAmvlG/4OBW4HBi1srA38FrgpIh4Bvqi07u/JjTd+JSJeBHZY+EZ+GM4wYIeIOKZoX5G0ZM4DKt+l7XjgsIh4CTgYOCE/fwTwy3xD61WqbOO3VP/5qPo5qrqN3wBPAfcCb1Ra7wRyn5uXyVX0rb3wjZTSDGAvcg27q2vuKUk1SinNJ3cR6B5yf1iNSim9GhFHR8TR+cXuJHeC/w5wKeDv8iKq5TE5A+hKrtrlhYh4NqNwm4VaHhM1sNocl5TS68DdwEvA08BlKaVqb3OuJVfLz8rvgK3y57f3kxsG+kX1W1QhRMSNwBNA34iYFBFHFOL3fKTk0HZJkiRJkiQtnpVIkiRJkiRJqpFJJEmSJEmSJNXIJJIkSZIkSZJqZBJJkiRJkiRJNTKJJEmSJEmSpBqZRJJqEBEL8rfRfSUiboqIdkuwrasiYkj+9WURsdZilu0XEVvVYx/vRcQytZ2/iG38OCL+VYj9SpIkNReVzhsXPlZazLLTC7C/qyJiYn5fz0XElvXYxrfnpBHxqyrvPb6kMea3U/l8+vaIWKqG5TeIiD0LsW9JhWUSSarZrJTSBimldYC5wNGV34yI8vpsNKV0ZErptcUs0g+ocxJJkiRJmVl43rjw8V4D7POXKaUNgFOB/9R15SrnpL+q8l6hzkUrn09/CRxbw/IbACaRpBJkEkmqm0eAVfNVQg9GxA3AyxFRHhF/iYhnIuKliPgJQOT8KyJei4hxQPeFG4qI8RGxSf717vmrRy9GxP35q1ZHA/+Xv2qzbUR0i4hb8vt4JiK2zq/bNSL+GxHPR8R/gKjtFxMRm0XE4/l1H4+IvpXe7h0Rd0fEmxFxZqV1DoqIp/Nx/adqEi0i2kfEuPzX8kpEDK3rN1mSJKkpiIgO+XO75yLi5YgYWM0yy0XEw5UqdbbNz981Ip7Ir3tTRHSoYXcPA6vm1z0xv61XIuLn+XnVnqMtPCeNiD8BbfNxXJ9/b3r+eWTlyqB8BdS+izoHrsETQM/8dn5wLhoRrYCzgaH5WIbmY78iv5/nq/s+SmoYLbIOQGosIqIFsAdwd37WZsA6KaWJETEc+CaltGlEtAYei4j/AhsCfYF1gR7Aa8AVVbbbDbgU2C6/raVTSl9GxMXA9JTSX/PL3QD8LaX0aESsANwDrAmcCTyaUjo7IvoDw+vwZb2R3+/8iNgZ+AOwb+WvD5gJPJNPgs0AhgJbp5TmRcS/gQOBayptc3fg45RS/3zcnesQjyRJUmPWNiJeyL+eCOwHDEopTY3csP8nI2JsSilVWudHwD0ppXPyF+fa5Zc9Hdg5pTQjIk4BTiSXXFmUvcld3NwYOAzYnNzFxaci4iFgZRZzjpZSOjUijstXNVU1gtw54J35JM9OwE+BI6jmHDilNLG6APNf307A5flZPzgXTSntGxFnAJuklI7Lr/cH4IGU0uGRGwr3dETcl1KasZjvh6QiMIkk1azyycAj5H7pbQU8XekX5K7AepHvdwR0BlYDtgNuTCktAD6OiAeq2f4WwMMLt5VS+nIRcewMrBXxbaFRp4jomN/H4Py64yLiqzp8bZ2BqyNiNSABLSu9d29KaQpARIwGtgHmAxuTSyoBtAU+q7LNl4G/RsS5wB0ppUfqEI+k/2/nfkKtqqI4jn9/QX8lJKGiBlkTB0FgNoowszDLIpQaGEI4FbJBJBQ4iSAhG4TQpCQKKyMkI4PQCM2KokzNSeTIgiB0EFH0IsHVYO8bt+t97z5LB8n3Mzrnnr3P3m/wOIu1196SpP+zqeEkTJILgWeT3A6colXgXA38NNTnK+CV3vbdqjqcZAlwIy0pA3ARrYJnnM1JNgInaEmdu4CdgwRLj+MW0xZC/22M9gGwpSeK7qHFrlNJpouBR5NIg3j6euBr4MOh9tPFosPuBh5I8kS/vwS4Dvj2DP4GSWeBSSRpsqnRFZn+MR9e+Qiwvqp2j7RbQfsgziSzaANt++mtVTU1Zi6z6T/OM8DeqlqVtoVu39Cz0XdWn+trVfXUdC+sqqN9BWwFsKmvRs20aiZJknS+WgNcCdzSq7iP0RIgf6uq/T3JdB+wLclm4Gfagt7DsxhjQ1XtGNz0ip7T/JcYrar+SLIPWE6rSNo+GI4xMfAYU1W1sFc/vU87E2kLM8eiwwI8WFXfzWa+ks4dz0SSzo7dwLq+gkSSBUnm0Pamr+77xa8Blo7p+zmwJMkNve+8/vuvwOVD7fYAjw5ukizsl/tpAQpJ7gWuOIN5zwV+7NdrR54tSzIvyaXASuAz4CPgoSRXDeaaZP5wpyTXAr9X1evA88CiM5iPJEnS+WQucLwnkJYC80cb9FjqeFW9TKt4XwR8AdyWZHDG0WVJFsxyzP3Ayt5nDrAK+GSWMdrJQTw7xlu0bXKLabEvTB8Dj1VVvwCPAU/0PtPFoqNx8G5gffrqaZKbpxtD0rllJZJ0dmyllece7B+3E7TEy07gTtoWr6PAx6Mdq+pEP1PpnSQX0LaHLQN2ATv6wYHraR/cF5Mcof3v7qcdvv00sD3Jwf7+H2aY55Ekp/r128BztBLix4HRrXafAttoBzS+WVUHAHq59J4+15O0laTvh/rdRCurPtWfr5thPpIkSeezN4BdSQ4Ah2lnAI26A9iQ5CTwG/BIjw/X0mK8i3u7jbR4ckZVdTDJq8CX/aetVXUoyXImx2gv0eLFg1W1ZuTZHto5mO9V1Z+DdzM+Bp5pfoeSfAOsZvpYdC/wZN8Ct4lWsfRCn1uAY8D9M40j6dzIP890kyRJkiRJkk7ndjZJkiRJkiRNZBJJkiRJkiRJE5lEkiRJkiRJ0kQmkSRJkiRJkjSRSSRJkiRJkiRNZBJJkiRJkiRJE5lEkiRJkiRJ0kQmkSRJkiRJkjTRX6CVWW2ZrOlxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8568019093078759\n",
      "Precision is: 0.6470588235294118\n",
      "Recall is: 0.6914285714285714\n",
      "F1 score is: 0.6685082872928177\n"
     ]
    }
   ],
   "source": [
    "pred = evaluate(model,test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROBERTAClassifier(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (d1): Dropout(p=0.3, inplace=False)\n",
       "  (l1): Linear(in_features=768, out_features=64, bias=True)\n",
       "  (bn1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (d2): Dropout(p=0.3, inplace=False)\n",
       "  (l2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model structure for roberta\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Test_cleaned = pd.read_csv('source_folder/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample data set are:\n",
      "bert model 2: [CLS] all the home alones watching 8 mile [SEP] the last rap battle in 8 mile nevr gets old ahah Lable is: 0 Prediction: 0\n",
      "bert model 2: [CLS] ok good the end of 8 mile is on [SEP] the end of 8 mile makes me so happy Lable is: 1 Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"The sample data set are:\")\n",
    "print(\"bert model 2:\", (Test_cleaned['sentences'][0]), 'Lable is:',Test_cleaned['label'][0], \"Prediction:\", pred[0])\n",
    "print(\"bert model 2:\", (Test_cleaned['sentences'][20]), 'Lable is:',Test_cleaned['label'][20], \"Prediction:\", pred[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
