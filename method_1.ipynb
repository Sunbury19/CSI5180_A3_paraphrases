{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "jTO9dWyrhg9-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'source_folder'\n",
    "saved_folder = 'saved_folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "oIQaFBvfhbHm"
   },
   "outputs": [],
   "source": [
    "Test = pd.read_csv('test.data', header=None, sep = '\\t')\n",
    "Test_label = pd.read_csv('test.label', header=None, sep = '\\t')\n",
    "Dev = pd.read_csv('dev.data', header=None, sep = '\\t')\n",
    "Train = pd.read_csv('train.data', header=None, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "WSR9uxveiDOQ",
    "outputId": "ec466737-bf20-4ffc-f51d-48fcd31ef3a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1st QB</td>\n",
       "      <td>EJ Manuel the 1st QB to go in this draft</td>\n",
       "      <td>But my bro from the 757 EJ Manuel is the 1st Q...</td>\n",
       "      <td>(5, 0)</td>\n",
       "      <td>EJ/B-person/NNP/B-NP/O Manuel/I-person/NNP/B-V...</td>\n",
       "      <td>But/O/CC/O/O my/O/PRP$/B-NP/O bro/O/NN/I-NP/O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1st QB</td>\n",
       "      <td>EJ Manuel the 1st QB to go in this draft</td>\n",
       "      <td>Can believe EJ Manuel went as the 1st QB in th...</td>\n",
       "      <td>(5, 0)</td>\n",
       "      <td>EJ/B-person/NNP/B-NP/O Manuel/I-person/NNP/B-V...</td>\n",
       "      <td>Can/O/MD/B-VP/O believe/O/VB/I-VP/B-EVENT EJ/B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1st QB</td>\n",
       "      <td>EJ Manuel the 1st QB to go in this draft</td>\n",
       "      <td>EJ MANUEL IS THE 1ST QB what</td>\n",
       "      <td>(3, 2)</td>\n",
       "      <td>EJ/B-person/NNP/B-NP/O Manuel/I-person/NNP/B-V...</td>\n",
       "      <td>EJ/B-person/NNP/B-NP/O MANUEL/I-person/NNP/I-N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1st QB</td>\n",
       "      <td>EJ Manuel the 1st QB to go in this draft</td>\n",
       "      <td>EJ da 1st QB off da board</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>EJ/B-person/NNP/B-NP/O Manuel/I-person/NNP/B-V...</td>\n",
       "      <td>EJ/O/NNP/B-NP/O da/O/DT/I-NP/O 1st/O/CD/I-NP/O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1st QB</td>\n",
       "      <td>EJ Manuel the 1st QB to go in this draft</td>\n",
       "      <td>Manuel is the 1st QB to get drafted</td>\n",
       "      <td>(4, 1)</td>\n",
       "      <td>EJ/B-person/NNP/B-NP/O Manuel/I-person/NNP/B-V...</td>\n",
       "      <td>Manuel/B-person/NNP/B-NP/O is/O/VBZ/B-VP/O the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1                                         2  \\\n",
       "0  4  1st QB  EJ Manuel the 1st QB to go in this draft   \n",
       "1  4  1st QB  EJ Manuel the 1st QB to go in this draft   \n",
       "2  4  1st QB  EJ Manuel the 1st QB to go in this draft   \n",
       "3  4  1st QB  EJ Manuel the 1st QB to go in this draft   \n",
       "4  4  1st QB  EJ Manuel the 1st QB to go in this draft   \n",
       "\n",
       "                                                   3       4  \\\n",
       "0  But my bro from the 757 EJ Manuel is the 1st Q...  (5, 0)   \n",
       "1  Can believe EJ Manuel went as the 1st QB in th...  (5, 0)   \n",
       "2                       EJ MANUEL IS THE 1ST QB what  (3, 2)   \n",
       "3                          EJ da 1st QB off da board  (2, 3)   \n",
       "4                Manuel is the 1st QB to get drafted  (4, 1)   \n",
       "\n",
       "                                                   5  \\\n",
       "0  EJ/B-person/NNP/B-NP/O Manuel/I-person/NNP/B-V...   \n",
       "1  EJ/B-person/NNP/B-NP/O Manuel/I-person/NNP/B-V...   \n",
       "2  EJ/B-person/NNP/B-NP/O Manuel/I-person/NNP/B-V...   \n",
       "3  EJ/B-person/NNP/B-NP/O Manuel/I-person/NNP/B-V...   \n",
       "4  EJ/B-person/NNP/B-NP/O Manuel/I-person/NNP/B-V...   \n",
       "\n",
       "                                                   6  \n",
       "0  But/O/CC/O/O my/O/PRP$/B-NP/O bro/O/NN/I-NP/O ...  \n",
       "1  Can/O/MD/B-VP/O believe/O/VB/I-VP/B-EVENT EJ/B...  \n",
       "2  EJ/B-person/NNP/B-NP/O MANUEL/I-person/NNP/I-N...  \n",
       "3  EJ/O/NNP/B-NP/O da/O/DT/I-NP/O 1st/O/CD/I-NP/O...  \n",
       "4  Manuel/B-person/NNP/B-NP/O is/O/VBZ/B-VP/O the...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "mT-YDmD5tZnk"
   },
   "outputs": [],
   "source": [
    "Column_name = ['ID', 'Topic_name', 'S1', 'S2', 'Label', 'S1Tag', 'S2Tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dcaZ7t9ttaMn"
   },
   "outputs": [],
   "source": [
    "Train.columns = Column_name\n",
    "Test.columns = Column_name\n",
    "Dev.columns = Column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4rX3vsf7tbmJ"
   },
   "outputs": [],
   "source": [
    "Train = Train.drop(Train.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "i0AMW6mtteV7",
    "outputId": "4adddb79-3311-4bc7-e0bb-713e8745e646"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Topic_name</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>Label</th>\n",
       "      <th>S1Tag</th>\n",
       "      <th>S2Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>All the home alones watching 8 mile</td>\n",
       "      <td>8 mile is on thats my movie</td>\n",
       "      <td>3</td>\n",
       "      <td>All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...</td>\n",
       "      <td>8/O/NN/B-NP/O mile/O/NN/I-NP/O is/O/VBZ/B-VP/O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>All the home alones watching 8 mile</td>\n",
       "      <td>The last rap battle in 8 Mile nevr gets old ahah</td>\n",
       "      <td>2</td>\n",
       "      <td>All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...</td>\n",
       "      <td>The/O/DT/B-NP/O last/O/JJ/I-NP/O rap/O/NN/I-NP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>All the home alones watching 8 mile</td>\n",
       "      <td>The rap battle at the end of 8 mile gets me so...</td>\n",
       "      <td>2</td>\n",
       "      <td>All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...</td>\n",
       "      <td>The/O/DT/B-NP/O rap/O/NN/I-NP/O battle/O/NN/I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>The Ending to 8 Mile is my fav part of the who...</td>\n",
       "      <td>Rabbit on 8 mile out of place but determined t...</td>\n",
       "      <td>1</td>\n",
       "      <td>The/O/DT/B-NP/O Ending/O/VBG/I-NP/B-EVENT to/O...</td>\n",
       "      <td>Rabbit/O/NNP/B-NP/O on/O/IN/B-PP/O 8/O/CD/B-NP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>The Ending to 8 Mile is my fav part of the who...</td>\n",
       "      <td>See 8 Mile is always on but it s the tv versio...</td>\n",
       "      <td>1</td>\n",
       "      <td>The/O/DT/B-NP/O Ending/O/VBG/I-NP/B-EVENT to/O...</td>\n",
       "      <td>See/O/VB/B-VP/O 8/O/CD/B-NP/O Mile/O/NNP/I-NP/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Topic_name                                                 S1  \\\n",
       "0  51     8 Mile                All the home alones watching 8 mile   \n",
       "1  51     8 Mile                All the home alones watching 8 mile   \n",
       "2  51     8 Mile                All the home alones watching 8 mile   \n",
       "3  51     8 Mile  The Ending to 8 Mile is my fav part of the who...   \n",
       "4  51     8 Mile  The Ending to 8 Mile is my fav part of the who...   \n",
       "\n",
       "                                                  S2  Label  \\\n",
       "0                        8 mile is on thats my movie      3   \n",
       "1   The last rap battle in 8 Mile nevr gets old ahah      2   \n",
       "2  The rap battle at the end of 8 mile gets me so...      2   \n",
       "3  Rabbit on 8 mile out of place but determined t...      1   \n",
       "4  See 8 Mile is always on but it s the tv versio...      1   \n",
       "\n",
       "                                               S1Tag  \\\n",
       "0  All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...   \n",
       "1  All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...   \n",
       "2  All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...   \n",
       "3  The/O/DT/B-NP/O Ending/O/VBG/I-NP/B-EVENT to/O...   \n",
       "4  The/O/DT/B-NP/O Ending/O/VBG/I-NP/B-EVENT to/O...   \n",
       "\n",
       "                                               S2Tag  \n",
       "0  8/O/NN/B-NP/O mile/O/NN/I-NP/O is/O/VBZ/B-VP/O...  \n",
       "1  The/O/DT/B-NP/O last/O/JJ/I-NP/O rap/O/NN/I-NP...  \n",
       "2  The/O/DT/B-NP/O rap/O/NN/I-NP/O battle/O/NN/I-...  \n",
       "3  Rabbit/O/NNP/B-NP/O on/O/IN/B-PP/O 8/O/CD/B-NP...  \n",
       "4  See/O/VB/B-VP/O 8/O/CD/B-NP/O Mile/O/NNP/I-NP/...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "9VMFhxu0th-U"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def Define_Lable(df, lable_name):\n",
    "    final_label = []\n",
    "    dicard_line = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if len(str(df[lable_name][i])) != 1:\n",
    "            score1 = df[lable_name][i][1]\n",
    "            score2 = df[lable_name][i][4]\n",
    "\n",
    "            if int(score1) > 2:\n",
    "                final_label.append(1)\n",
    "            elif int(score1) == 2:\n",
    "                dicard_line.append(i)\n",
    "            else:\n",
    "                final_label.append(0)\n",
    "        else:\n",
    "            score = int(df[lable_name][i])\n",
    "            if score > 3:\n",
    "                final_label.append(1)\n",
    "            elif score == 3:\n",
    "                dicard_line.append(i)\n",
    "            else:\n",
    "                final_label.append(0)\n",
    "            \n",
    "    final_df = df.drop(df.index[dicard_line])\n",
    "    final_df[lable_name] = final_label\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxGhuvm1tknm",
    "outputId": "779f086a-215e-4f23-982f-27de8654c0a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 13063/13063 [00:00<00:00, 63882.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 4727/4727 [00:00<00:00, 73096.77it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 972/972 [00:00<00:00, 88612.06it/s]\n"
     ]
    }
   ],
   "source": [
    "Train_final = Define_Lable(Train, 'Label')\n",
    "Dev_final = Define_Lable(Dev, 'Label')\n",
    "Test_final = Define_Lable(Test, 'Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "0RAzBrxktmgU"
   },
   "outputs": [],
   "source": [
    "# data preprocessing class\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "\n",
    "class Word_Preprocessing():\n",
    "    def eliminate_url(self,df,target):\n",
    "        print('Start eliminate url: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        text = df_temp[target_column_name]\n",
    "        for i in tqdm(text):\n",
    "            urls = re.findall(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', i)\n",
    "            for i in urls:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    def eliminate_username(self,df,target):\n",
    "        print('Start eliminate username: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        for i in tqdm(df_temp[target_column_name]):\n",
    "            user_name = re.findall(r'@\\w*', i)\n",
    "            for i in user_name:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "     \n",
    "    def eliminate_hashtag(self, df, target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        for i in tqdm(df_temp[target_column_name]):\n",
    "            user_name = re.findall(r'#\\w*', i)\n",
    "            for i in user_name:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    def convert_abbreviation(self, df, target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        am = \"'m\"\n",
    "        are = \"'re\"\n",
    "        have = \"'ve\"\n",
    "        not_ = \"n't\"\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(am, \" am\"))\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(are, \"  are\"))\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(have, \" have\"))\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(not_, \" not\"))\n",
    "        return df_temp\n",
    "        \n",
    "    def lowercase_all(self, df, target):\n",
    "        df_tmp = df\n",
    "        tmp_str = [str.lower(i) for i in df[target]] \n",
    "        df_tmp[target] = tmp_str\n",
    "        return df_tmp \n",
    "        \n",
    "        \n",
    "    def eliminate_symbol(self,df,target):\n",
    "        print('Start eliminate symbol: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        symbol_list = [',',\"'\",'!','@','$','%','^','&','*','(',')','-','+','?','>','<','=','.',':',';','  ','  ','   ','    ','      ','      ','  ']\n",
    "        for i in tqdm(symbol_list):\n",
    "            df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, ' '))\n",
    "        return df_temp\n",
    "    \n",
    "    def process_all(self, df,target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        df_fresh = self.convert_abbreviation(df_temp,target_column_name)\n",
    "        df_remove_url = self.eliminate_url(df_fresh,target_column_name)\n",
    "        df_eliminate_hashtag = self.eliminate_hashtag(df_remove_url, target_column_name)\n",
    "        df_remove_username = self.eliminate_username(df_eliminate_hashtag, target_column_name)\n",
    "        df_remove_symbol = self.eliminate_symbol(df_remove_username, target_column_name)\n",
    "        df_final = self.lowercase_all(df_remove_symbol, target_column_name)\n",
    "        print(\"finished!!\")\n",
    "        return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Qd2LLGvMtn_T"
   },
   "outputs": [],
   "source": [
    "word_preprocesser = Word_Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bh0grS6It7zQ",
    "outputId": "54c8c89e-87c5-4ada-bc2a-ac8903e4a801"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 11530/11530 [00:00<00:00, 678665.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11530/11530 [00:00<00:00, 960216.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11530/11530 [00:00<00:00, 963429.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 477.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11530/11530 [00:00<00:00, 608482.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11530/11530 [00:00<00:00, 825670.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11530/11530 [00:00<00:00, 963314.71it/s]\n",
      "  0%|                                                                                           | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate url: : )\n",
      "Start eliminate username: : )\n",
      "Start eliminate symbol: : )\n",
      "finished!!\n",
      "Start eliminate url: : )\n",
      "Start eliminate username: : )\n",
      "Start eliminate symbol: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 401.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 4142/4142 [00:00<00:00, 692281.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 4142/4142 [00:00<00:00, 830718.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4142/4142 [00:00<00:00, 1038670.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 1002.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 4142/4142 [00:00<00:00, 686400.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 4142/4142 [00:00<00:00, 830598.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 4142/4142 [00:00<00:00, 825429.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 965.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 838/838 [00:00<00:00, 826434.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 838/838 [00:00<00:00, 840465.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 838/838 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 1933.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 838/838 [00:00<00:00, 840465.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 838/838 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 838/838 [00:00<00:00, 839261.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 2255.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!!\n",
      "Start eliminate url: : )\n",
      "Start eliminate username: : )\n",
      "Start eliminate symbol: : )\n",
      "finished!!\n",
      "Start eliminate url: : )\n",
      "Start eliminate username: : )\n",
      "Start eliminate symbol: : )\n",
      "finished!!\n",
      "Start eliminate url: : )\n",
      "Start eliminate username: : )\n",
      "Start eliminate symbol: : )\n",
      "finished!!\n",
      "Start eliminate url: : )\n",
      "Start eliminate username: : )\n",
      "Start eliminate symbol: : )\n",
      "finished!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_Train = word_preprocesser.process_all(Train_final, 'S1')\n",
    "Train_cleaned  = word_preprocesser.process_all(tmp_Train, 'S2')\n",
    "tmp_Dev = word_preprocesser.process_all(Dev_final, 'S1')\n",
    "Dev_cleaned = word_preprocesser.process_all(tmp_Dev, 'S2')\n",
    "tmp_Test = word_preprocesser.process_all(Test_final, 'S1')\n",
    "Test_cleaned = word_preprocesser.process_all(tmp_Test, 'S2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "id": "9TmbLwtbt9pU",
    "outputId": "cadc2b30-6361-4904-c5bf-55e2e9288612"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>Topic_name</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>Label</th>\n",
       "      <th>S1Tag</th>\n",
       "      <th>S2Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>all the home alones watching 8 mile</td>\n",
       "      <td>the last rap battle in 8 mile nevr gets old ahah</td>\n",
       "      <td>0</td>\n",
       "      <td>All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...</td>\n",
       "      <td>The/O/DT/B-NP/O last/O/JJ/I-NP/O rap/O/NN/I-NP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>all the home alones watching 8 mile</td>\n",
       "      <td>the rap battle at the end of 8 mile gets me so...</td>\n",
       "      <td>0</td>\n",
       "      <td>All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...</td>\n",
       "      <td>The/O/DT/B-NP/O rap/O/NN/I-NP/O battle/O/NN/I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>the ending to 8 mile is my fav part of the who...</td>\n",
       "      <td>rabbit on 8 mile out of place but determined t...</td>\n",
       "      <td>0</td>\n",
       "      <td>The/O/DT/B-NP/O Ending/O/VBG/I-NP/B-EVENT to/O...</td>\n",
       "      <td>Rabbit/O/NNP/B-NP/O on/O/IN/B-PP/O 8/O/CD/B-NP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>the ending to 8 mile is my fav part of the who...</td>\n",
       "      <td>see 8 mile is always on but it s the tv versio...</td>\n",
       "      <td>0</td>\n",
       "      <td>The/O/DT/B-NP/O Ending/O/VBG/I-NP/B-EVENT to/O...</td>\n",
       "      <td>See/O/VB/B-VP/O 8/O/CD/B-NP/O Mile/O/NNP/I-NP/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>8 Mile</td>\n",
       "      <td>the ending to 8 mile is my fav part of the who...</td>\n",
       "      <td>those last 3 battles in 8 mile are the shit</td>\n",
       "      <td>1</td>\n",
       "      <td>The/O/DT/B-NP/O Ending/O/VBG/I-NP/B-EVENT to/O...</td>\n",
       "      <td>Those/O/DT/B-NP/O last/O/JJ/I-NP/O 3/O/CD/I-NP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>967</td>\n",
       "      <td>5273</td>\n",
       "      <td>Z-Bo</td>\n",
       "      <td>ball dont lie zbo</td>\n",
       "      <td>if zbo didnt make that shot</td>\n",
       "      <td>0</td>\n",
       "      <td>Ball/O/NN/B-NP/O dont/O/MD/B-VP/O lie/O/VB/I-V...</td>\n",
       "      <td>If/O/IN/B-SBAR/O Zbo/O/NNP/B-NP/O didnt/O/MD/B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>968</td>\n",
       "      <td>5273</td>\n",
       "      <td>Z-Bo</td>\n",
       "      <td>the spurs own zbo</td>\n",
       "      <td>zbo is a fucking scrub</td>\n",
       "      <td>0</td>\n",
       "      <td>The/B-sportsteam/DT/B-NP/O Spurs/I-sportsteam/...</td>\n",
       "      <td>ZBo/B-person/NNP/B-NP/O is/O/VBZ/B-VP/O a/O/DT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>969</td>\n",
       "      <td>5273</td>\n",
       "      <td>Z-Bo</td>\n",
       "      <td>the spurs own zbo</td>\n",
       "      <td>zbo hit these free throws bruh</td>\n",
       "      <td>0</td>\n",
       "      <td>The/B-sportsteam/DT/B-NP/O Spurs/I-sportsteam/...</td>\n",
       "      <td>ZBo/O/MD/B-VP/O hit/O/VB/I-VP/B-EVENT these/O/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>970</td>\n",
       "      <td>5273</td>\n",
       "      <td>Z-Bo</td>\n",
       "      <td>the spurs own zbo</td>\n",
       "      <td>i told you feed zbo</td>\n",
       "      <td>0</td>\n",
       "      <td>The/B-sportsteam/DT/B-NP/O Spurs/I-sportsteam/...</td>\n",
       "      <td>I/O/PRP/B-NP/O told/O/VBD/B-VP/B-EVENT you/O/P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>971</td>\n",
       "      <td>5273</td>\n",
       "      <td>Z-Bo</td>\n",
       "      <td>the spurs own zbo</td>\n",
       "      <td>memphis is still my team and zbo is still my dude</td>\n",
       "      <td>0</td>\n",
       "      <td>The/B-sportsteam/DT/B-NP/O Spurs/I-sportsteam/...</td>\n",
       "      <td>Memphis/B-geo-loc/NNP/B-NP/O is/O/VBZ/B-VP/O s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>838 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    ID Topic_name  \\\n",
       "0        1    51     8 Mile   \n",
       "1        2    51     8 Mile   \n",
       "2        3    51     8 Mile   \n",
       "3        4    51     8 Mile   \n",
       "4        5    51     8 Mile   \n",
       "..     ...   ...        ...   \n",
       "833    967  5273       Z-Bo   \n",
       "834    968  5273       Z-Bo   \n",
       "835    969  5273       Z-Bo   \n",
       "836    970  5273       Z-Bo   \n",
       "837    971  5273       Z-Bo   \n",
       "\n",
       "                                                    S1  \\\n",
       "0                  all the home alones watching 8 mile   \n",
       "1                  all the home alones watching 8 mile   \n",
       "2    the ending to 8 mile is my fav part of the who...   \n",
       "3    the ending to 8 mile is my fav part of the who...   \n",
       "4    the ending to 8 mile is my fav part of the who...   \n",
       "..                                                 ...   \n",
       "833                                  ball dont lie zbo   \n",
       "834                                  the spurs own zbo   \n",
       "835                                  the spurs own zbo   \n",
       "836                                  the spurs own zbo   \n",
       "837                                  the spurs own zbo   \n",
       "\n",
       "                                                    S2  Label  \\\n",
       "0     the last rap battle in 8 mile nevr gets old ahah      0   \n",
       "1    the rap battle at the end of 8 mile gets me so...      0   \n",
       "2    rabbit on 8 mile out of place but determined t...      0   \n",
       "3    see 8 mile is always on but it s the tv versio...      0   \n",
       "4          those last 3 battles in 8 mile are the shit      1   \n",
       "..                                                 ...    ...   \n",
       "833                        if zbo didnt make that shot      0   \n",
       "834                             zbo is a fucking scrub      0   \n",
       "835                     zbo hit these free throws bruh      0   \n",
       "836                                i told you feed zbo      0   \n",
       "837  memphis is still my team and zbo is still my dude      0   \n",
       "\n",
       "                                                 S1Tag  \\\n",
       "0    All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...   \n",
       "1    All/O/DT/B-NP/O the/O/DT/I-NP/O home/O/NN/I-NP...   \n",
       "2    The/O/DT/B-NP/O Ending/O/VBG/I-NP/B-EVENT to/O...   \n",
       "3    The/O/DT/B-NP/O Ending/O/VBG/I-NP/B-EVENT to/O...   \n",
       "4    The/O/DT/B-NP/O Ending/O/VBG/I-NP/B-EVENT to/O...   \n",
       "..                                                 ...   \n",
       "833  Ball/O/NN/B-NP/O dont/O/MD/B-VP/O lie/O/VB/I-V...   \n",
       "834  The/B-sportsteam/DT/B-NP/O Spurs/I-sportsteam/...   \n",
       "835  The/B-sportsteam/DT/B-NP/O Spurs/I-sportsteam/...   \n",
       "836  The/B-sportsteam/DT/B-NP/O Spurs/I-sportsteam/...   \n",
       "837  The/B-sportsteam/DT/B-NP/O Spurs/I-sportsteam/...   \n",
       "\n",
       "                                                 S2Tag  \n",
       "0    The/O/DT/B-NP/O last/O/JJ/I-NP/O rap/O/NN/I-NP...  \n",
       "1    The/O/DT/B-NP/O rap/O/NN/I-NP/O battle/O/NN/I-...  \n",
       "2    Rabbit/O/NNP/B-NP/O on/O/IN/B-PP/O 8/O/CD/B-NP...  \n",
       "3    See/O/VB/B-VP/O 8/O/CD/B-NP/O Mile/O/NNP/I-NP/...  \n",
       "4    Those/O/DT/B-NP/O last/O/JJ/I-NP/O 3/O/CD/I-NP...  \n",
       "..                                                 ...  \n",
       "833  If/O/IN/B-SBAR/O Zbo/O/NNP/B-NP/O didnt/O/MD/B...  \n",
       "834  ZBo/B-person/NNP/B-NP/O is/O/VBZ/B-VP/O a/O/DT...  \n",
       "835  ZBo/O/MD/B-VP/O hit/O/VB/I-VP/B-EVENT these/O/...  \n",
       "836  I/O/PRP/B-NP/O told/O/VBD/B-VP/B-EVENT you/O/P...  \n",
       "837  Memphis/B-geo-loc/NNP/B-NP/O is/O/VBZ/B-VP/O s...  \n",
       "\n",
       "[838 rows x 8 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "oRSa7mQCusGV"
   },
   "outputs": [],
   "source": [
    "test_label_column = ['Binary Label' ,'Degreed Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "f_hLOUpNwrbv"
   },
   "outputs": [],
   "source": [
    "Test_label.columns = test_label_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4lq3z5k3K20"
   },
   "source": [
    "## model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "3_FFk0pcw02x"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "8eca8af0ddf845fb9af180af46a1c893",
      "2975d8d5f30c455faf7b4c3e2207a34f",
      "8a57d35be08c402a9e95fc460be6d893",
      "1e00b6cd83fd41c29b93b3420efce1ff",
      "adc7f582ff6349d18a178239650c3667",
      "e3053991b3054e60bf7bca1b823bec27",
      "3d770df66e614d7288f2adc92d09f966",
      "0a3df4068fb84ccabb53b8ba6c5fee33"
     ]
    },
    "id": "bJzUS3Zl3eLU",
    "outputId": "5a43a488-f847-4fb8-ef82-0009f558489d"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_cleaned = Train_cleaned.reset_index()\n",
    "Dev_cleaned = Dev_cleaned.reset_index()\n",
    "Test_cleaned = Test_cleaned.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "id": "k02x9xqP3rq3",
    "outputId": "f40726dd-827f-4bdf-ffd8-19424796c3cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 11530/11530 [00:00<00:00, 172518.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 4142/4142 [00:00<00:00, 180547.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 838/838 [00:00<00:00, 166943.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS] ej manuel the 1st qb to go in this draft [SEP] but my bro from the 757 ej manuel is the 1st qb gone',\n",
       " '[CLS] ej manuel the 1st qb to go in this draft [SEP] can believe ej manuel went as the 1st qb in the draft',\n",
       " '[CLS] ej manuel the 1st qb to go in this draft [SEP] ej manuel is the 1st qb what',\n",
       " '[CLS] ej manuel the 1st qb to go in this draft [SEP] manuel is the 1st qb to get drafted',\n",
       " '[CLS] ej manuel the 1st qb to go in this draft [SEP] my boy ej manuel being the 1st qb picked']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mergeSents(dataframe, s1_name, s2_name):\n",
    "  s1 = dataframe[s1_name]\n",
    "  s2 = dataframe[s2_name]\n",
    "  tmp = []\n",
    "  for i in tqdm(range(len(s1))):\n",
    "    merged_sent = '[CLS] '+s1[i]+' [SEP] '+s2[i]\n",
    "    tmp.append(merged_sent)\n",
    "  return tmp\n",
    "\n",
    "train_sent_tokens = mergeSents(Train_cleaned,'S1','S2')\n",
    "dev_sent_tokens = mergeSents(Dev_cleaned,'S1','S2')\n",
    "test_sent_tokens = mergeSents(Test_cleaned,'S1','S2')\n",
    "train_sent_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = {'sentences':train_sent_tokens,'label':Train_cleaned['Label']}\n",
    "dev_final = {'sentences':dev_sent_tokens,'label':Dev_cleaned['Label']}\n",
    "test_final = {'sentences':test_sent_tokens,'label':Test_cleaned['Label']}\n",
    "\n",
    "train_df = pd.DataFrame(train_final)\n",
    "dev_df = pd.DataFrame(dev_final)\n",
    "test_df = pd.DataFrame(test_final)\n",
    "\n",
    "train_df.to_csv('train_df.csv',header=True)\n",
    "dev_df.to_csv('dev_df.csv',header=True)\n",
    "test_df.to_csv('test_df.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = []\n",
    "for i in train_sent_tokens:\n",
    "    tmp = i.split(' ')\n",
    "    sent_len.append(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the range of the sentence token is: (10, 34)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sent_len, bins = 30)\n",
    "plt.show()\n",
    "print('the range of the sentence token is:', (min(sent_len),max(sent_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11530 entries, 0 to 11529\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentences  11530 non-null  object\n",
      " 1   label      11530 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 180.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3465741543798786\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in train_df['label']:\n",
    "    if i == 1:\n",
    "        count +=1\n",
    "print(count/len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 35\n",
    "BATCH_SIZE = 8\n",
    "pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "unk_index = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "label_field = Field(sequential = False, use_vocab=False, batch_first=True)\n",
    "text_field = Field(use_vocab=False, tokenize = tokenizer.encode, include_lengths=False, \n",
    "                   batch_first=True, fix_length=max_len,pad_token = pad_index,unk_token=unk_index)\n",
    "fields = [('sentences', text_field), ('label', label_field)]\n",
    "\n",
    "\n",
    "\n",
    "train, dev, test = TabularDataset.splits(path=source_folder, train='train_df.csv',\n",
    "                                           validation='dev_df.csv',\n",
    "                                           test='test_df.csv', format='CSV', \n",
    "                                           fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.sentences),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "dev_iter = BucketIterator(dev, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.sentences),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for saving and loading model parameters and metrics.\n",
    "def save_checkpoint(path, model, valid_loss):\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}, path)\n",
    "\n",
    "    \n",
    "def load_checkpoint(path, model):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(path, train_loss_list, valid_loss_list, global_steps_list):   \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, path)\n",
    "\n",
    "\n",
    "def load_metrics(path):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def forward(self,input_ids, attention_mask):\n",
    "        x = self.bert(input_ids=input_ids, attention_mask = attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter, dev_iter, epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    dev_loss = 0.0\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        count = 0\n",
    "        for (source, target), _ in train_iter:\n",
    "            mask = (source!=pad_index).type(torch.uint8)\n",
    "            y_pred = model(input_ids = source, attention_mask=mask)[0]\n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred,target)\n",
    "            print('batch_no[{}/{}]:'.format(count, int(len(train_iter))),'training_loss:', loss)\n",
    "            count+=1\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_loss == loss.item()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step% len(train_iter)==0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for (source,target), _ in dev_iter:\n",
    "                        mask = (source != pad_index).type(torch.uint8)\n",
    "                        \n",
    "                        y_pred = model(input_ids = source, attention_mask = mask)[0]\n",
    "                        \n",
    "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "                        \n",
    "                        dev_loss += loss.item()\n",
    "                        \n",
    "                train_loss = train_loss/ len(train_iter)\n",
    "                dev_loss = dev_loss/len(dev_iter)\n",
    "                \n",
    "                model.train()\n",
    "                print('Epoch [{}/{}],global step [{}/{}], pt loss:{:.4f}, dev loss:{:.4f}'.format(epoch+1, epochs, \n",
    "                                                                                                  global_step, \n",
    "                                                                                                  epochs*len(train_iter),\n",
    "                                                                                                 train_loss,\n",
    "                                                                                              dev_loss))\n",
    "                train_loss = 0.0\n",
    "                dev_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[0/1442]: training_loss: tensor(0.8335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1/1442]: training_loss: tensor(0.7971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[2/1442]: training_loss: tensor(1.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[3/1442]: training_loss: tensor(0.7799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[4/1442]: training_loss: tensor(0.8429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[5/1442]: training_loss: tensor(0.8874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[6/1442]: training_loss: tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[7/1442]: training_loss: tensor(0.8379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[8/1442]: training_loss: tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[9/1442]: training_loss: tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[10/1442]: training_loss: tensor(0.7466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[11/1442]: training_loss: tensor(0.7683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[12/1442]: training_loss: tensor(0.8428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[13/1442]: training_loss: tensor(0.7444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[14/1442]: training_loss: tensor(0.7598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[15/1442]: training_loss: tensor(0.8064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[16/1442]: training_loss: tensor(0.7194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[17/1442]: training_loss: tensor(0.8111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[18/1442]: training_loss: tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[19/1442]: training_loss: tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[20/1442]: training_loss: tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[21/1442]: training_loss: tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[22/1442]: training_loss: tensor(0.7653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[23/1442]: training_loss: tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[24/1442]: training_loss: tensor(0.7684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[25/1442]: training_loss: tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[26/1442]: training_loss: tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[27/1442]: training_loss: tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[28/1442]: training_loss: tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[29/1442]: training_loss: tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[30/1442]: training_loss: tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[31/1442]: training_loss: tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[32/1442]: training_loss: tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[33/1442]: training_loss: tensor(0.6194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[34/1442]: training_loss: tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[35/1442]: training_loss: tensor(0.6382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[36/1442]: training_loss: tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[37/1442]: training_loss: tensor(0.5973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[38/1442]: training_loss: tensor(0.5260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[39/1442]: training_loss: tensor(0.5757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[40/1442]: training_loss: tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[41/1442]: training_loss: tensor(0.6714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[42/1442]: training_loss: tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[43/1442]: training_loss: tensor(0.5982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[44/1442]: training_loss: tensor(0.7323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[45/1442]: training_loss: tensor(0.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[46/1442]: training_loss: tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[47/1442]: training_loss: tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[48/1442]: training_loss: tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[49/1442]: training_loss: tensor(0.5979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[50/1442]: training_loss: tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[51/1442]: training_loss: tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[52/1442]: training_loss: tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[53/1442]: training_loss: tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[54/1442]: training_loss: tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[55/1442]: training_loss: tensor(0.6056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[56/1442]: training_loss: tensor(0.8252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[57/1442]: training_loss: tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[58/1442]: training_loss: tensor(0.6524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[59/1442]: training_loss: tensor(0.4469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[60/1442]: training_loss: tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[61/1442]: training_loss: tensor(0.7894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[62/1442]: training_loss: tensor(0.5383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[63/1442]: training_loss: tensor(0.6210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[64/1442]: training_loss: tensor(0.4099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[65/1442]: training_loss: tensor(0.4946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[66/1442]: training_loss: tensor(0.4567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[67/1442]: training_loss: tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[68/1442]: training_loss: tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[69/1442]: training_loss: tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[70/1442]: training_loss: tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[71/1442]: training_loss: tensor(0.5321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[72/1442]: training_loss: tensor(0.7210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[73/1442]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[74/1442]: training_loss: tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[75/1442]: training_loss: tensor(0.4246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[76/1442]: training_loss: tensor(0.8543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[77/1442]: training_loss: tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[78/1442]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[79/1442]: training_loss: tensor(0.5145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[80/1442]: training_loss: tensor(0.5444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[81/1442]: training_loss: tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[82/1442]: training_loss: tensor(0.3441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[83/1442]: training_loss: tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[84/1442]: training_loss: tensor(0.4597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[85/1442]: training_loss: tensor(0.3945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[86/1442]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[87/1442]: training_loss: tensor(0.4560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[88/1442]: training_loss: tensor(0.9398, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[89/1442]: training_loss: tensor(0.4510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[90/1442]: training_loss: tensor(0.4366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[91/1442]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[92/1442]: training_loss: tensor(0.4548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[93/1442]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[94/1442]: training_loss: tensor(0.5831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[95/1442]: training_loss: tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[96/1442]: training_loss: tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[97/1442]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[98/1442]: training_loss: tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[99/1442]: training_loss: tensor(0.9463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[100/1442]: training_loss: tensor(0.7966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[101/1442]: training_loss: tensor(0.8866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[102/1442]: training_loss: tensor(0.6405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[103/1442]: training_loss: tensor(0.6361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[104/1442]: training_loss: tensor(0.8622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[105/1442]: training_loss: tensor(0.4139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[106/1442]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[107/1442]: training_loss: tensor(0.5173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[108/1442]: training_loss: tensor(1.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[109/1442]: training_loss: tensor(0.5448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[110/1442]: training_loss: tensor(0.4772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[111/1442]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[112/1442]: training_loss: tensor(0.8104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[113/1442]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[114/1442]: training_loss: tensor(0.6830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[115/1442]: training_loss: tensor(0.5692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[116/1442]: training_loss: tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[117/1442]: training_loss: tensor(0.5022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[118/1442]: training_loss: tensor(0.4765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[119/1442]: training_loss: tensor(0.5366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[120/1442]: training_loss: tensor(0.5891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[121/1442]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[122/1442]: training_loss: tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[123/1442]: training_loss: tensor(0.4591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[124/1442]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[125/1442]: training_loss: tensor(0.4212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[126/1442]: training_loss: tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[127/1442]: training_loss: tensor(0.8128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[128/1442]: training_loss: tensor(1.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[129/1442]: training_loss: tensor(0.4923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[130/1442]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[131/1442]: training_loss: tensor(0.4276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[132/1442]: training_loss: tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[133/1442]: training_loss: tensor(0.5073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[134/1442]: training_loss: tensor(0.9014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[135/1442]: training_loss: tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[136/1442]: training_loss: tensor(0.8953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[137/1442]: training_loss: tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[138/1442]: training_loss: tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[139/1442]: training_loss: tensor(0.8547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[140/1442]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[141/1442]: training_loss: tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[142/1442]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[143/1442]: training_loss: tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[144/1442]: training_loss: tensor(1.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[145/1442]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[146/1442]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[147/1442]: training_loss: tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[148/1442]: training_loss: tensor(0.9817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[149/1442]: training_loss: tensor(0.4655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[150/1442]: training_loss: tensor(0.4361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[151/1442]: training_loss: tensor(0.5317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[152/1442]: training_loss: tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[153/1442]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[154/1442]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[155/1442]: training_loss: tensor(0.4988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[156/1442]: training_loss: tensor(0.3246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[157/1442]: training_loss: tensor(0.5721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[158/1442]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[159/1442]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[160/1442]: training_loss: tensor(0.3124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[161/1442]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[162/1442]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[163/1442]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[164/1442]: training_loss: tensor(0.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[165/1442]: training_loss: tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[166/1442]: training_loss: tensor(0.6202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[167/1442]: training_loss: tensor(0.8901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[168/1442]: training_loss: tensor(0.6197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[169/1442]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[170/1442]: training_loss: tensor(0.4335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[171/1442]: training_loss: tensor(0.6706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[172/1442]: training_loss: tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[173/1442]: training_loss: tensor(0.8383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[174/1442]: training_loss: tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[175/1442]: training_loss: tensor(0.3871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[176/1442]: training_loss: tensor(0.8056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[177/1442]: training_loss: tensor(0.7394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[178/1442]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[179/1442]: training_loss: tensor(0.9548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[180/1442]: training_loss: tensor(0.9639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[181/1442]: training_loss: tensor(0.8750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[182/1442]: training_loss: tensor(0.7167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[183/1442]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[184/1442]: training_loss: tensor(0.3088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[185/1442]: training_loss: tensor(0.7793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[186/1442]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[187/1442]: training_loss: tensor(1.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[188/1442]: training_loss: tensor(1.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[189/1442]: training_loss: tensor(1.3196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[190/1442]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[191/1442]: training_loss: tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[192/1442]: training_loss: tensor(0.9482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[193/1442]: training_loss: tensor(0.4387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[194/1442]: training_loss: tensor(0.3831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[195/1442]: training_loss: tensor(0.4339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[196/1442]: training_loss: tensor(0.5449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[197/1442]: training_loss: tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[198/1442]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[199/1442]: training_loss: tensor(0.5009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[200/1442]: training_loss: tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[201/1442]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[202/1442]: training_loss: tensor(0.8529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[203/1442]: training_loss: tensor(0.5505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[204/1442]: training_loss: tensor(0.2823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[205/1442]: training_loss: tensor(0.5486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[206/1442]: training_loss: tensor(1.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[207/1442]: training_loss: tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[208/1442]: training_loss: tensor(0.4729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[209/1442]: training_loss: tensor(0.4156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[210/1442]: training_loss: tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[211/1442]: training_loss: tensor(0.3216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[212/1442]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[213/1442]: training_loss: tensor(0.4737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[214/1442]: training_loss: tensor(0.9110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[215/1442]: training_loss: tensor(0.4911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[216/1442]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[217/1442]: training_loss: tensor(0.4429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[218/1442]: training_loss: tensor(0.5430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[219/1442]: training_loss: tensor(0.4328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[220/1442]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[221/1442]: training_loss: tensor(0.9570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[222/1442]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[223/1442]: training_loss: tensor(0.8974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[224/1442]: training_loss: tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[225/1442]: training_loss: tensor(0.8333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[226/1442]: training_loss: tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[227/1442]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[228/1442]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[229/1442]: training_loss: tensor(0.6064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[230/1442]: training_loss: tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[231/1442]: training_loss: tensor(0.5372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[232/1442]: training_loss: tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[233/1442]: training_loss: tensor(0.6382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[234/1442]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[235/1442]: training_loss: tensor(0.8705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[236/1442]: training_loss: tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[237/1442]: training_loss: tensor(0.6338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[238/1442]: training_loss: tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[239/1442]: training_loss: tensor(0.4369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[240/1442]: training_loss: tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[241/1442]: training_loss: tensor(0.8290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[242/1442]: training_loss: tensor(0.9578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[243/1442]: training_loss: tensor(0.7437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[244/1442]: training_loss: tensor(0.8195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[245/1442]: training_loss: tensor(0.8523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[246/1442]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[247/1442]: training_loss: tensor(0.8842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[248/1442]: training_loss: tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[249/1442]: training_loss: tensor(0.7320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[250/1442]: training_loss: tensor(0.8029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[251/1442]: training_loss: tensor(0.4418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[252/1442]: training_loss: tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[253/1442]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[254/1442]: training_loss: tensor(0.4361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[255/1442]: training_loss: tensor(0.5679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[256/1442]: training_loss: tensor(0.3648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[257/1442]: training_loss: tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[258/1442]: training_loss: tensor(0.4637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[259/1442]: training_loss: tensor(0.6017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[260/1442]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[261/1442]: training_loss: tensor(0.4584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[262/1442]: training_loss: tensor(0.5851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[263/1442]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[264/1442]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[265/1442]: training_loss: tensor(0.2758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[266/1442]: training_loss: tensor(0.4702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[267/1442]: training_loss: tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[268/1442]: training_loss: tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[269/1442]: training_loss: tensor(0.4474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[270/1442]: training_loss: tensor(0.4855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[271/1442]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[272/1442]: training_loss: tensor(0.4051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[273/1442]: training_loss: tensor(0.5298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[274/1442]: training_loss: tensor(0.7582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[275/1442]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[276/1442]: training_loss: tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[277/1442]: training_loss: tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[278/1442]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[279/1442]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[280/1442]: training_loss: tensor(0.4306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[281/1442]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[282/1442]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[283/1442]: training_loss: tensor(1.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[284/1442]: training_loss: tensor(1.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[285/1442]: training_loss: tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[286/1442]: training_loss: tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[287/1442]: training_loss: tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[288/1442]: training_loss: tensor(0.9856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[289/1442]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[290/1442]: training_loss: tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[291/1442]: training_loss: tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[292/1442]: training_loss: tensor(1.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[293/1442]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[294/1442]: training_loss: tensor(0.6168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[295/1442]: training_loss: tensor(0.6872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[296/1442]: training_loss: tensor(0.9116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[297/1442]: training_loss: tensor(0.9512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[298/1442]: training_loss: tensor(1.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[299/1442]: training_loss: tensor(0.4930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[300/1442]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[301/1442]: training_loss: tensor(0.6994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[302/1442]: training_loss: tensor(0.3573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[303/1442]: training_loss: tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[304/1442]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[305/1442]: training_loss: tensor(0.7441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[306/1442]: training_loss: tensor(0.8517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[307/1442]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[308/1442]: training_loss: tensor(0.3262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[309/1442]: training_loss: tensor(0.4820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[310/1442]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[311/1442]: training_loss: tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[312/1442]: training_loss: tensor(0.5206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[313/1442]: training_loss: tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[314/1442]: training_loss: tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[315/1442]: training_loss: tensor(0.6650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[316/1442]: training_loss: tensor(0.9315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[317/1442]: training_loss: tensor(0.9858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[318/1442]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[319/1442]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[320/1442]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[321/1442]: training_loss: tensor(0.5371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[322/1442]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[323/1442]: training_loss: tensor(0.8642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[324/1442]: training_loss: tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[325/1442]: training_loss: tensor(0.5628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[326/1442]: training_loss: tensor(0.3073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[327/1442]: training_loss: tensor(0.4813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[328/1442]: training_loss: tensor(0.4408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[329/1442]: training_loss: tensor(0.5070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[330/1442]: training_loss: tensor(0.8947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[331/1442]: training_loss: tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[332/1442]: training_loss: tensor(0.5742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[333/1442]: training_loss: tensor(0.9069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[334/1442]: training_loss: tensor(0.7622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[335/1442]: training_loss: tensor(0.9228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[336/1442]: training_loss: tensor(0.6200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[337/1442]: training_loss: tensor(0.9142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[338/1442]: training_loss: tensor(0.2785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[339/1442]: training_loss: tensor(0.5351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[340/1442]: training_loss: tensor(0.5124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[341/1442]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[342/1442]: training_loss: tensor(0.4313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[343/1442]: training_loss: tensor(0.4344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[344/1442]: training_loss: tensor(0.6544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[345/1442]: training_loss: tensor(0.5990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[346/1442]: training_loss: tensor(0.9110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[347/1442]: training_loss: tensor(0.5454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[348/1442]: training_loss: tensor(0.7520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[349/1442]: training_loss: tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[350/1442]: training_loss: tensor(0.5176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[351/1442]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[352/1442]: training_loss: tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[353/1442]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[354/1442]: training_loss: tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[355/1442]: training_loss: tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[356/1442]: training_loss: tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[357/1442]: training_loss: tensor(0.4845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[358/1442]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[359/1442]: training_loss: tensor(0.4055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[360/1442]: training_loss: tensor(0.9112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[361/1442]: training_loss: tensor(0.9295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[362/1442]: training_loss: tensor(0.5006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[363/1442]: training_loss: tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[364/1442]: training_loss: tensor(0.8371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[365/1442]: training_loss: tensor(0.6198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[366/1442]: training_loss: tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[367/1442]: training_loss: tensor(0.4947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[368/1442]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[369/1442]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[370/1442]: training_loss: tensor(0.3184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[371/1442]: training_loss: tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[372/1442]: training_loss: tensor(0.4328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[373/1442]: training_loss: tensor(0.4887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[374/1442]: training_loss: tensor(0.5680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[375/1442]: training_loss: tensor(0.4926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[376/1442]: training_loss: tensor(0.4981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[377/1442]: training_loss: tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[378/1442]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[379/1442]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[380/1442]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[381/1442]: training_loss: tensor(0.3303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[382/1442]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[383/1442]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[384/1442]: training_loss: tensor(0.4105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[385/1442]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[386/1442]: training_loss: tensor(0.4414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[387/1442]: training_loss: tensor(0.9080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[388/1442]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[389/1442]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[390/1442]: training_loss: tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[391/1442]: training_loss: tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[392/1442]: training_loss: tensor(0.8865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[393/1442]: training_loss: tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[394/1442]: training_loss: tensor(0.5105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[395/1442]: training_loss: tensor(0.4310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[396/1442]: training_loss: tensor(0.4312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[397/1442]: training_loss: tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[398/1442]: training_loss: tensor(0.4917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[399/1442]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[400/1442]: training_loss: tensor(0.4721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[401/1442]: training_loss: tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[402/1442]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[403/1442]: training_loss: tensor(0.5927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[404/1442]: training_loss: tensor(0.3635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[405/1442]: training_loss: tensor(0.8654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[406/1442]: training_loss: tensor(1.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[407/1442]: training_loss: tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[408/1442]: training_loss: tensor(0.4907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[409/1442]: training_loss: tensor(0.9936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[410/1442]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[411/1442]: training_loss: tensor(0.4478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[412/1442]: training_loss: tensor(0.5293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[413/1442]: training_loss: tensor(0.9199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[414/1442]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[415/1442]: training_loss: tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[416/1442]: training_loss: tensor(0.4452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[417/1442]: training_loss: tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[418/1442]: training_loss: tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[419/1442]: training_loss: tensor(0.7809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[420/1442]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[421/1442]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[422/1442]: training_loss: tensor(0.3659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[423/1442]: training_loss: tensor(0.2844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[424/1442]: training_loss: tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[425/1442]: training_loss: tensor(0.9662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[426/1442]: training_loss: tensor(0.5409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[427/1442]: training_loss: tensor(0.4969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[428/1442]: training_loss: tensor(0.4396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[429/1442]: training_loss: tensor(0.4298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[430/1442]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[431/1442]: training_loss: tensor(0.7982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[432/1442]: training_loss: tensor(0.5145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[433/1442]: training_loss: tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[434/1442]: training_loss: tensor(0.7791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[435/1442]: training_loss: tensor(0.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[436/1442]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[437/1442]: training_loss: tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[438/1442]: training_loss: tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[439/1442]: training_loss: tensor(0.9074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[440/1442]: training_loss: tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[441/1442]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[442/1442]: training_loss: tensor(0.4067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[443/1442]: training_loss: tensor(0.8006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[444/1442]: training_loss: tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[445/1442]: training_loss: tensor(0.4676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[446/1442]: training_loss: tensor(0.4864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[447/1442]: training_loss: tensor(0.4625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[448/1442]: training_loss: tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[449/1442]: training_loss: tensor(0.8254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[450/1442]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[451/1442]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[452/1442]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[453/1442]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[454/1442]: training_loss: tensor(0.6202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[455/1442]: training_loss: tensor(0.9456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[456/1442]: training_loss: tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[457/1442]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[458/1442]: training_loss: tensor(0.8041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[459/1442]: training_loss: tensor(0.4802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[460/1442]: training_loss: tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[461/1442]: training_loss: tensor(0.5002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[462/1442]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[463/1442]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[464/1442]: training_loss: tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[465/1442]: training_loss: tensor(0.7567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[466/1442]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[467/1442]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[468/1442]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[469/1442]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[470/1442]: training_loss: tensor(0.3343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[471/1442]: training_loss: tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[472/1442]: training_loss: tensor(0.9285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[473/1442]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[474/1442]: training_loss: tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[475/1442]: training_loss: tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[476/1442]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[477/1442]: training_loss: tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[478/1442]: training_loss: tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[479/1442]: training_loss: tensor(0.9337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[480/1442]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[481/1442]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[482/1442]: training_loss: tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[483/1442]: training_loss: tensor(0.4730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[484/1442]: training_loss: tensor(0.4645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[485/1442]: training_loss: tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[486/1442]: training_loss: tensor(0.7651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[487/1442]: training_loss: tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[488/1442]: training_loss: tensor(0.6643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[489/1442]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[490/1442]: training_loss: tensor(0.7909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[491/1442]: training_loss: tensor(0.8035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[492/1442]: training_loss: tensor(0.6793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[493/1442]: training_loss: tensor(0.7983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[494/1442]: training_loss: tensor(0.5088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[495/1442]: training_loss: tensor(0.8507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[496/1442]: training_loss: tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[497/1442]: training_loss: tensor(0.4776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[498/1442]: training_loss: tensor(0.7596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[499/1442]: training_loss: tensor(0.5423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[500/1442]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[501/1442]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[502/1442]: training_loss: tensor(0.3540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[503/1442]: training_loss: tensor(0.4472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[504/1442]: training_loss: tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[505/1442]: training_loss: tensor(0.4410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[506/1442]: training_loss: tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[507/1442]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[508/1442]: training_loss: tensor(0.7418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[509/1442]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[510/1442]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[511/1442]: training_loss: tensor(0.4413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[512/1442]: training_loss: tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[513/1442]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[514/1442]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[515/1442]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[516/1442]: training_loss: tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[517/1442]: training_loss: tensor(0.8030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[518/1442]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[519/1442]: training_loss: tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[520/1442]: training_loss: tensor(0.5144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[521/1442]: training_loss: tensor(1.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[522/1442]: training_loss: tensor(0.9921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[523/1442]: training_loss: tensor(0.7503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[524/1442]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[525/1442]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[526/1442]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[527/1442]: training_loss: tensor(0.4803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[528/1442]: training_loss: tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[529/1442]: training_loss: tensor(0.8063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[530/1442]: training_loss: tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[531/1442]: training_loss: tensor(0.5767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[532/1442]: training_loss: tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[533/1442]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[534/1442]: training_loss: tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[535/1442]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[536/1442]: training_loss: tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[537/1442]: training_loss: tensor(0.7338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[538/1442]: training_loss: tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[539/1442]: training_loss: tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[540/1442]: training_loss: tensor(0.5238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[541/1442]: training_loss: tensor(0.5823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[542/1442]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[543/1442]: training_loss: tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[544/1442]: training_loss: tensor(0.3190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[545/1442]: training_loss: tensor(0.4220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[546/1442]: training_loss: tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[547/1442]: training_loss: tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[548/1442]: training_loss: tensor(0.4662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[549/1442]: training_loss: tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[550/1442]: training_loss: tensor(0.3659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[551/1442]: training_loss: tensor(0.5791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[552/1442]: training_loss: tensor(0.7217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[553/1442]: training_loss: tensor(0.5734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[554/1442]: training_loss: tensor(0.4349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[555/1442]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[556/1442]: training_loss: tensor(0.8278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[557/1442]: training_loss: tensor(0.5731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[558/1442]: training_loss: tensor(0.4708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[559/1442]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[560/1442]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[561/1442]: training_loss: tensor(0.7772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[562/1442]: training_loss: tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[563/1442]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[564/1442]: training_loss: tensor(0.4961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[565/1442]: training_loss: tensor(0.4294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[566/1442]: training_loss: tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[567/1442]: training_loss: tensor(0.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[568/1442]: training_loss: tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[569/1442]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[570/1442]: training_loss: tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[571/1442]: training_loss: tensor(0.6191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[572/1442]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[573/1442]: training_loss: tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[574/1442]: training_loss: tensor(0.4369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[575/1442]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[576/1442]: training_loss: tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[577/1442]: training_loss: tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[578/1442]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[579/1442]: training_loss: tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[580/1442]: training_loss: tensor(0.4212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[581/1442]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[582/1442]: training_loss: tensor(0.8918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[583/1442]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[584/1442]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[585/1442]: training_loss: tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[586/1442]: training_loss: tensor(0.4715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[587/1442]: training_loss: tensor(0.4177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[588/1442]: training_loss: tensor(0.4940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[589/1442]: training_loss: tensor(0.2852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[590/1442]: training_loss: tensor(0.4614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[591/1442]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[592/1442]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[593/1442]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[594/1442]: training_loss: tensor(0.7229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[595/1442]: training_loss: tensor(0.9167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[596/1442]: training_loss: tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[597/1442]: training_loss: tensor(1.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[598/1442]: training_loss: tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[599/1442]: training_loss: tensor(0.5566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[600/1442]: training_loss: tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[601/1442]: training_loss: tensor(0.7820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[602/1442]: training_loss: tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[603/1442]: training_loss: tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[604/1442]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[605/1442]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[606/1442]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[607/1442]: training_loss: tensor(0.3625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[608/1442]: training_loss: tensor(0.5741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[609/1442]: training_loss: tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[610/1442]: training_loss: tensor(0.6157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[611/1442]: training_loss: tensor(0.5472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[612/1442]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[613/1442]: training_loss: tensor(0.7919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[614/1442]: training_loss: tensor(0.7958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[615/1442]: training_loss: tensor(0.5592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[616/1442]: training_loss: tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[617/1442]: training_loss: tensor(0.4798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[618/1442]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[619/1442]: training_loss: tensor(0.4265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[620/1442]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[621/1442]: training_loss: tensor(0.4265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[622/1442]: training_loss: tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[623/1442]: training_loss: tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[624/1442]: training_loss: tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[625/1442]: training_loss: tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[626/1442]: training_loss: tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[627/1442]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[628/1442]: training_loss: tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[629/1442]: training_loss: tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[630/1442]: training_loss: tensor(0.5267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[631/1442]: training_loss: tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[632/1442]: training_loss: tensor(0.4611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[633/1442]: training_loss: tensor(0.4409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[634/1442]: training_loss: tensor(0.5255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[635/1442]: training_loss: tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[636/1442]: training_loss: tensor(0.6205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[637/1442]: training_loss: tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[638/1442]: training_loss: tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[639/1442]: training_loss: tensor(0.4453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[640/1442]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[641/1442]: training_loss: tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[642/1442]: training_loss: tensor(0.3185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[643/1442]: training_loss: tensor(0.4344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[644/1442]: training_loss: tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[645/1442]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[646/1442]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[647/1442]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[648/1442]: training_loss: tensor(0.5837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[649/1442]: training_loss: tensor(0.5074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[650/1442]: training_loss: tensor(0.4716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[651/1442]: training_loss: tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[652/1442]: training_loss: tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[653/1442]: training_loss: tensor(0.9541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[654/1442]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[655/1442]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[656/1442]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[657/1442]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[658/1442]: training_loss: tensor(0.5537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[659/1442]: training_loss: tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[660/1442]: training_loss: tensor(0.4738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[661/1442]: training_loss: tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[662/1442]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[663/1442]: training_loss: tensor(1.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[664/1442]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[665/1442]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[666/1442]: training_loss: tensor(0.5212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[667/1442]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[668/1442]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[669/1442]: training_loss: tensor(0.8092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[670/1442]: training_loss: tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[671/1442]: training_loss: tensor(0.6111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[672/1442]: training_loss: tensor(0.4532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[673/1442]: training_loss: tensor(0.8002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[674/1442]: training_loss: tensor(0.3648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[675/1442]: training_loss: tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[676/1442]: training_loss: tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[677/1442]: training_loss: tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[678/1442]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[679/1442]: training_loss: tensor(0.5862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[680/1442]: training_loss: tensor(0.6828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[681/1442]: training_loss: tensor(0.3948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[682/1442]: training_loss: tensor(0.8245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[683/1442]: training_loss: tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[684/1442]: training_loss: tensor(0.4296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[685/1442]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[686/1442]: training_loss: tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[687/1442]: training_loss: tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[688/1442]: training_loss: tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[689/1442]: training_loss: tensor(0.5222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[690/1442]: training_loss: tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[691/1442]: training_loss: tensor(0.3560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[692/1442]: training_loss: tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[693/1442]: training_loss: tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[694/1442]: training_loss: tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[695/1442]: training_loss: tensor(0.7244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[696/1442]: training_loss: tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[697/1442]: training_loss: tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[698/1442]: training_loss: tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[699/1442]: training_loss: tensor(0.5328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[700/1442]: training_loss: tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[701/1442]: training_loss: tensor(0.4513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[702/1442]: training_loss: tensor(0.6685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[703/1442]: training_loss: tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[704/1442]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[705/1442]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[706/1442]: training_loss: tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[707/1442]: training_loss: tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[708/1442]: training_loss: tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[709/1442]: training_loss: tensor(0.5054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[710/1442]: training_loss: tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[711/1442]: training_loss: tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[712/1442]: training_loss: tensor(0.4447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[713/1442]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[714/1442]: training_loss: tensor(0.4768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[715/1442]: training_loss: tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[716/1442]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[717/1442]: training_loss: tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[718/1442]: training_loss: tensor(0.5424, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[719/1442]: training_loss: tensor(0.4093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[720/1442]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[721/1442]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[722/1442]: training_loss: tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[723/1442]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[724/1442]: training_loss: tensor(0.6148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[725/1442]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[726/1442]: training_loss: tensor(0.7341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[727/1442]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[728/1442]: training_loss: tensor(0.5891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[729/1442]: training_loss: tensor(0.7872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[730/1442]: training_loss: tensor(0.3500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[731/1442]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[732/1442]: training_loss: tensor(0.5217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[733/1442]: training_loss: tensor(0.9673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[734/1442]: training_loss: tensor(0.7060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[735/1442]: training_loss: tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[736/1442]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[737/1442]: training_loss: tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[738/1442]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[739/1442]: training_loss: tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[740/1442]: training_loss: tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[741/1442]: training_loss: tensor(0.4522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[742/1442]: training_loss: tensor(0.5444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[743/1442]: training_loss: tensor(0.7595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[744/1442]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[745/1442]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[746/1442]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[747/1442]: training_loss: tensor(0.4196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[748/1442]: training_loss: tensor(0.5768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[749/1442]: training_loss: tensor(0.2748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[750/1442]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[751/1442]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[752/1442]: training_loss: tensor(0.6377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[753/1442]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[754/1442]: training_loss: tensor(0.7236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[755/1442]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[756/1442]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[757/1442]: training_loss: tensor(0.4891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[758/1442]: training_loss: tensor(0.9210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[759/1442]: training_loss: tensor(0.7532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[760/1442]: training_loss: tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[761/1442]: training_loss: tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[762/1442]: training_loss: tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[763/1442]: training_loss: tensor(0.4767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[764/1442]: training_loss: tensor(0.4850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[765/1442]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[766/1442]: training_loss: tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[767/1442]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[768/1442]: training_loss: tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[769/1442]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[770/1442]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[771/1442]: training_loss: tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[772/1442]: training_loss: tensor(0.6338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[773/1442]: training_loss: tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[774/1442]: training_loss: tensor(0.4767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[775/1442]: training_loss: tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[776/1442]: training_loss: tensor(0.4169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[777/1442]: training_loss: tensor(0.3612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[778/1442]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[779/1442]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[780/1442]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[781/1442]: training_loss: tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[782/1442]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[783/1442]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[784/1442]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[785/1442]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[786/1442]: training_loss: tensor(0.4488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[787/1442]: training_loss: tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[788/1442]: training_loss: tensor(0.8931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[789/1442]: training_loss: tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[790/1442]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[791/1442]: training_loss: tensor(0.3531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[792/1442]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[793/1442]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[794/1442]: training_loss: tensor(0.8264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[795/1442]: training_loss: tensor(0.7188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[796/1442]: training_loss: tensor(0.4370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[797/1442]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[798/1442]: training_loss: tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[799/1442]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[800/1442]: training_loss: tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[801/1442]: training_loss: tensor(0.7943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[802/1442]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[803/1442]: training_loss: tensor(0.5263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[804/1442]: training_loss: tensor(0.6186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[805/1442]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[806/1442]: training_loss: tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[807/1442]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[808/1442]: training_loss: tensor(0.5845, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[809/1442]: training_loss: tensor(0.3304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[810/1442]: training_loss: tensor(0.6068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[811/1442]: training_loss: tensor(0.5041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[812/1442]: training_loss: tensor(0.8094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[813/1442]: training_loss: tensor(0.7284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[814/1442]: training_loss: tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[815/1442]: training_loss: tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[816/1442]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[817/1442]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[818/1442]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[819/1442]: training_loss: tensor(0.4835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[820/1442]: training_loss: tensor(0.5925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[821/1442]: training_loss: tensor(0.8067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[822/1442]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[823/1442]: training_loss: tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[824/1442]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[825/1442]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[826/1442]: training_loss: tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[827/1442]: training_loss: tensor(0.6042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[828/1442]: training_loss: tensor(0.5206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[829/1442]: training_loss: tensor(0.4438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[830/1442]: training_loss: tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[831/1442]: training_loss: tensor(0.5967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[832/1442]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[833/1442]: training_loss: tensor(0.4779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[834/1442]: training_loss: tensor(0.7870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[835/1442]: training_loss: tensor(0.4469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[836/1442]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[837/1442]: training_loss: tensor(0.3531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[838/1442]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[839/1442]: training_loss: tensor(0.5124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[840/1442]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[841/1442]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[842/1442]: training_loss: tensor(0.4396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[843/1442]: training_loss: tensor(0.4937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[844/1442]: training_loss: tensor(0.8230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[845/1442]: training_loss: tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[846/1442]: training_loss: tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[847/1442]: training_loss: tensor(0.4844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[848/1442]: training_loss: tensor(0.5371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[849/1442]: training_loss: tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[850/1442]: training_loss: tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[851/1442]: training_loss: tensor(0.4632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[852/1442]: training_loss: tensor(0.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[853/1442]: training_loss: tensor(0.4473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[854/1442]: training_loss: tensor(0.4628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[855/1442]: training_loss: tensor(0.5576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[856/1442]: training_loss: tensor(0.4695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[857/1442]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[858/1442]: training_loss: tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[859/1442]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[860/1442]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[861/1442]: training_loss: tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[862/1442]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[863/1442]: training_loss: tensor(0.4229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[864/1442]: training_loss: tensor(0.6362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[865/1442]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[866/1442]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[867/1442]: training_loss: tensor(0.2667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[868/1442]: training_loss: tensor(0.4484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[869/1442]: training_loss: tensor(0.7836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[870/1442]: training_loss: tensor(0.5806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[871/1442]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[872/1442]: training_loss: tensor(0.5314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[873/1442]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[874/1442]: training_loss: tensor(0.5232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[875/1442]: training_loss: tensor(0.2875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[876/1442]: training_loss: tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[877/1442]: training_loss: tensor(0.7417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[878/1442]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[879/1442]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[880/1442]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[881/1442]: training_loss: tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[882/1442]: training_loss: tensor(0.9512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[883/1442]: training_loss: tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[884/1442]: training_loss: tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[885/1442]: training_loss: tensor(0.4455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[886/1442]: training_loss: tensor(1.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[887/1442]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[888/1442]: training_loss: tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[889/1442]: training_loss: tensor(0.8298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[890/1442]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[891/1442]: training_loss: tensor(0.5041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[892/1442]: training_loss: tensor(0.5264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[893/1442]: training_loss: tensor(0.4977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[894/1442]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[895/1442]: training_loss: tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[896/1442]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[897/1442]: training_loss: tensor(0.4426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[898/1442]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[899/1442]: training_loss: tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[900/1442]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[901/1442]: training_loss: tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[902/1442]: training_loss: tensor(0.3951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[903/1442]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[904/1442]: training_loss: tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[905/1442]: training_loss: tensor(0.8365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[906/1442]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[907/1442]: training_loss: tensor(0.7623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[908/1442]: training_loss: tensor(0.3386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[909/1442]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[910/1442]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[911/1442]: training_loss: tensor(0.4872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[912/1442]: training_loss: tensor(0.4520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[913/1442]: training_loss: tensor(0.5956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[914/1442]: training_loss: tensor(0.4161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[915/1442]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[916/1442]: training_loss: tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[917/1442]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[918/1442]: training_loss: tensor(0.4252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[919/1442]: training_loss: tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[920/1442]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[921/1442]: training_loss: tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[922/1442]: training_loss: tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[923/1442]: training_loss: tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[924/1442]: training_loss: tensor(0.2838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[925/1442]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[926/1442]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[927/1442]: training_loss: tensor(0.5671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[928/1442]: training_loss: tensor(1.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[929/1442]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[930/1442]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[931/1442]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[932/1442]: training_loss: tensor(0.4574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[933/1442]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[934/1442]: training_loss: tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[935/1442]: training_loss: tensor(0.7749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[936/1442]: training_loss: tensor(0.8741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[937/1442]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[938/1442]: training_loss: tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[939/1442]: training_loss: tensor(0.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[940/1442]: training_loss: tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[941/1442]: training_loss: tensor(0.5402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[942/1442]: training_loss: tensor(0.8216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[943/1442]: training_loss: tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[944/1442]: training_loss: tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[945/1442]: training_loss: tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[946/1442]: training_loss: tensor(0.8224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[947/1442]: training_loss: tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[948/1442]: training_loss: tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[949/1442]: training_loss: tensor(0.4182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[950/1442]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[951/1442]: training_loss: tensor(0.4709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[952/1442]: training_loss: tensor(0.7780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[953/1442]: training_loss: tensor(0.8064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[954/1442]: training_loss: tensor(0.9573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[955/1442]: training_loss: tensor(0.5647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[956/1442]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[957/1442]: training_loss: tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[958/1442]: training_loss: tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[959/1442]: training_loss: tensor(0.3777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[960/1442]: training_loss: tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[961/1442]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[962/1442]: training_loss: tensor(0.2728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[963/1442]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[964/1442]: training_loss: tensor(0.6116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[965/1442]: training_loss: tensor(0.5769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[966/1442]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[967/1442]: training_loss: tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[968/1442]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[969/1442]: training_loss: tensor(0.4790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[970/1442]: training_loss: tensor(0.3500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[971/1442]: training_loss: tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[972/1442]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[973/1442]: training_loss: tensor(0.3622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[974/1442]: training_loss: tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[975/1442]: training_loss: tensor(0.4085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[976/1442]: training_loss: tensor(0.5165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[977/1442]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[978/1442]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[979/1442]: training_loss: tensor(0.4587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[980/1442]: training_loss: tensor(0.2637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[981/1442]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[982/1442]: training_loss: tensor(0.5206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[983/1442]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[984/1442]: training_loss: tensor(0.6118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[985/1442]: training_loss: tensor(0.5208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[986/1442]: training_loss: tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[987/1442]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[988/1442]: training_loss: tensor(0.9535, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[989/1442]: training_loss: tensor(0.4322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[990/1442]: training_loss: tensor(0.4209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[991/1442]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[992/1442]: training_loss: tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[993/1442]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[994/1442]: training_loss: tensor(1.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[995/1442]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[996/1442]: training_loss: tensor(0.5433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[997/1442]: training_loss: tensor(0.4541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[998/1442]: training_loss: tensor(0.6065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[999/1442]: training_loss: tensor(0.4199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1000/1442]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1001/1442]: training_loss: tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1002/1442]: training_loss: tensor(0.4336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1003/1442]: training_loss: tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1004/1442]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1005/1442]: training_loss: tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1006/1442]: training_loss: tensor(0.4581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1007/1442]: training_loss: tensor(0.3325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1008/1442]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1009/1442]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1010/1442]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1011/1442]: training_loss: tensor(0.4580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1012/1442]: training_loss: tensor(0.4991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1013/1442]: training_loss: tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1014/1442]: training_loss: tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1015/1442]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1016/1442]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1017/1442]: training_loss: tensor(0.4298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1018/1442]: training_loss: tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1019/1442]: training_loss: tensor(0.5018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1020/1442]: training_loss: tensor(0.4981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1021/1442]: training_loss: tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1022/1442]: training_loss: tensor(0.8489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1023/1442]: training_loss: tensor(0.6668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1024/1442]: training_loss: tensor(0.3721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1025/1442]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1026/1442]: training_loss: tensor(0.4624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1027/1442]: training_loss: tensor(0.4936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1028/1442]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1029/1442]: training_loss: tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1030/1442]: training_loss: tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1031/1442]: training_loss: tensor(0.2667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1032/1442]: training_loss: tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1033/1442]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1034/1442]: training_loss: tensor(0.7337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1035/1442]: training_loss: tensor(0.8642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1036/1442]: training_loss: tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1037/1442]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1038/1442]: training_loss: tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1039/1442]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1040/1442]: training_loss: tensor(0.3941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1041/1442]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1042/1442]: training_loss: tensor(0.4802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1043/1442]: training_loss: tensor(0.5113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1044/1442]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1045/1442]: training_loss: tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1046/1442]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1047/1442]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1048/1442]: training_loss: tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1049/1442]: training_loss: tensor(0.5400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1050/1442]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1051/1442]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1052/1442]: training_loss: tensor(0.4618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1053/1442]: training_loss: tensor(1.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1054/1442]: training_loss: tensor(0.8764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1055/1442]: training_loss: tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1056/1442]: training_loss: tensor(0.4960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1057/1442]: training_loss: tensor(0.4204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1058/1442]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1059/1442]: training_loss: tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1060/1442]: training_loss: tensor(0.5371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1061/1442]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1062/1442]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1063/1442]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1064/1442]: training_loss: tensor(0.5779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1065/1442]: training_loss: tensor(0.5103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1066/1442]: training_loss: tensor(0.4722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1067/1442]: training_loss: tensor(0.6077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1068/1442]: training_loss: tensor(0.4361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1069/1442]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1070/1442]: training_loss: tensor(0.4540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1071/1442]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1072/1442]: training_loss: tensor(0.4166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1073/1442]: training_loss: tensor(0.4345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1074/1442]: training_loss: tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1075/1442]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1076/1442]: training_loss: tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1077/1442]: training_loss: tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1078/1442]: training_loss: tensor(0.3721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1079/1442]: training_loss: tensor(0.6650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1080/1442]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1081/1442]: training_loss: tensor(0.5239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1082/1442]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1083/1442]: training_loss: tensor(0.5341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1084/1442]: training_loss: tensor(0.2790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1085/1442]: training_loss: tensor(0.9425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1086/1442]: training_loss: tensor(0.5273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1087/1442]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1088/1442]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1089/1442]: training_loss: tensor(0.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1090/1442]: training_loss: tensor(0.2610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1091/1442]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1092/1442]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1093/1442]: training_loss: tensor(0.7299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1094/1442]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1095/1442]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1096/1442]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1097/1442]: training_loss: tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1098/1442]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1099/1442]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1100/1442]: training_loss: tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1101/1442]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1102/1442]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1103/1442]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1104/1442]: training_loss: tensor(0.4514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1105/1442]: training_loss: tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1106/1442]: training_loss: tensor(0.4913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1107/1442]: training_loss: tensor(0.8641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1108/1442]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1109/1442]: training_loss: tensor(0.4546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1110/1442]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1111/1442]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1112/1442]: training_loss: tensor(0.8260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1113/1442]: training_loss: tensor(0.4717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1114/1442]: training_loss: tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1115/1442]: training_loss: tensor(0.8045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1116/1442]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1117/1442]: training_loss: tensor(0.6382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1118/1442]: training_loss: tensor(0.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1119/1442]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1120/1442]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1121/1442]: training_loss: tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1122/1442]: training_loss: tensor(0.4978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1123/1442]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1124/1442]: training_loss: tensor(0.3424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1125/1442]: training_loss: tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1126/1442]: training_loss: tensor(0.4826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1127/1442]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1128/1442]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1129/1442]: training_loss: tensor(0.5569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1130/1442]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1131/1442]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1132/1442]: training_loss: tensor(0.5007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1133/1442]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1134/1442]: training_loss: tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1135/1442]: training_loss: tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1136/1442]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1137/1442]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1138/1442]: training_loss: tensor(0.5532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1139/1442]: training_loss: tensor(0.5098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1140/1442]: training_loss: tensor(0.4385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1141/1442]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1142/1442]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1143/1442]: training_loss: tensor(0.2866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1144/1442]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1145/1442]: training_loss: tensor(0.4117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1146/1442]: training_loss: tensor(0.4306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1147/1442]: training_loss: tensor(0.4202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1148/1442]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1149/1442]: training_loss: tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1150/1442]: training_loss: tensor(0.4747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1151/1442]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1152/1442]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1153/1442]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1154/1442]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1155/1442]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1156/1442]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1157/1442]: training_loss: tensor(0.5733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1158/1442]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1159/1442]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1160/1442]: training_loss: tensor(0.5309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1161/1442]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1162/1442]: training_loss: tensor(0.5010, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1163/1442]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1164/1442]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1165/1442]: training_loss: tensor(0.4685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1166/1442]: training_loss: tensor(0.3635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1167/1442]: training_loss: tensor(0.7648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1168/1442]: training_loss: tensor(0.4723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1169/1442]: training_loss: tensor(0.5301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1170/1442]: training_loss: tensor(0.4344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1171/1442]: training_loss: tensor(0.7360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1172/1442]: training_loss: tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1173/1442]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1174/1442]: training_loss: tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1175/1442]: training_loss: tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1176/1442]: training_loss: tensor(0.4382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1177/1442]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1178/1442]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1179/1442]: training_loss: tensor(0.9221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1180/1442]: training_loss: tensor(0.4687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1181/1442]: training_loss: tensor(0.4649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1182/1442]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1183/1442]: training_loss: tensor(0.5569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1184/1442]: training_loss: tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1185/1442]: training_loss: tensor(0.4149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1186/1442]: training_loss: tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1187/1442]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1188/1442]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1189/1442]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1190/1442]: training_loss: tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1191/1442]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1192/1442]: training_loss: tensor(0.5453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1193/1442]: training_loss: tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1194/1442]: training_loss: tensor(0.5387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1195/1442]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1196/1442]: training_loss: tensor(0.4190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1197/1442]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1198/1442]: training_loss: tensor(0.6222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1199/1442]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1200/1442]: training_loss: tensor(0.5997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1201/1442]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1202/1442]: training_loss: tensor(0.3538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1203/1442]: training_loss: tensor(0.4738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1204/1442]: training_loss: tensor(0.2950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1205/1442]: training_loss: tensor(0.5620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1206/1442]: training_loss: tensor(0.4289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1207/1442]: training_loss: tensor(0.6105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1208/1442]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1209/1442]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1210/1442]: training_loss: tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1211/1442]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1212/1442]: training_loss: tensor(0.4348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1213/1442]: training_loss: tensor(0.6355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1214/1442]: training_loss: tensor(0.5251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1215/1442]: training_loss: tensor(0.4193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1216/1442]: training_loss: tensor(0.4694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1217/1442]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1218/1442]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1219/1442]: training_loss: tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1220/1442]: training_loss: tensor(0.4373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1221/1442]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1222/1442]: training_loss: tensor(0.3453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1223/1442]: training_loss: tensor(0.4848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1224/1442]: training_loss: tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1225/1442]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1226/1442]: training_loss: tensor(0.5231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1227/1442]: training_loss: tensor(0.7228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1228/1442]: training_loss: tensor(0.4406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1229/1442]: training_loss: tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1230/1442]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1231/1442]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1232/1442]: training_loss: tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1233/1442]: training_loss: tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1234/1442]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1235/1442]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1236/1442]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1237/1442]: training_loss: tensor(0.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1238/1442]: training_loss: tensor(0.4466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1239/1442]: training_loss: tensor(0.4978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1240/1442]: training_loss: tensor(0.7640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1241/1442]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1242/1442]: training_loss: tensor(0.3762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1243/1442]: training_loss: tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1244/1442]: training_loss: tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1245/1442]: training_loss: tensor(0.7589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1246/1442]: training_loss: tensor(0.4185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1247/1442]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1248/1442]: training_loss: tensor(0.3083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1249/1442]: training_loss: tensor(0.4729, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1250/1442]: training_loss: tensor(0.3463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1251/1442]: training_loss: tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1252/1442]: training_loss: tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1253/1442]: training_loss: tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1254/1442]: training_loss: tensor(0.6007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1255/1442]: training_loss: tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1256/1442]: training_loss: tensor(0.5230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1257/1442]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1258/1442]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1259/1442]: training_loss: tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1260/1442]: training_loss: tensor(0.7666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1261/1442]: training_loss: tensor(0.4798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1262/1442]: training_loss: tensor(0.3625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1263/1442]: training_loss: tensor(0.5682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1264/1442]: training_loss: tensor(0.4828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1265/1442]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1266/1442]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1267/1442]: training_loss: tensor(0.5445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1268/1442]: training_loss: tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1269/1442]: training_loss: tensor(0.8345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1270/1442]: training_loss: tensor(0.5266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1271/1442]: training_loss: tensor(0.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1272/1442]: training_loss: tensor(0.5113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1273/1442]: training_loss: tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1274/1442]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1275/1442]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1276/1442]: training_loss: tensor(1.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1277/1442]: training_loss: tensor(0.4938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1278/1442]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1279/1442]: training_loss: tensor(0.4416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1280/1442]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1281/1442]: training_loss: tensor(0.4412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1282/1442]: training_loss: tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1283/1442]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1284/1442]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1285/1442]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1286/1442]: training_loss: tensor(0.5086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1287/1442]: training_loss: tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1288/1442]: training_loss: tensor(0.5699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1289/1442]: training_loss: tensor(0.4117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1290/1442]: training_loss: tensor(0.2795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1291/1442]: training_loss: tensor(0.5096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1292/1442]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1293/1442]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1294/1442]: training_loss: tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1295/1442]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1296/1442]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1297/1442]: training_loss: tensor(0.8279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1298/1442]: training_loss: tensor(0.7419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1299/1442]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1300/1442]: training_loss: tensor(0.4951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1301/1442]: training_loss: tensor(0.4756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1302/1442]: training_loss: tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1303/1442]: training_loss: tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1304/1442]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1305/1442]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1306/1442]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1307/1442]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1308/1442]: training_loss: tensor(0.4622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1309/1442]: training_loss: tensor(0.5282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1310/1442]: training_loss: tensor(0.8174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1311/1442]: training_loss: tensor(0.7253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1312/1442]: training_loss: tensor(0.5195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1313/1442]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1314/1442]: training_loss: tensor(0.4806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1315/1442]: training_loss: tensor(1.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1316/1442]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1317/1442]: training_loss: tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1318/1442]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1319/1442]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1320/1442]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1321/1442]: training_loss: tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1322/1442]: training_loss: tensor(1.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1323/1442]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1324/1442]: training_loss: tensor(0.5508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1325/1442]: training_loss: tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1326/1442]: training_loss: tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1327/1442]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1328/1442]: training_loss: tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1329/1442]: training_loss: tensor(0.5502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1330/1442]: training_loss: tensor(0.4299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1331/1442]: training_loss: tensor(0.4722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1332/1442]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1333/1442]: training_loss: tensor(0.6337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1334/1442]: training_loss: tensor(0.4137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1335/1442]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1336/1442]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1337/1442]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1338/1442]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1339/1442]: training_loss: tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1340/1442]: training_loss: tensor(0.5056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1341/1442]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1342/1442]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1343/1442]: training_loss: tensor(0.7889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1344/1442]: training_loss: tensor(0.5899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1345/1442]: training_loss: tensor(0.4282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1346/1442]: training_loss: tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1347/1442]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1348/1442]: training_loss: tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1349/1442]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1350/1442]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1351/1442]: training_loss: tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1352/1442]: training_loss: tensor(0.4941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1353/1442]: training_loss: tensor(0.8244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1354/1442]: training_loss: tensor(0.4775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1355/1442]: training_loss: tensor(0.4667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1356/1442]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1357/1442]: training_loss: tensor(0.4563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1358/1442]: training_loss: tensor(0.4120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1359/1442]: training_loss: tensor(0.3531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1360/1442]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1361/1442]: training_loss: tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1362/1442]: training_loss: tensor(0.4835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1363/1442]: training_loss: tensor(0.7706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1364/1442]: training_loss: tensor(0.5865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1365/1442]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1366/1442]: training_loss: tensor(0.4351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1367/1442]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1368/1442]: training_loss: tensor(0.6398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1369/1442]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1370/1442]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1371/1442]: training_loss: tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1372/1442]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1373/1442]: training_loss: tensor(0.8231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1374/1442]: training_loss: tensor(0.5186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1375/1442]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1376/1442]: training_loss: tensor(0.5113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1377/1442]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1378/1442]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1379/1442]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1380/1442]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1381/1442]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1382/1442]: training_loss: tensor(0.4586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1383/1442]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1384/1442]: training_loss: tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1385/1442]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1386/1442]: training_loss: tensor(0.5240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1387/1442]: training_loss: tensor(0.4935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1388/1442]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1389/1442]: training_loss: tensor(0.2230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1390/1442]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1391/1442]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1392/1442]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1393/1442]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1394/1442]: training_loss: tensor(0.4192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1395/1442]: training_loss: tensor(0.4981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1396/1442]: training_loss: tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1397/1442]: training_loss: tensor(0.4373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1398/1442]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1399/1442]: training_loss: tensor(0.5143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1400/1442]: training_loss: tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1401/1442]: training_loss: tensor(0.7642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1402/1442]: training_loss: tensor(0.4846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1403/1442]: training_loss: tensor(0.3510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1404/1442]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1405/1442]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1406/1442]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1407/1442]: training_loss: tensor(0.4057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1408/1442]: training_loss: tensor(0.5063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1409/1442]: training_loss: tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1410/1442]: training_loss: tensor(0.5429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1411/1442]: training_loss: tensor(0.5952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1412/1442]: training_loss: tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1413/1442]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1414/1442]: training_loss: tensor(0.9042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1415/1442]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1416/1442]: training_loss: tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1417/1442]: training_loss: tensor(0.4244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1418/1442]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1419/1442]: training_loss: tensor(0.6045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1420/1442]: training_loss: tensor(0.4911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1421/1442]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1422/1442]: training_loss: tensor(0.4300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1423/1442]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1424/1442]: training_loss: tensor(0.4624, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1425/1442]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1426/1442]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1427/1442]: training_loss: tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1428/1442]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1429/1442]: training_loss: tensor(0.4197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1430/1442]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1431/1442]: training_loss: tensor(0.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1432/1442]: training_loss: tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1433/1442]: training_loss: tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1434/1442]: training_loss: tensor(0.4922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1435/1442]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1436/1442]: training_loss: tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1437/1442]: training_loss: tensor(0.4684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1438/1442]: training_loss: tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1439/1442]: training_loss: tensor(0.9675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1440/1442]: training_loss: tensor(1.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1441/1442]: training_loss: tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/3],global step [1442/4326], pt loss:0.0000, dev loss:0.5542\n",
      "batch_no[0/1442]: training_loss: tensor(0.4707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1/1442]: training_loss: tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[2/1442]: training_loss: tensor(0.4415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[3/1442]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[4/1442]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[5/1442]: training_loss: tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[6/1442]: training_loss: tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[7/1442]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[8/1442]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[9/1442]: training_loss: tensor(0.5172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[10/1442]: training_loss: tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[11/1442]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[12/1442]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[13/1442]: training_loss: tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[14/1442]: training_loss: tensor(0.4613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[15/1442]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[16/1442]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[17/1442]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[18/1442]: training_loss: tensor(0.9949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[19/1442]: training_loss: tensor(0.5673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[20/1442]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[21/1442]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[22/1442]: training_loss: tensor(0.4281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[23/1442]: training_loss: tensor(0.4256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[24/1442]: training_loss: tensor(1.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[25/1442]: training_loss: tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[26/1442]: training_loss: tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[27/1442]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[28/1442]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[29/1442]: training_loss: tensor(0.4942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[30/1442]: training_loss: tensor(1.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[31/1442]: training_loss: tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[32/1442]: training_loss: tensor(0.4691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[33/1442]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[34/1442]: training_loss: tensor(0.4486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[35/1442]: training_loss: tensor(0.8344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[36/1442]: training_loss: tensor(0.4747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[37/1442]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[38/1442]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[39/1442]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[40/1442]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[41/1442]: training_loss: tensor(0.7045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[42/1442]: training_loss: tensor(0.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[43/1442]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[44/1442]: training_loss: tensor(0.3621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[45/1442]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[46/1442]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[47/1442]: training_loss: tensor(0.4442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[48/1442]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[49/1442]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[50/1442]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[51/1442]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[52/1442]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[53/1442]: training_loss: tensor(0.4554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[54/1442]: training_loss: tensor(0.4519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[55/1442]: training_loss: tensor(0.4099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[56/1442]: training_loss: tensor(1.3479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[57/1442]: training_loss: tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[58/1442]: training_loss: tensor(0.8814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[59/1442]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[60/1442]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[61/1442]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[62/1442]: training_loss: tensor(0.8241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[63/1442]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[64/1442]: training_loss: tensor(0.2630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[65/1442]: training_loss: tensor(0.4855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[66/1442]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[67/1442]: training_loss: tensor(0.6009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[68/1442]: training_loss: tensor(0.4422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[69/1442]: training_loss: tensor(0.4928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[70/1442]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[71/1442]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[72/1442]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[73/1442]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[74/1442]: training_loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[75/1442]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[76/1442]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[77/1442]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[78/1442]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[79/1442]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[80/1442]: training_loss: tensor(0.5480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[81/1442]: training_loss: tensor(0.7142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[82/1442]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[83/1442]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[84/1442]: training_loss: tensor(0.4229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[85/1442]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[86/1442]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[87/1442]: training_loss: tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[88/1442]: training_loss: tensor(0.8168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[89/1442]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[90/1442]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[91/1442]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[92/1442]: training_loss: tensor(0.4145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[93/1442]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[94/1442]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[95/1442]: training_loss: tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[96/1442]: training_loss: tensor(0.4415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[97/1442]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[98/1442]: training_loss: tensor(0.2726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[99/1442]: training_loss: tensor(0.6015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[100/1442]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[101/1442]: training_loss: tensor(0.8646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[102/1442]: training_loss: tensor(0.1534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[103/1442]: training_loss: tensor(0.5424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[104/1442]: training_loss: tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[105/1442]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[106/1442]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[107/1442]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[108/1442]: training_loss: tensor(0.2951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[109/1442]: training_loss: tensor(0.5435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[110/1442]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[111/1442]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[112/1442]: training_loss: tensor(0.5999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[113/1442]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[114/1442]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[115/1442]: training_loss: tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[116/1442]: training_loss: tensor(0.5738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[117/1442]: training_loss: tensor(0.3314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[118/1442]: training_loss: tensor(0.4969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[119/1442]: training_loss: tensor(0.5053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[120/1442]: training_loss: tensor(0.5834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[121/1442]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[122/1442]: training_loss: tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[123/1442]: training_loss: tensor(0.4146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[124/1442]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[125/1442]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[126/1442]: training_loss: tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[127/1442]: training_loss: tensor(0.5024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[128/1442]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[129/1442]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[130/1442]: training_loss: tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[131/1442]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[132/1442]: training_loss: tensor(1.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[133/1442]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[134/1442]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[135/1442]: training_loss: tensor(0.9398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[136/1442]: training_loss: tensor(1.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[137/1442]: training_loss: tensor(0.6480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[138/1442]: training_loss: tensor(0.6604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[139/1442]: training_loss: tensor(0.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[140/1442]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[141/1442]: training_loss: tensor(0.4947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[142/1442]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[143/1442]: training_loss: tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[144/1442]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[145/1442]: training_loss: tensor(0.4247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[146/1442]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[147/1442]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[148/1442]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[149/1442]: training_loss: tensor(0.4994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[150/1442]: training_loss: tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[151/1442]: training_loss: tensor(0.6139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[152/1442]: training_loss: tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[153/1442]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[154/1442]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[155/1442]: training_loss: tensor(0.4081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[156/1442]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[157/1442]: training_loss: tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[158/1442]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[159/1442]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[160/1442]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[161/1442]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[162/1442]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[163/1442]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[164/1442]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[165/1442]: training_loss: tensor(0.3412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[166/1442]: training_loss: tensor(0.4860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[167/1442]: training_loss: tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[168/1442]: training_loss: tensor(0.5008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[169/1442]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[170/1442]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[171/1442]: training_loss: tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[172/1442]: training_loss: tensor(0.2795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[173/1442]: training_loss: tensor(0.4794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[174/1442]: training_loss: tensor(0.4503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[175/1442]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[176/1442]: training_loss: tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[177/1442]: training_loss: tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[178/1442]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[179/1442]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[180/1442]: training_loss: tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[181/1442]: training_loss: tensor(0.4891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[182/1442]: training_loss: tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[183/1442]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[184/1442]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[185/1442]: training_loss: tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[186/1442]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[187/1442]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[188/1442]: training_loss: tensor(0.4842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[189/1442]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[190/1442]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[191/1442]: training_loss: tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[192/1442]: training_loss: tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[193/1442]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[194/1442]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[195/1442]: training_loss: tensor(0.2810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[196/1442]: training_loss: tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[197/1442]: training_loss: tensor(0.8795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[198/1442]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[199/1442]: training_loss: tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[200/1442]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[201/1442]: training_loss: tensor(0.5304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[202/1442]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[203/1442]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[204/1442]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[205/1442]: training_loss: tensor(0.3406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[206/1442]: training_loss: tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[207/1442]: training_loss: tensor(0.3499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[208/1442]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[209/1442]: training_loss: tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[210/1442]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[211/1442]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[212/1442]: training_loss: tensor(0.4205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[213/1442]: training_loss: tensor(0.2834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[214/1442]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[215/1442]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[216/1442]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[217/1442]: training_loss: tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[218/1442]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[219/1442]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[220/1442]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[221/1442]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[222/1442]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[223/1442]: training_loss: tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[224/1442]: training_loss: tensor(0.3234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[225/1442]: training_loss: tensor(1.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[226/1442]: training_loss: tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[227/1442]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[228/1442]: training_loss: tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[229/1442]: training_loss: tensor(0.6720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[230/1442]: training_loss: tensor(0.4339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[231/1442]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[232/1442]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[233/1442]: training_loss: tensor(0.9047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[234/1442]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[235/1442]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[236/1442]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[237/1442]: training_loss: tensor(0.5459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[238/1442]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[239/1442]: training_loss: tensor(0.4831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[240/1442]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[241/1442]: training_loss: tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[242/1442]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[243/1442]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[244/1442]: training_loss: tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[245/1442]: training_loss: tensor(0.5908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[246/1442]: training_loss: tensor(0.4495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[247/1442]: training_loss: tensor(0.4281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[248/1442]: training_loss: tensor(0.5009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[249/1442]: training_loss: tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[250/1442]: training_loss: tensor(0.5436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[251/1442]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[252/1442]: training_loss: tensor(0.5892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[253/1442]: training_loss: tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[254/1442]: training_loss: tensor(0.4295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[255/1442]: training_loss: tensor(0.4464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[256/1442]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[257/1442]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[258/1442]: training_loss: tensor(0.4695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[259/1442]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[260/1442]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[261/1442]: training_loss: tensor(0.3484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[262/1442]: training_loss: tensor(0.5790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[263/1442]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[264/1442]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[265/1442]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[266/1442]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[267/1442]: training_loss: tensor(0.7556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[268/1442]: training_loss: tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[269/1442]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[270/1442]: training_loss: tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[271/1442]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[272/1442]: training_loss: tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[273/1442]: training_loss: tensor(0.5163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[274/1442]: training_loss: tensor(0.4155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[275/1442]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[276/1442]: training_loss: tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[277/1442]: training_loss: tensor(0.8571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[278/1442]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[279/1442]: training_loss: tensor(0.5639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[280/1442]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[281/1442]: training_loss: tensor(0.4386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[282/1442]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[283/1442]: training_loss: tensor(0.3610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[284/1442]: training_loss: tensor(0.4172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[285/1442]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[286/1442]: training_loss: tensor(0.3656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[287/1442]: training_loss: tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[288/1442]: training_loss: tensor(0.5244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[289/1442]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[290/1442]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[291/1442]: training_loss: tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[292/1442]: training_loss: tensor(0.9985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[293/1442]: training_loss: tensor(0.4508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[294/1442]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[295/1442]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[296/1442]: training_loss: tensor(0.5997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[297/1442]: training_loss: tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[298/1442]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[299/1442]: training_loss: tensor(0.4847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[300/1442]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[301/1442]: training_loss: tensor(0.5248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[302/1442]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[303/1442]: training_loss: tensor(0.1829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[304/1442]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[305/1442]: training_loss: tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[306/1442]: training_loss: tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[307/1442]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[308/1442]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[309/1442]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[310/1442]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[311/1442]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[312/1442]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[313/1442]: training_loss: tensor(0.4996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[314/1442]: training_loss: tensor(0.4360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[315/1442]: training_loss: tensor(0.4964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[316/1442]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[317/1442]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[318/1442]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[319/1442]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[320/1442]: training_loss: tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[321/1442]: training_loss: tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[322/1442]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[323/1442]: training_loss: tensor(1.5047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[324/1442]: training_loss: tensor(1.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[325/1442]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[326/1442]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[327/1442]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[328/1442]: training_loss: tensor(0.4240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[329/1442]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[330/1442]: training_loss: tensor(1.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[331/1442]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[332/1442]: training_loss: tensor(0.3246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[333/1442]: training_loss: tensor(0.3242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[334/1442]: training_loss: tensor(0.4611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[335/1442]: training_loss: tensor(0.4226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[336/1442]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[337/1442]: training_loss: tensor(0.4700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[338/1442]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[339/1442]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[340/1442]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[341/1442]: training_loss: tensor(0.7701, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[342/1442]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[343/1442]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[344/1442]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[345/1442]: training_loss: tensor(0.4892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[346/1442]: training_loss: tensor(1.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[347/1442]: training_loss: tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[348/1442]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[349/1442]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[350/1442]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[351/1442]: training_loss: tensor(0.3433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[352/1442]: training_loss: tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[353/1442]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[354/1442]: training_loss: tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[355/1442]: training_loss: tensor(0.4652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[356/1442]: training_loss: tensor(0.5683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[357/1442]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[358/1442]: training_loss: tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[359/1442]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[360/1442]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[361/1442]: training_loss: tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[362/1442]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[363/1442]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[364/1442]: training_loss: tensor(0.4436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[365/1442]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[366/1442]: training_loss: tensor(0.4429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[367/1442]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[368/1442]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[369/1442]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[370/1442]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[371/1442]: training_loss: tensor(0.5917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[372/1442]: training_loss: tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[373/1442]: training_loss: tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[374/1442]: training_loss: tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[375/1442]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[376/1442]: training_loss: tensor(0.5304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[377/1442]: training_loss: tensor(0.8789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[378/1442]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[379/1442]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[380/1442]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[381/1442]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[382/1442]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[383/1442]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[384/1442]: training_loss: tensor(0.4185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[385/1442]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[386/1442]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[387/1442]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[388/1442]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[389/1442]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[390/1442]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[391/1442]: training_loss: tensor(0.5525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[392/1442]: training_loss: tensor(0.9193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[393/1442]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[394/1442]: training_loss: tensor(0.4473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[395/1442]: training_loss: tensor(0.4797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[396/1442]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[397/1442]: training_loss: tensor(0.4338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[398/1442]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[399/1442]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[400/1442]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[401/1442]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[402/1442]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[403/1442]: training_loss: tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[404/1442]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[405/1442]: training_loss: tensor(0.4731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[406/1442]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[407/1442]: training_loss: tensor(0.2937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[408/1442]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[409/1442]: training_loss: tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[410/1442]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[411/1442]: training_loss: tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[412/1442]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[413/1442]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[414/1442]: training_loss: tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[415/1442]: training_loss: tensor(0.6793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[416/1442]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[417/1442]: training_loss: tensor(0.9647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[418/1442]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[419/1442]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[420/1442]: training_loss: tensor(0.4818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[421/1442]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[422/1442]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[423/1442]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[424/1442]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[425/1442]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[426/1442]: training_loss: tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[427/1442]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[428/1442]: training_loss: tensor(0.4468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[429/1442]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[430/1442]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[431/1442]: training_loss: tensor(0.6141, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[432/1442]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[433/1442]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[434/1442]: training_loss: tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[435/1442]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[436/1442]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[437/1442]: training_loss: tensor(0.7573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[438/1442]: training_loss: tensor(0.3222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[439/1442]: training_loss: tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[440/1442]: training_loss: tensor(0.4886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[441/1442]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[442/1442]: training_loss: tensor(0.4797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[443/1442]: training_loss: tensor(0.7386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[444/1442]: training_loss: tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[445/1442]: training_loss: tensor(0.2684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[446/1442]: training_loss: tensor(0.4835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[447/1442]: training_loss: tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[448/1442]: training_loss: tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[449/1442]: training_loss: tensor(1.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[450/1442]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[451/1442]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[452/1442]: training_loss: tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[453/1442]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[454/1442]: training_loss: tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[455/1442]: training_loss: tensor(0.8470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[456/1442]: training_loss: tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[457/1442]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[458/1442]: training_loss: tensor(0.5055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[459/1442]: training_loss: tensor(0.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[460/1442]: training_loss: tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[461/1442]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[462/1442]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[463/1442]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[464/1442]: training_loss: tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[465/1442]: training_loss: tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[466/1442]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[467/1442]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[468/1442]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[469/1442]: training_loss: tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[470/1442]: training_loss: tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[471/1442]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[472/1442]: training_loss: tensor(1.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[473/1442]: training_loss: tensor(0.2104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[474/1442]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[475/1442]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[476/1442]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[477/1442]: training_loss: tensor(0.5628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[478/1442]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[479/1442]: training_loss: tensor(1.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[480/1442]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[481/1442]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[482/1442]: training_loss: tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[483/1442]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[484/1442]: training_loss: tensor(0.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[485/1442]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[486/1442]: training_loss: tensor(0.8131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[487/1442]: training_loss: tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[488/1442]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[489/1442]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[490/1442]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[491/1442]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[492/1442]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[493/1442]: training_loss: tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[494/1442]: training_loss: tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[495/1442]: training_loss: tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[496/1442]: training_loss: tensor(0.4055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[497/1442]: training_loss: tensor(0.4771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[498/1442]: training_loss: tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[499/1442]: training_loss: tensor(0.3488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[500/1442]: training_loss: tensor(0.4270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[501/1442]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[502/1442]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[503/1442]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[504/1442]: training_loss: tensor(0.5189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[505/1442]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[506/1442]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[507/1442]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[508/1442]: training_loss: tensor(0.5209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[509/1442]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[510/1442]: training_loss: tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[511/1442]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[512/1442]: training_loss: tensor(0.4604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[513/1442]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[514/1442]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[515/1442]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[516/1442]: training_loss: tensor(0.9041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[517/1442]: training_loss: tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[518/1442]: training_loss: tensor(0.4310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[519/1442]: training_loss: tensor(0.5748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[520/1442]: training_loss: tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[521/1442]: training_loss: tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[522/1442]: training_loss: tensor(0.8127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[523/1442]: training_loss: tensor(0.7607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[524/1442]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[525/1442]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[526/1442]: training_loss: tensor(0.4160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[527/1442]: training_loss: tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[528/1442]: training_loss: tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[529/1442]: training_loss: tensor(0.5473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[530/1442]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[531/1442]: training_loss: tensor(0.5007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[532/1442]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[533/1442]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[534/1442]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[535/1442]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[536/1442]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[537/1442]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[538/1442]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[539/1442]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[540/1442]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[541/1442]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[542/1442]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[543/1442]: training_loss: tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[544/1442]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[545/1442]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[546/1442]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[547/1442]: training_loss: tensor(0.4924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[548/1442]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[549/1442]: training_loss: tensor(0.5001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[550/1442]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[551/1442]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[552/1442]: training_loss: tensor(0.3724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[553/1442]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[554/1442]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[555/1442]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[556/1442]: training_loss: tensor(0.9200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[557/1442]: training_loss: tensor(0.4838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[558/1442]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[559/1442]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[560/1442]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[561/1442]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[562/1442]: training_loss: tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[563/1442]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[564/1442]: training_loss: tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[565/1442]: training_loss: tensor(0.4808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[566/1442]: training_loss: tensor(0.5888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[567/1442]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[568/1442]: training_loss: tensor(0.3624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[569/1442]: training_loss: tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[570/1442]: training_loss: tensor(0.2802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[571/1442]: training_loss: tensor(0.2857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[572/1442]: training_loss: tensor(0.3751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[573/1442]: training_loss: tensor(0.4713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[574/1442]: training_loss: tensor(0.4036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[575/1442]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[576/1442]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[577/1442]: training_loss: tensor(0.4194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[578/1442]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[579/1442]: training_loss: tensor(0.4153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[580/1442]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[581/1442]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[582/1442]: training_loss: tensor(0.8408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[583/1442]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[584/1442]: training_loss: tensor(0.4138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[585/1442]: training_loss: tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[586/1442]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[587/1442]: training_loss: tensor(0.3489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[588/1442]: training_loss: tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[589/1442]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[590/1442]: training_loss: tensor(0.5243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[591/1442]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[592/1442]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[593/1442]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[594/1442]: training_loss: tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[595/1442]: training_loss: tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[596/1442]: training_loss: tensor(0.4100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[597/1442]: training_loss: tensor(1.1622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[598/1442]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[599/1442]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[600/1442]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[601/1442]: training_loss: tensor(0.5976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[602/1442]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[603/1442]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[604/1442]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[605/1442]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[606/1442]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[607/1442]: training_loss: tensor(0.3479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[608/1442]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[609/1442]: training_loss: tensor(0.3369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[610/1442]: training_loss: tensor(0.5145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[611/1442]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[612/1442]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[613/1442]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[614/1442]: training_loss: tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[615/1442]: training_loss: tensor(0.5313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[616/1442]: training_loss: tensor(0.3674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[617/1442]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[618/1442]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[619/1442]: training_loss: tensor(0.3134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[620/1442]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[621/1442]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[622/1442]: training_loss: tensor(0.6728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[623/1442]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[624/1442]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[625/1442]: training_loss: tensor(0.3885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[626/1442]: training_loss: tensor(0.4561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[627/1442]: training_loss: tensor(0.4784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[628/1442]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[629/1442]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[630/1442]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[631/1442]: training_loss: tensor(0.7758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[632/1442]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[633/1442]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[634/1442]: training_loss: tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[635/1442]: training_loss: tensor(0.4596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[636/1442]: training_loss: tensor(0.4157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[637/1442]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[638/1442]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[639/1442]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[640/1442]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[641/1442]: training_loss: tensor(0.7886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[642/1442]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[643/1442]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[644/1442]: training_loss: tensor(0.4184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[645/1442]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[646/1442]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[647/1442]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[648/1442]: training_loss: tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[649/1442]: training_loss: tensor(0.4534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[650/1442]: training_loss: tensor(0.4194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[651/1442]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[652/1442]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[653/1442]: training_loss: tensor(0.5149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[654/1442]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[655/1442]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[656/1442]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[657/1442]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[658/1442]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[659/1442]: training_loss: tensor(0.6598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[660/1442]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[661/1442]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[662/1442]: training_loss: tensor(0.3403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[663/1442]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[664/1442]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[665/1442]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[666/1442]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[667/1442]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[668/1442]: training_loss: tensor(0.4508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[669/1442]: training_loss: tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[670/1442]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[671/1442]: training_loss: tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[672/1442]: training_loss: tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[673/1442]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[674/1442]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[675/1442]: training_loss: tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[676/1442]: training_loss: tensor(0.4722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[677/1442]: training_loss: tensor(0.4984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[678/1442]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[679/1442]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[680/1442]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[681/1442]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[682/1442]: training_loss: tensor(1.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[683/1442]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[684/1442]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[685/1442]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[686/1442]: training_loss: tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[687/1442]: training_loss: tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[688/1442]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[689/1442]: training_loss: tensor(0.4577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[690/1442]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[691/1442]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[692/1442]: training_loss: tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[693/1442]: training_loss: tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[694/1442]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[695/1442]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[696/1442]: training_loss: tensor(0.5782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[697/1442]: training_loss: tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[698/1442]: training_loss: tensor(0.5596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[699/1442]: training_loss: tensor(0.3242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[700/1442]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[701/1442]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[702/1442]: training_loss: tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[703/1442]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[704/1442]: training_loss: tensor(0.5854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[705/1442]: training_loss: tensor(0.3290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[706/1442]: training_loss: tensor(0.2749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[707/1442]: training_loss: tensor(0.4349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[708/1442]: training_loss: tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[709/1442]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[710/1442]: training_loss: tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[711/1442]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[712/1442]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[713/1442]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[714/1442]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[715/1442]: training_loss: tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[716/1442]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[717/1442]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[718/1442]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[719/1442]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[720/1442]: training_loss: tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[721/1442]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[722/1442]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[723/1442]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[724/1442]: training_loss: tensor(0.4746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[725/1442]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[726/1442]: training_loss: tensor(0.9425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[727/1442]: training_loss: tensor(0.4503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[728/1442]: training_loss: tensor(0.7832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[729/1442]: training_loss: tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[730/1442]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[731/1442]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[732/1442]: training_loss: tensor(0.5317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[733/1442]: training_loss: tensor(0.5072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[734/1442]: training_loss: tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[735/1442]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[736/1442]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[737/1442]: training_loss: tensor(0.7854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[738/1442]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[739/1442]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[740/1442]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[741/1442]: training_loss: tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[742/1442]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[743/1442]: training_loss: tensor(0.4573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[744/1442]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[745/1442]: training_loss: tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[746/1442]: training_loss: tensor(0.5232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[747/1442]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[748/1442]: training_loss: tensor(0.3689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[749/1442]: training_loss: tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[750/1442]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[751/1442]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[752/1442]: training_loss: tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[753/1442]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[754/1442]: training_loss: tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[755/1442]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[756/1442]: training_loss: tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[757/1442]: training_loss: tensor(0.2766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[758/1442]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[759/1442]: training_loss: tensor(0.4652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[760/1442]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[761/1442]: training_loss: tensor(0.3540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[762/1442]: training_loss: tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[763/1442]: training_loss: tensor(0.6485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[764/1442]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[765/1442]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[766/1442]: training_loss: tensor(0.7092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[767/1442]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[768/1442]: training_loss: tensor(0.4248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[769/1442]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[770/1442]: training_loss: tensor(0.1534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[771/1442]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[772/1442]: training_loss: tensor(0.6531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[773/1442]: training_loss: tensor(0.6045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[774/1442]: training_loss: tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[775/1442]: training_loss: tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[776/1442]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[777/1442]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[778/1442]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[779/1442]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[780/1442]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[781/1442]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[782/1442]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[783/1442]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[784/1442]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[785/1442]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[786/1442]: training_loss: tensor(0.2887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[787/1442]: training_loss: tensor(0.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[788/1442]: training_loss: tensor(0.4507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[789/1442]: training_loss: tensor(0.4818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[790/1442]: training_loss: tensor(0.5760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[791/1442]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[792/1442]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[793/1442]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[794/1442]: training_loss: tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[795/1442]: training_loss: tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[796/1442]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[797/1442]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[798/1442]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[799/1442]: training_loss: tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[800/1442]: training_loss: tensor(0.2687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[801/1442]: training_loss: tensor(0.8184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[802/1442]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[803/1442]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[804/1442]: training_loss: tensor(0.4713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[805/1442]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[806/1442]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[807/1442]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[808/1442]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[809/1442]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[810/1442]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[811/1442]: training_loss: tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[812/1442]: training_loss: tensor(0.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[813/1442]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[814/1442]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[815/1442]: training_loss: tensor(0.7546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[816/1442]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[817/1442]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[818/1442]: training_loss: tensor(0.5540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[819/1442]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[820/1442]: training_loss: tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[821/1442]: training_loss: tensor(0.4099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[822/1442]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[823/1442]: training_loss: tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[824/1442]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[825/1442]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[826/1442]: training_loss: tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[827/1442]: training_loss: tensor(0.3239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[828/1442]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[829/1442]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[830/1442]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[831/1442]: training_loss: tensor(0.4153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[832/1442]: training_loss: tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[833/1442]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[834/1442]: training_loss: tensor(0.7993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[835/1442]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[836/1442]: training_loss: tensor(0.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[837/1442]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[838/1442]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[839/1442]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[840/1442]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[841/1442]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[842/1442]: training_loss: tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[843/1442]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[844/1442]: training_loss: tensor(0.5348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[845/1442]: training_loss: tensor(0.5115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[846/1442]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[847/1442]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[848/1442]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[849/1442]: training_loss: tensor(0.4467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[850/1442]: training_loss: tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[851/1442]: training_loss: tensor(0.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[852/1442]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[853/1442]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[854/1442]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[855/1442]: training_loss: tensor(0.5763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[856/1442]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[857/1442]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[858/1442]: training_loss: tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[859/1442]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[860/1442]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[861/1442]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[862/1442]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[863/1442]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[864/1442]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[865/1442]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[866/1442]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[867/1442]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[868/1442]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[869/1442]: training_loss: tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[870/1442]: training_loss: tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[871/1442]: training_loss: tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[872/1442]: training_loss: tensor(0.4472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[873/1442]: training_loss: tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[874/1442]: training_loss: tensor(0.4917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[875/1442]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[876/1442]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[877/1442]: training_loss: tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[878/1442]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[879/1442]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[880/1442]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[881/1442]: training_loss: tensor(0.8028, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[882/1442]: training_loss: tensor(0.5301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[883/1442]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[884/1442]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[885/1442]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[886/1442]: training_loss: tensor(0.8128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[887/1442]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[888/1442]: training_loss: tensor(0.6014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[889/1442]: training_loss: tensor(0.9666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[890/1442]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[891/1442]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[892/1442]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[893/1442]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[894/1442]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[895/1442]: training_loss: tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[896/1442]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[897/1442]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[898/1442]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[899/1442]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[900/1442]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[901/1442]: training_loss: tensor(0.4172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[902/1442]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[903/1442]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[904/1442]: training_loss: tensor(0.6684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[905/1442]: training_loss: tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[906/1442]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[907/1442]: training_loss: tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[908/1442]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[909/1442]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[910/1442]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[911/1442]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[912/1442]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[913/1442]: training_loss: tensor(0.6360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[914/1442]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[915/1442]: training_loss: tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[916/1442]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[917/1442]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[918/1442]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[919/1442]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[920/1442]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[921/1442]: training_loss: tensor(0.5031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[922/1442]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[923/1442]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[924/1442]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[925/1442]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[926/1442]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[927/1442]: training_loss: tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[928/1442]: training_loss: tensor(0.7588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[929/1442]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[930/1442]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[931/1442]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[932/1442]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[933/1442]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[934/1442]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[935/1442]: training_loss: tensor(0.6998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[936/1442]: training_loss: tensor(0.6513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[937/1442]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[938/1442]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[939/1442]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[940/1442]: training_loss: tensor(0.5115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[941/1442]: training_loss: tensor(0.5058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[942/1442]: training_loss: tensor(0.5675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[943/1442]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[944/1442]: training_loss: tensor(0.5126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[945/1442]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[946/1442]: training_loss: tensor(0.9707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[947/1442]: training_loss: tensor(0.5254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[948/1442]: training_loss: tensor(0.4179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[949/1442]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[950/1442]: training_loss: tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[951/1442]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[952/1442]: training_loss: tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[953/1442]: training_loss: tensor(0.4163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[954/1442]: training_loss: tensor(0.7242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[955/1442]: training_loss: tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[956/1442]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[957/1442]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[958/1442]: training_loss: tensor(0.7217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[959/1442]: training_loss: tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[960/1442]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[961/1442]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[962/1442]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[963/1442]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[964/1442]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[965/1442]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[966/1442]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[967/1442]: training_loss: tensor(0.4977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[968/1442]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[969/1442]: training_loss: tensor(0.4184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[970/1442]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[971/1442]: training_loss: tensor(0.5784, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[972/1442]: training_loss: tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[973/1442]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[974/1442]: training_loss: tensor(0.4172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[975/1442]: training_loss: tensor(0.3707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[976/1442]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[977/1442]: training_loss: tensor(0.3250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[978/1442]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[979/1442]: training_loss: tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[980/1442]: training_loss: tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[981/1442]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[982/1442]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[983/1442]: training_loss: tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[984/1442]: training_loss: tensor(0.7256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[985/1442]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[986/1442]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[987/1442]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[988/1442]: training_loss: tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[989/1442]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[990/1442]: training_loss: tensor(0.4290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[991/1442]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[992/1442]: training_loss: tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[993/1442]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[994/1442]: training_loss: tensor(0.8702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[995/1442]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[996/1442]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[997/1442]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[998/1442]: training_loss: tensor(0.4714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[999/1442]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1000/1442]: training_loss: tensor(0.3083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1001/1442]: training_loss: tensor(0.4173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1002/1442]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1003/1442]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1004/1442]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1005/1442]: training_loss: tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1006/1442]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1007/1442]: training_loss: tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1008/1442]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1009/1442]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1010/1442]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1011/1442]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1012/1442]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1013/1442]: training_loss: tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1014/1442]: training_loss: tensor(0.2664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1015/1442]: training_loss: tensor(0.1602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1016/1442]: training_loss: tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1017/1442]: training_loss: tensor(0.4123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1018/1442]: training_loss: tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1019/1442]: training_loss: tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1020/1442]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1021/1442]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1022/1442]: training_loss: tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1023/1442]: training_loss: tensor(0.5360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1024/1442]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1025/1442]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1026/1442]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1027/1442]: training_loss: tensor(0.4084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1028/1442]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1029/1442]: training_loss: tensor(1.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1030/1442]: training_loss: tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1031/1442]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1032/1442]: training_loss: tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1033/1442]: training_loss: tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1034/1442]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1035/1442]: training_loss: tensor(0.9686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1036/1442]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1037/1442]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1038/1442]: training_loss: tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1039/1442]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1040/1442]: training_loss: tensor(0.2586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1041/1442]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1042/1442]: training_loss: tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1043/1442]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1044/1442]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1045/1442]: training_loss: tensor(0.7161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1046/1442]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1047/1442]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1048/1442]: training_loss: tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1049/1442]: training_loss: tensor(0.4275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1050/1442]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1051/1442]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1052/1442]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1053/1442]: training_loss: tensor(0.8708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1054/1442]: training_loss: tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1055/1442]: training_loss: tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1056/1442]: training_loss: tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1057/1442]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1058/1442]: training_loss: tensor(0.2939, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1059/1442]: training_loss: tensor(0.3320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1060/1442]: training_loss: tensor(0.5126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1061/1442]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1062/1442]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1063/1442]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1064/1442]: training_loss: tensor(0.4170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1065/1442]: training_loss: tensor(0.5299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1066/1442]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1067/1442]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1068/1442]: training_loss: tensor(0.5207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1069/1442]: training_loss: tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1070/1442]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1071/1442]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1072/1442]: training_loss: tensor(0.2667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1073/1442]: training_loss: tensor(0.2795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1074/1442]: training_loss: tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1075/1442]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1076/1442]: training_loss: tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1077/1442]: training_loss: tensor(0.5067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1078/1442]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1079/1442]: training_loss: tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1080/1442]: training_loss: tensor(0.3424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1081/1442]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1082/1442]: training_loss: tensor(0.4536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1083/1442]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1084/1442]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1085/1442]: training_loss: tensor(0.3088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1086/1442]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1087/1442]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1088/1442]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1089/1442]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1090/1442]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1091/1442]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1092/1442]: training_loss: tensor(0.3500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1093/1442]: training_loss: tensor(0.5280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1094/1442]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1095/1442]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1096/1442]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1097/1442]: training_loss: tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1098/1442]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1099/1442]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1100/1442]: training_loss: tensor(0.6279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1101/1442]: training_loss: tensor(0.2933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1102/1442]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1103/1442]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1104/1442]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1105/1442]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1106/1442]: training_loss: tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1107/1442]: training_loss: tensor(0.5235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1108/1442]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1109/1442]: training_loss: tensor(0.4519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1110/1442]: training_loss: tensor(0.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1111/1442]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1112/1442]: training_loss: tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1113/1442]: training_loss: tensor(0.6296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1114/1442]: training_loss: tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1115/1442]: training_loss: tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1116/1442]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1117/1442]: training_loss: tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1118/1442]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1119/1442]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1120/1442]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1121/1442]: training_loss: tensor(0.5172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1122/1442]: training_loss: tensor(0.3320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1123/1442]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1124/1442]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1125/1442]: training_loss: tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1126/1442]: training_loss: tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1127/1442]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1128/1442]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1129/1442]: training_loss: tensor(0.4923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1130/1442]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1131/1442]: training_loss: tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1132/1442]: training_loss: tensor(0.2637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1133/1442]: training_loss: tensor(0.2750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1134/1442]: training_loss: tensor(0.5053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1135/1442]: training_loss: tensor(0.5170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1136/1442]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1137/1442]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1138/1442]: training_loss: tensor(0.6038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1139/1442]: training_loss: tensor(0.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1140/1442]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1141/1442]: training_loss: tensor(0.5054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1142/1442]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1143/1442]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1144/1442]: training_loss: tensor(0.2041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1145/1442]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1146/1442]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1147/1442]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1148/1442]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1149/1442]: training_loss: tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1150/1442]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1151/1442]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1152/1442]: training_loss: tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1153/1442]: training_loss: tensor(0.4331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1154/1442]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1155/1442]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1156/1442]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1157/1442]: training_loss: tensor(0.8604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1158/1442]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1159/1442]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1160/1442]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1161/1442]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1162/1442]: training_loss: tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1163/1442]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1164/1442]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1165/1442]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1166/1442]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1167/1442]: training_loss: tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1168/1442]: training_loss: tensor(0.2887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1169/1442]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1170/1442]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1171/1442]: training_loss: tensor(0.8358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1172/1442]: training_loss: tensor(0.2616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1173/1442]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1174/1442]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1175/1442]: training_loss: tensor(0.8829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1176/1442]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1177/1442]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1178/1442]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1179/1442]: training_loss: tensor(0.8065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1180/1442]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1181/1442]: training_loss: tensor(0.5042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1182/1442]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1183/1442]: training_loss: tensor(0.5312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1184/1442]: training_loss: tensor(0.7155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1185/1442]: training_loss: tensor(0.5090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1186/1442]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1187/1442]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1188/1442]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1189/1442]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1190/1442]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1191/1442]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1192/1442]: training_loss: tensor(0.2866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1193/1442]: training_loss: tensor(0.4691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1194/1442]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1195/1442]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1196/1442]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1197/1442]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1198/1442]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1199/1442]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1200/1442]: training_loss: tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1201/1442]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1202/1442]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1203/1442]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1204/1442]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1205/1442]: training_loss: tensor(0.4468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1206/1442]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1207/1442]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1208/1442]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1209/1442]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1210/1442]: training_loss: tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1211/1442]: training_loss: tensor(0.5303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1212/1442]: training_loss: tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1213/1442]: training_loss: tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1214/1442]: training_loss: tensor(0.4210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1215/1442]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1216/1442]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1217/1442]: training_loss: tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1218/1442]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1219/1442]: training_loss: tensor(0.7448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1220/1442]: training_loss: tensor(0.4083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1221/1442]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1222/1442]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1223/1442]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1224/1442]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1225/1442]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1226/1442]: training_loss: tensor(0.4284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1227/1442]: training_loss: tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1228/1442]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1229/1442]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1230/1442]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1231/1442]: training_loss: tensor(0.3383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1232/1442]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1233/1442]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1234/1442]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1235/1442]: training_loss: tensor(0.5055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1236/1442]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1237/1442]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1238/1442]: training_loss: tensor(0.4268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1239/1442]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1240/1442]: training_loss: tensor(0.6008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1241/1442]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1242/1442]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1243/1442]: training_loss: tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1244/1442]: training_loss: tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1245/1442]: training_loss: tensor(0.4816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1246/1442]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1247/1442]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1248/1442]: training_loss: tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1249/1442]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1250/1442]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1251/1442]: training_loss: tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1252/1442]: training_loss: tensor(0.4396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1253/1442]: training_loss: tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1254/1442]: training_loss: tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1255/1442]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1256/1442]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1257/1442]: training_loss: tensor(0.2685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1258/1442]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1259/1442]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1260/1442]: training_loss: tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1261/1442]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1262/1442]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1263/1442]: training_loss: tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1264/1442]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1265/1442]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1266/1442]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1267/1442]: training_loss: tensor(0.3216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1268/1442]: training_loss: tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1269/1442]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1270/1442]: training_loss: tensor(0.2832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1271/1442]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1272/1442]: training_loss: tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1273/1442]: training_loss: tensor(0.4379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1274/1442]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1275/1442]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1276/1442]: training_loss: tensor(0.8914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1277/1442]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1278/1442]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1279/1442]: training_loss: tensor(0.5668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1280/1442]: training_loss: tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1281/1442]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1282/1442]: training_loss: tensor(0.4184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1283/1442]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1284/1442]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1285/1442]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1286/1442]: training_loss: tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1287/1442]: training_loss: tensor(0.5366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1288/1442]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1289/1442]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1290/1442]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1291/1442]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1292/1442]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1293/1442]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1294/1442]: training_loss: tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1295/1442]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1296/1442]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1297/1442]: training_loss: tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1298/1442]: training_loss: tensor(0.8322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1299/1442]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1300/1442]: training_loss: tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1301/1442]: training_loss: tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1302/1442]: training_loss: tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1303/1442]: training_loss: tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1304/1442]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1305/1442]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1306/1442]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1307/1442]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1308/1442]: training_loss: tensor(0.4684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1309/1442]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1310/1442]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1311/1442]: training_loss: tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1312/1442]: training_loss: tensor(0.4893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1313/1442]: training_loss: tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1314/1442]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1315/1442]: training_loss: tensor(1.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1316/1442]: training_loss: tensor(0.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1317/1442]: training_loss: tensor(0.4752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1318/1442]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1319/1442]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1320/1442]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1321/1442]: training_loss: tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1322/1442]: training_loss: tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1323/1442]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1324/1442]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1325/1442]: training_loss: tensor(0.3262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1326/1442]: training_loss: tensor(1.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1327/1442]: training_loss: tensor(0.2793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1328/1442]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1329/1442]: training_loss: tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1330/1442]: training_loss: tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1331/1442]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1332/1442]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1333/1442]: training_loss: tensor(0.4197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1334/1442]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1335/1442]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1336/1442]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1337/1442]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1338/1442]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1339/1442]: training_loss: tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1340/1442]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1341/1442]: training_loss: tensor(0.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1342/1442]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1343/1442]: training_loss: tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1344/1442]: training_loss: tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1345/1442]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1346/1442]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1347/1442]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1348/1442]: training_loss: tensor(0.4468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1349/1442]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1350/1442]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1351/1442]: training_loss: tensor(0.5942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1352/1442]: training_loss: tensor(0.4609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1353/1442]: training_loss: tensor(0.4798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1354/1442]: training_loss: tensor(0.4727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1355/1442]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1356/1442]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1357/1442]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1358/1442]: training_loss: tensor(0.3494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1359/1442]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1360/1442]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1361/1442]: training_loss: tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1362/1442]: training_loss: tensor(0.3590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1363/1442]: training_loss: tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1364/1442]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1365/1442]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1366/1442]: training_loss: tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1367/1442]: training_loss: tensor(0.3453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1368/1442]: training_loss: tensor(0.9777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1369/1442]: training_loss: tensor(0.2842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1370/1442]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1371/1442]: training_loss: tensor(0.4367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1372/1442]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1373/1442]: training_loss: tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1374/1442]: training_loss: tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1375/1442]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1376/1442]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1377/1442]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1378/1442]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1379/1442]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1380/1442]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1381/1442]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1382/1442]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1383/1442]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1384/1442]: training_loss: tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1385/1442]: training_loss: tensor(0.2785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1386/1442]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1387/1442]: training_loss: tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1388/1442]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1389/1442]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1390/1442]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1391/1442]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1392/1442]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1393/1442]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1394/1442]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1395/1442]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1396/1442]: training_loss: tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1397/1442]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1398/1442]: training_loss: tensor(0.4665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1399/1442]: training_loss: tensor(0.4773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1400/1442]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1401/1442]: training_loss: tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1402/1442]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1403/1442]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1404/1442]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1405/1442]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1406/1442]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1407/1442]: training_loss: tensor(0.5334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1408/1442]: training_loss: tensor(0.2783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1409/1442]: training_loss: tensor(0.7594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1410/1442]: training_loss: tensor(0.5081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1411/1442]: training_loss: tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1412/1442]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1413/1442]: training_loss: tensor(0.3670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1414/1442]: training_loss: tensor(0.7688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1415/1442]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1416/1442]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1417/1442]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1418/1442]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1419/1442]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1420/1442]: training_loss: tensor(0.2780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1421/1442]: training_loss: tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1422/1442]: training_loss: tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1423/1442]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1424/1442]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1425/1442]: training_loss: tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1426/1442]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1427/1442]: training_loss: tensor(0.7578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1428/1442]: training_loss: tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1429/1442]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1430/1442]: training_loss: tensor(0.4103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1431/1442]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1432/1442]: training_loss: tensor(0.4369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1433/1442]: training_loss: tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1434/1442]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1435/1442]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1436/1442]: training_loss: tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1437/1442]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1438/1442]: training_loss: tensor(0.4401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1439/1442]: training_loss: tensor(0.8620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1440/1442]: training_loss: tensor(0.8935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1441/1442]: training_loss: tensor(0.5059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [2/3],global step [2884/4326], pt loss:0.0000, dev loss:0.5753\n",
      "batch_no[0/1442]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1/1442]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[2/1442]: training_loss: tensor(0.5425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[3/1442]: training_loss: tensor(0.4597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[4/1442]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[5/1442]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[6/1442]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[7/1442]: training_loss: tensor(0.3276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[8/1442]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[9/1442]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[10/1442]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[11/1442]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[12/1442]: training_loss: tensor(0.2703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[13/1442]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[14/1442]: training_loss: tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[15/1442]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[16/1442]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[17/1442]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[18/1442]: training_loss: tensor(1.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[19/1442]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[20/1442]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[21/1442]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[22/1442]: training_loss: tensor(0.4457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[23/1442]: training_loss: tensor(0.2969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[24/1442]: training_loss: tensor(0.9984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[25/1442]: training_loss: tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[26/1442]: training_loss: tensor(0.6741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[27/1442]: training_loss: tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[28/1442]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[29/1442]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[30/1442]: training_loss: tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[31/1442]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[32/1442]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[33/1442]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[34/1442]: training_loss: tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[35/1442]: training_loss: tensor(0.7556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[36/1442]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[37/1442]: training_loss: tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[38/1442]: training_loss: tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[39/1442]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[40/1442]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[41/1442]: training_loss: tensor(0.8245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[42/1442]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[43/1442]: training_loss: tensor(0.4169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[44/1442]: training_loss: tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[45/1442]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[46/1442]: training_loss: tensor(0.4489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[47/1442]: training_loss: tensor(0.4134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[48/1442]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[49/1442]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[50/1442]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[51/1442]: training_loss: tensor(0.4739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[52/1442]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[53/1442]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[54/1442]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[55/1442]: training_loss: tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[56/1442]: training_loss: tensor(1.5828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[57/1442]: training_loss: tensor(0.4769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[58/1442]: training_loss: tensor(0.7295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[59/1442]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[60/1442]: training_loss: tensor(0.3707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[61/1442]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[62/1442]: training_loss: tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[63/1442]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[64/1442]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[65/1442]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[66/1442]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[67/1442]: training_loss: tensor(0.4562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[68/1442]: training_loss: tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[69/1442]: training_loss: tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[70/1442]: training_loss: tensor(0.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[71/1442]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[72/1442]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[73/1442]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[74/1442]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[75/1442]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[76/1442]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[77/1442]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[78/1442]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[79/1442]: training_loss: tensor(0.3484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[80/1442]: training_loss: tensor(0.5444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[81/1442]: training_loss: tensor(0.4814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[82/1442]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[83/1442]: training_loss: tensor(0.3479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[84/1442]: training_loss: tensor(0.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[85/1442]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[86/1442]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[87/1442]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[88/1442]: training_loss: tensor(0.6735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[89/1442]: training_loss: tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[90/1442]: training_loss: tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[91/1442]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[92/1442]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[93/1442]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[94/1442]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[95/1442]: training_loss: tensor(0.7748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[96/1442]: training_loss: tensor(0.4949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[97/1442]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[98/1442]: training_loss: tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[99/1442]: training_loss: tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[100/1442]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[101/1442]: training_loss: tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[102/1442]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[103/1442]: training_loss: tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[104/1442]: training_loss: tensor(0.5248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[105/1442]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[106/1442]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[107/1442]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[108/1442]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[109/1442]: training_loss: tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[110/1442]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[111/1442]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[112/1442]: training_loss: tensor(0.5751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[113/1442]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[114/1442]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[115/1442]: training_loss: tensor(0.4242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[116/1442]: training_loss: tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[117/1442]: training_loss: tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[118/1442]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[119/1442]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[120/1442]: training_loss: tensor(0.3654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[121/1442]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[122/1442]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[123/1442]: training_loss: tensor(0.2915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[124/1442]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[125/1442]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[126/1442]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[127/1442]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[128/1442]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[129/1442]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[130/1442]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[131/1442]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[132/1442]: training_loss: tensor(0.8837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[133/1442]: training_loss: tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[134/1442]: training_loss: tensor(0.5332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[135/1442]: training_loss: tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[136/1442]: training_loss: tensor(0.5922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[137/1442]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[138/1442]: training_loss: tensor(0.4665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[139/1442]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[140/1442]: training_loss: tensor(0.1905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[141/1442]: training_loss: tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[142/1442]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[143/1442]: training_loss: tensor(0.7598, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[144/1442]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[145/1442]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[146/1442]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[147/1442]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[148/1442]: training_loss: tensor(0.4717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[149/1442]: training_loss: tensor(0.4767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[150/1442]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[151/1442]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[152/1442]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[153/1442]: training_loss: tensor(0.5291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[154/1442]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[155/1442]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[156/1442]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[157/1442]: training_loss: tensor(0.7167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[158/1442]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[159/1442]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[160/1442]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[161/1442]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[162/1442]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[163/1442]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[164/1442]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[165/1442]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[166/1442]: training_loss: tensor(0.3181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[167/1442]: training_loss: tensor(0.6604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[168/1442]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[169/1442]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[170/1442]: training_loss: tensor(0.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[171/1442]: training_loss: tensor(0.4740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[172/1442]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[173/1442]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[174/1442]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[175/1442]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[176/1442]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[177/1442]: training_loss: tensor(0.4786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[178/1442]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[179/1442]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[180/1442]: training_loss: tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[181/1442]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[182/1442]: training_loss: tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[183/1442]: training_loss: tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[184/1442]: training_loss: tensor(0.4465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[185/1442]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[186/1442]: training_loss: tensor(0.3412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[187/1442]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[188/1442]: training_loss: tensor(0.4996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[189/1442]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[190/1442]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[191/1442]: training_loss: tensor(0.3699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[192/1442]: training_loss: tensor(0.4560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[193/1442]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[194/1442]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[195/1442]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[196/1442]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[197/1442]: training_loss: tensor(0.7460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[198/1442]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[199/1442]: training_loss: tensor(0.9602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[200/1442]: training_loss: tensor(0.4511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[201/1442]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[202/1442]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[203/1442]: training_loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[204/1442]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[205/1442]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[206/1442]: training_loss: tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[207/1442]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[208/1442]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[209/1442]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[210/1442]: training_loss: tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[211/1442]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[212/1442]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[213/1442]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[214/1442]: training_loss: tensor(0.3132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[215/1442]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[216/1442]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[217/1442]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[218/1442]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[219/1442]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[220/1442]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[221/1442]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[222/1442]: training_loss: tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[223/1442]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[224/1442]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[225/1442]: training_loss: tensor(1.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[226/1442]: training_loss: tensor(0.4371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[227/1442]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[228/1442]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[229/1442]: training_loss: tensor(0.4532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[230/1442]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[231/1442]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[232/1442]: training_loss: tensor(0.4265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[233/1442]: training_loss: tensor(0.5031, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[234/1442]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[235/1442]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[236/1442]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[237/1442]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[238/1442]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[239/1442]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[240/1442]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[241/1442]: training_loss: tensor(0.6396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[242/1442]: training_loss: tensor(0.3258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[243/1442]: training_loss: tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[244/1442]: training_loss: tensor(0.3654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[245/1442]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[246/1442]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[247/1442]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[248/1442]: training_loss: tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[249/1442]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[250/1442]: training_loss: tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[251/1442]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[252/1442]: training_loss: tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[253/1442]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[254/1442]: training_loss: tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[255/1442]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[256/1442]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[257/1442]: training_loss: tensor(0.3562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[258/1442]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[259/1442]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[260/1442]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[261/1442]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[262/1442]: training_loss: tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[263/1442]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[264/1442]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[265/1442]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[266/1442]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[267/1442]: training_loss: tensor(0.5816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[268/1442]: training_loss: tensor(0.5212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[269/1442]: training_loss: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[270/1442]: training_loss: tensor(0.3857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[271/1442]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[272/1442]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[273/1442]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[274/1442]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[275/1442]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[276/1442]: training_loss: tensor(0.5051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[277/1442]: training_loss: tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[278/1442]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[279/1442]: training_loss: tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[280/1442]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[281/1442]: training_loss: tensor(0.4612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[282/1442]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[283/1442]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[284/1442]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[285/1442]: training_loss: tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[286/1442]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[287/1442]: training_loss: tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[288/1442]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[289/1442]: training_loss: tensor(0.1877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[290/1442]: training_loss: tensor(0.4557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[291/1442]: training_loss: tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[292/1442]: training_loss: tensor(0.4332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[293/1442]: training_loss: tensor(0.4106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[294/1442]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[295/1442]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[296/1442]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[297/1442]: training_loss: tensor(0.4260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[298/1442]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[299/1442]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[300/1442]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[301/1442]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[302/1442]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[303/1442]: training_loss: tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[304/1442]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[305/1442]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[306/1442]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[307/1442]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[308/1442]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[309/1442]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[310/1442]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[311/1442]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[312/1442]: training_loss: tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[313/1442]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[314/1442]: training_loss: tensor(0.4597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[315/1442]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[316/1442]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[317/1442]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[318/1442]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[319/1442]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[320/1442]: training_loss: tensor(0.5566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[321/1442]: training_loss: tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[322/1442]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[323/1442]: training_loss: tensor(1.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[324/1442]: training_loss: tensor(0.9273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[325/1442]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[326/1442]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[327/1442]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[328/1442]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[329/1442]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[330/1442]: training_loss: tensor(0.9546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[331/1442]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[332/1442]: training_loss: tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[333/1442]: training_loss: tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[334/1442]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[335/1442]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[336/1442]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[337/1442]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[338/1442]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[339/1442]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[340/1442]: training_loss: tensor(0.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[341/1442]: training_loss: tensor(0.9246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[342/1442]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[343/1442]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[344/1442]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[345/1442]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[346/1442]: training_loss: tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[347/1442]: training_loss: tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[348/1442]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[349/1442]: training_loss: tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[350/1442]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[351/1442]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[352/1442]: training_loss: tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[353/1442]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[354/1442]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[355/1442]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[356/1442]: training_loss: tensor(0.7524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[357/1442]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[358/1442]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[359/1442]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[360/1442]: training_loss: tensor(0.5327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[361/1442]: training_loss: tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[362/1442]: training_loss: tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[363/1442]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[364/1442]: training_loss: tensor(0.8591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[365/1442]: training_loss: tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[366/1442]: training_loss: tensor(0.3468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[367/1442]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[368/1442]: training_loss: tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[369/1442]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[370/1442]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[371/1442]: training_loss: tensor(0.3462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[372/1442]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[373/1442]: training_loss: tensor(0.2104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[374/1442]: training_loss: tensor(0.4642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[375/1442]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[376/1442]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[377/1442]: training_loss: tensor(0.8228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[378/1442]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[379/1442]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[380/1442]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[381/1442]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[382/1442]: training_loss: tensor(0.3371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[383/1442]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[384/1442]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[385/1442]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[386/1442]: training_loss: tensor(0.3885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[387/1442]: training_loss: tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[388/1442]: training_loss: tensor(0.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[389/1442]: training_loss: tensor(0.3279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[390/1442]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[391/1442]: training_loss: tensor(0.5007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[392/1442]: training_loss: tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[393/1442]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[394/1442]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[395/1442]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[396/1442]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[397/1442]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[398/1442]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[399/1442]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[400/1442]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[401/1442]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[402/1442]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[403/1442]: training_loss: tensor(0.6023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[404/1442]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[405/1442]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[406/1442]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[407/1442]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[408/1442]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[409/1442]: training_loss: tensor(0.5671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[410/1442]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[411/1442]: training_loss: tensor(0.5760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[412/1442]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[413/1442]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[414/1442]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[415/1442]: training_loss: tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[416/1442]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[417/1442]: training_loss: tensor(1.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[418/1442]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[419/1442]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[420/1442]: training_loss: tensor(0.5800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[421/1442]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[422/1442]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[423/1442]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[424/1442]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[425/1442]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[426/1442]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[427/1442]: training_loss: tensor(0.1995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[428/1442]: training_loss: tensor(0.4925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[429/1442]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[430/1442]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[431/1442]: training_loss: tensor(0.4740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[432/1442]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[433/1442]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[434/1442]: training_loss: tensor(0.4815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[435/1442]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[436/1442]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[437/1442]: training_loss: tensor(0.5610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[438/1442]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[439/1442]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[440/1442]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[441/1442]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[442/1442]: training_loss: tensor(0.4558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[443/1442]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[444/1442]: training_loss: tensor(0.8156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[445/1442]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[446/1442]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[447/1442]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[448/1442]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[449/1442]: training_loss: tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[450/1442]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[451/1442]: training_loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[452/1442]: training_loss: tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[453/1442]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[454/1442]: training_loss: tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[455/1442]: training_loss: tensor(0.5240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[456/1442]: training_loss: tensor(0.5364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[457/1442]: training_loss: tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[458/1442]: training_loss: tensor(0.5158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[459/1442]: training_loss: tensor(0.4371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[460/1442]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[461/1442]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[462/1442]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[463/1442]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[464/1442]: training_loss: tensor(0.4500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[465/1442]: training_loss: tensor(0.4996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[466/1442]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[467/1442]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[468/1442]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[469/1442]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[470/1442]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[471/1442]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[472/1442]: training_loss: tensor(0.7229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[473/1442]: training_loss: tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[474/1442]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[475/1442]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[476/1442]: training_loss: tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[477/1442]: training_loss: tensor(0.7801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[478/1442]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[479/1442]: training_loss: tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[480/1442]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[481/1442]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[482/1442]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[483/1442]: training_loss: tensor(0.3314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[484/1442]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[485/1442]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[486/1442]: training_loss: tensor(1.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[487/1442]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[488/1442]: training_loss: tensor(0.3081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[489/1442]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[490/1442]: training_loss: tensor(0.3375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[491/1442]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[492/1442]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[493/1442]: training_loss: tensor(0.8857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[494/1442]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[495/1442]: training_loss: tensor(0.6212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[496/1442]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[497/1442]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[498/1442]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[499/1442]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[500/1442]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[501/1442]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[502/1442]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[503/1442]: training_loss: tensor(0.4551, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[504/1442]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[505/1442]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[506/1442]: training_loss: tensor(0.1717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[507/1442]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[508/1442]: training_loss: tensor(0.6604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[509/1442]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[510/1442]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[511/1442]: training_loss: tensor(0.3295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[512/1442]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[513/1442]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[514/1442]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[515/1442]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[516/1442]: training_loss: tensor(1.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[517/1442]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[518/1442]: training_loss: tensor(0.3161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[519/1442]: training_loss: tensor(0.2728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[520/1442]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[521/1442]: training_loss: tensor(0.5486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[522/1442]: training_loss: tensor(0.7265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[523/1442]: training_loss: tensor(0.7435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[524/1442]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[525/1442]: training_loss: tensor(0.3617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[526/1442]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[527/1442]: training_loss: tensor(0.2953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[528/1442]: training_loss: tensor(0.8115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[529/1442]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[530/1442]: training_loss: tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[531/1442]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[532/1442]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[533/1442]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[534/1442]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[535/1442]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[536/1442]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[537/1442]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[538/1442]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[539/1442]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[540/1442]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[541/1442]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[542/1442]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[543/1442]: training_loss: tensor(0.5714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[544/1442]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[545/1442]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[546/1442]: training_loss: tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[547/1442]: training_loss: tensor(0.5341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[548/1442]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[549/1442]: training_loss: tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[550/1442]: training_loss: tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[551/1442]: training_loss: tensor(0.4747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[552/1442]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[553/1442]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[554/1442]: training_loss: tensor(0.4308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[555/1442]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[556/1442]: training_loss: tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[557/1442]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[558/1442]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[559/1442]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[560/1442]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[561/1442]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[562/1442]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[563/1442]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[564/1442]: training_loss: tensor(0.2631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[565/1442]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[566/1442]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[567/1442]: training_loss: tensor(0.5284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[568/1442]: training_loss: tensor(0.4432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[569/1442]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[570/1442]: training_loss: tensor(0.4084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[571/1442]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[572/1442]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[573/1442]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[574/1442]: training_loss: tensor(0.3395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[575/1442]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[576/1442]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[577/1442]: training_loss: tensor(0.5880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[578/1442]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[579/1442]: training_loss: tensor(0.4400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[580/1442]: training_loss: tensor(0.4754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[581/1442]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[582/1442]: training_loss: tensor(1.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[583/1442]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[584/1442]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[585/1442]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[586/1442]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[587/1442]: training_loss: tensor(0.3203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[588/1442]: training_loss: tensor(0.4848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[589/1442]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[590/1442]: training_loss: tensor(0.4767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[591/1442]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[592/1442]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[593/1442]: training_loss: tensor(0.4115, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[594/1442]: training_loss: tensor(0.6794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[595/1442]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[596/1442]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[597/1442]: training_loss: tensor(0.7747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[598/1442]: training_loss: tensor(0.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[599/1442]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[600/1442]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[601/1442]: training_loss: tensor(0.4581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[602/1442]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[603/1442]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[604/1442]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[605/1442]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[606/1442]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[607/1442]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[608/1442]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[609/1442]: training_loss: tensor(0.3309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[610/1442]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[611/1442]: training_loss: tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[612/1442]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[613/1442]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[614/1442]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[615/1442]: training_loss: tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[616/1442]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[617/1442]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[618/1442]: training_loss: tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[619/1442]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[620/1442]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[621/1442]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[622/1442]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[623/1442]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[624/1442]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[625/1442]: training_loss: tensor(0.3645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[626/1442]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[627/1442]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[628/1442]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[629/1442]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[630/1442]: training_loss: tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[631/1442]: training_loss: tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[632/1442]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[633/1442]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[634/1442]: training_loss: tensor(0.5469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[635/1442]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[636/1442]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[637/1442]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[638/1442]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[639/1442]: training_loss: tensor(0.3605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[640/1442]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[641/1442]: training_loss: tensor(0.5993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[642/1442]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[643/1442]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[644/1442]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[645/1442]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[646/1442]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[647/1442]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[648/1442]: training_loss: tensor(0.3677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[649/1442]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[650/1442]: training_loss: tensor(0.4376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[651/1442]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[652/1442]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[653/1442]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[654/1442]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[655/1442]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[656/1442]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[657/1442]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[658/1442]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[659/1442]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[660/1442]: training_loss: tensor(0.4607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[661/1442]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[662/1442]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[663/1442]: training_loss: tensor(0.4100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[664/1442]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[665/1442]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[666/1442]: training_loss: tensor(0.5319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[667/1442]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[668/1442]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[669/1442]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[670/1442]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[671/1442]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[672/1442]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[673/1442]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[674/1442]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[675/1442]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[676/1442]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[677/1442]: training_loss: tensor(0.5687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[678/1442]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[679/1442]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[680/1442]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[681/1442]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[682/1442]: training_loss: tensor(0.8839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[683/1442]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[684/1442]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[685/1442]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[686/1442]: training_loss: tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[687/1442]: training_loss: tensor(0.3653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[688/1442]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[689/1442]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[690/1442]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[691/1442]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[692/1442]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[693/1442]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[694/1442]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[695/1442]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[696/1442]: training_loss: tensor(0.5348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[697/1442]: training_loss: tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[698/1442]: training_loss: tensor(0.5389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[699/1442]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[700/1442]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[701/1442]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[702/1442]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[703/1442]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[704/1442]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[705/1442]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[706/1442]: training_loss: tensor(0.3239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[707/1442]: training_loss: tensor(0.5078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[708/1442]: training_loss: tensor(0.8154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[709/1442]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[710/1442]: training_loss: tensor(0.4685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[711/1442]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[712/1442]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[713/1442]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[714/1442]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[715/1442]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[716/1442]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[717/1442]: training_loss: tensor(0.2875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[718/1442]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[719/1442]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[720/1442]: training_loss: tensor(0.2832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[721/1442]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[722/1442]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[723/1442]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[724/1442]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[725/1442]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[726/1442]: training_loss: tensor(0.9016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[727/1442]: training_loss: tensor(0.4500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[728/1442]: training_loss: tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[729/1442]: training_loss: tensor(0.5935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[730/1442]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[731/1442]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[732/1442]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[733/1442]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[734/1442]: training_loss: tensor(0.7950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[735/1442]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[736/1442]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[737/1442]: training_loss: tensor(0.5592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[738/1442]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[739/1442]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[740/1442]: training_loss: tensor(0.5438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[741/1442]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[742/1442]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[743/1442]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[744/1442]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[745/1442]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[746/1442]: training_loss: tensor(0.4346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[747/1442]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[748/1442]: training_loss: tensor(0.5050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[749/1442]: training_loss: tensor(0.3213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[750/1442]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[751/1442]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[752/1442]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[753/1442]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[754/1442]: training_loss: tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[755/1442]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[756/1442]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[757/1442]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[758/1442]: training_loss: tensor(0.1702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[759/1442]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[760/1442]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[761/1442]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[762/1442]: training_loss: tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[763/1442]: training_loss: tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[764/1442]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[765/1442]: training_loss: tensor(0.4076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[766/1442]: training_loss: tensor(0.6353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[767/1442]: training_loss: tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[768/1442]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[769/1442]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[770/1442]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[771/1442]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[772/1442]: training_loss: tensor(0.6255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[773/1442]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[774/1442]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[775/1442]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[776/1442]: training_loss: tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[777/1442]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[778/1442]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[779/1442]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[780/1442]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[781/1442]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[782/1442]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[783/1442]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[784/1442]: training_loss: tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[785/1442]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[786/1442]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[787/1442]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[788/1442]: training_loss: tensor(0.4371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[789/1442]: training_loss: tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[790/1442]: training_loss: tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[791/1442]: training_loss: tensor(0.4610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[792/1442]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[793/1442]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[794/1442]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[795/1442]: training_loss: tensor(0.6116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[796/1442]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[797/1442]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[798/1442]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[799/1442]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[800/1442]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[801/1442]: training_loss: tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[802/1442]: training_loss: tensor(0.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[803/1442]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[804/1442]: training_loss: tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[805/1442]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[806/1442]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[807/1442]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[808/1442]: training_loss: tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[809/1442]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[810/1442]: training_loss: tensor(0.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[811/1442]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[812/1442]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[813/1442]: training_loss: tensor(0.4184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[814/1442]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[815/1442]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[816/1442]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[817/1442]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[818/1442]: training_loss: tensor(0.5000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[819/1442]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[820/1442]: training_loss: tensor(0.4950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[821/1442]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[822/1442]: training_loss: tensor(0.3533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[823/1442]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[824/1442]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[825/1442]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[826/1442]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[827/1442]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[828/1442]: training_loss: tensor(0.5341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[829/1442]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[830/1442]: training_loss: tensor(0.4363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[831/1442]: training_loss: tensor(0.6353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[832/1442]: training_loss: tensor(0.2051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[833/1442]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[834/1442]: training_loss: tensor(0.5281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[835/1442]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[836/1442]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[837/1442]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[838/1442]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[839/1442]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[840/1442]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[841/1442]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[842/1442]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[843/1442]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[844/1442]: training_loss: tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[845/1442]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[846/1442]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[847/1442]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[848/1442]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[849/1442]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[850/1442]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[851/1442]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[852/1442]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[853/1442]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[854/1442]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[855/1442]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[856/1442]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[857/1442]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[858/1442]: training_loss: tensor(0.5066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[859/1442]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[860/1442]: training_loss: tensor(0.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[861/1442]: training_loss: tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[862/1442]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[863/1442]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[864/1442]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[865/1442]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[866/1442]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[867/1442]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[868/1442]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[869/1442]: training_loss: tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[870/1442]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[871/1442]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[872/1442]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[873/1442]: training_loss: tensor(0.5097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[874/1442]: training_loss: tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[875/1442]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[876/1442]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[877/1442]: training_loss: tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[878/1442]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[879/1442]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[880/1442]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[881/1442]: training_loss: tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[882/1442]: training_loss: tensor(0.2085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[883/1442]: training_loss: tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[884/1442]: training_loss: tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[885/1442]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[886/1442]: training_loss: tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[887/1442]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[888/1442]: training_loss: tensor(0.5130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[889/1442]: training_loss: tensor(0.9637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[890/1442]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[891/1442]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[892/1442]: training_loss: tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[893/1442]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[894/1442]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[895/1442]: training_loss: tensor(0.5909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[896/1442]: training_loss: tensor(0.3948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[897/1442]: training_loss: tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[898/1442]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[899/1442]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[900/1442]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[901/1442]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[902/1442]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[903/1442]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[904/1442]: training_loss: tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[905/1442]: training_loss: tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[906/1442]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[907/1442]: training_loss: tensor(0.8532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[908/1442]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[909/1442]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[910/1442]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[911/1442]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[912/1442]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[913/1442]: training_loss: tensor(0.4071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[914/1442]: training_loss: tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[915/1442]: training_loss: tensor(0.3371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[916/1442]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[917/1442]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[918/1442]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[919/1442]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[920/1442]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[921/1442]: training_loss: tensor(0.7783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[922/1442]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[923/1442]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[924/1442]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[925/1442]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[926/1442]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[927/1442]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[928/1442]: training_loss: tensor(0.9965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[929/1442]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[930/1442]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[931/1442]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[932/1442]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[933/1442]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[934/1442]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[935/1442]: training_loss: tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[936/1442]: training_loss: tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[937/1442]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[938/1442]: training_loss: tensor(0.3479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[939/1442]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[940/1442]: training_loss: tensor(0.5061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[941/1442]: training_loss: tensor(0.4696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[942/1442]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[943/1442]: training_loss: tensor(0.4459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[944/1442]: training_loss: tensor(0.5245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[945/1442]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[946/1442]: training_loss: tensor(1.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[947/1442]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[948/1442]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[949/1442]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[950/1442]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[951/1442]: training_loss: tensor(0.3306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[952/1442]: training_loss: tensor(0.5695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[953/1442]: training_loss: tensor(0.5269, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[954/1442]: training_loss: tensor(0.4407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[955/1442]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[956/1442]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[957/1442]: training_loss: tensor(0.2051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[958/1442]: training_loss: tensor(0.9771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[959/1442]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[960/1442]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[961/1442]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[962/1442]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[963/1442]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[964/1442]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[965/1442]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[966/1442]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[967/1442]: training_loss: tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[968/1442]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[969/1442]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[970/1442]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[971/1442]: training_loss: tensor(0.4576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[972/1442]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[973/1442]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[974/1442]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[975/1442]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[976/1442]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[977/1442]: training_loss: tensor(0.2631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[978/1442]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[979/1442]: training_loss: tensor(0.7403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[980/1442]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[981/1442]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[982/1442]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[983/1442]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[984/1442]: training_loss: tensor(0.4999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[985/1442]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[986/1442]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[987/1442]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[988/1442]: training_loss: tensor(0.5917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[989/1442]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[990/1442]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[991/1442]: training_loss: tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[992/1442]: training_loss: tensor(0.5454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[993/1442]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[994/1442]: training_loss: tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[995/1442]: training_loss: tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[996/1442]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[997/1442]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[998/1442]: training_loss: tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[999/1442]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1000/1442]: training_loss: tensor(0.3403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1001/1442]: training_loss: tensor(0.3237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1002/1442]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1003/1442]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1004/1442]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1005/1442]: training_loss: tensor(0.7185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1006/1442]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1007/1442]: training_loss: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1008/1442]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1009/1442]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1010/1442]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1011/1442]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1012/1442]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1013/1442]: training_loss: tensor(0.4134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1014/1442]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1015/1442]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1016/1442]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1017/1442]: training_loss: tensor(0.4923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1018/1442]: training_loss: tensor(0.4939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1019/1442]: training_loss: tensor(0.8970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1020/1442]: training_loss: tensor(0.2883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1021/1442]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1022/1442]: training_loss: tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1023/1442]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1024/1442]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1025/1442]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1026/1442]: training_loss: tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1027/1442]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1028/1442]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1029/1442]: training_loss: tensor(0.9632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1030/1442]: training_loss: tensor(0.5313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1031/1442]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1032/1442]: training_loss: tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1033/1442]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1034/1442]: training_loss: tensor(0.4124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1035/1442]: training_loss: tensor(0.8814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1036/1442]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1037/1442]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1038/1442]: training_loss: tensor(0.6795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1039/1442]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1040/1442]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1041/1442]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1042/1442]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1043/1442]: training_loss: tensor(0.3294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1044/1442]: training_loss: tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1045/1442]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1046/1442]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1047/1442]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1048/1442]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1049/1442]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1050/1442]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1051/1442]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1052/1442]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1053/1442]: training_loss: tensor(0.8559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1054/1442]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1055/1442]: training_loss: tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1056/1442]: training_loss: tensor(0.4612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1057/1442]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1058/1442]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1059/1442]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1060/1442]: training_loss: tensor(0.4445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1061/1442]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1062/1442]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1063/1442]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1064/1442]: training_loss: tensor(0.4260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1065/1442]: training_loss: tensor(0.5949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1066/1442]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1067/1442]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1068/1442]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1069/1442]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1070/1442]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1071/1442]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1072/1442]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1073/1442]: training_loss: tensor(0.2783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1074/1442]: training_loss: tensor(0.5711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1075/1442]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1076/1442]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1077/1442]: training_loss: tensor(0.4755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1078/1442]: training_loss: tensor(0.4062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1079/1442]: training_loss: tensor(0.4992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1080/1442]: training_loss: tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1081/1442]: training_loss: tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1082/1442]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1083/1442]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1084/1442]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1085/1442]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1086/1442]: training_loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1087/1442]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1088/1442]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1089/1442]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1090/1442]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1091/1442]: training_loss: tensor(0.1829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1092/1442]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1093/1442]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1094/1442]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1095/1442]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1096/1442]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1097/1442]: training_loss: tensor(0.3570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1098/1442]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1099/1442]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1100/1442]: training_loss: tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1101/1442]: training_loss: tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1102/1442]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1103/1442]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1104/1442]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1105/1442]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1106/1442]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1107/1442]: training_loss: tensor(0.4277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1108/1442]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1109/1442]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1110/1442]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1111/1442]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1112/1442]: training_loss: tensor(0.8628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1113/1442]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1114/1442]: training_loss: tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1115/1442]: training_loss: tensor(0.5050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1116/1442]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1117/1442]: training_loss: tensor(0.5460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1118/1442]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1119/1442]: training_loss: tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1120/1442]: training_loss: tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1121/1442]: training_loss: tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1122/1442]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1123/1442]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1124/1442]: training_loss: tensor(0.4448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1125/1442]: training_loss: tensor(0.4085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1126/1442]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1127/1442]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1128/1442]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1129/1442]: training_loss: tensor(0.4892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1130/1442]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1131/1442]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1132/1442]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1133/1442]: training_loss: tensor(0.3616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1134/1442]: training_loss: tensor(0.2683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1135/1442]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1136/1442]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1137/1442]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1138/1442]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1139/1442]: training_loss: tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1140/1442]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1141/1442]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1142/1442]: training_loss: tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1143/1442]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1144/1442]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1145/1442]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1146/1442]: training_loss: tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1147/1442]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1148/1442]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1149/1442]: training_loss: tensor(0.8364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1150/1442]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1151/1442]: training_loss: tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1152/1442]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1153/1442]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1154/1442]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1155/1442]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1156/1442]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1157/1442]: training_loss: tensor(0.8714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1158/1442]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1159/1442]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1160/1442]: training_loss: tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1161/1442]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1162/1442]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1163/1442]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1164/1442]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1165/1442]: training_loss: tensor(0.5073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1166/1442]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1167/1442]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1168/1442]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1169/1442]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1170/1442]: training_loss: tensor(0.5099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1171/1442]: training_loss: tensor(0.8209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1172/1442]: training_loss: tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1173/1442]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1174/1442]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1175/1442]: training_loss: tensor(0.5484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1176/1442]: training_loss: tensor(0.3262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1177/1442]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1178/1442]: training_loss: tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1179/1442]: training_loss: tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1180/1442]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1181/1442]: training_loss: tensor(0.4601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1182/1442]: training_loss: tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1183/1442]: training_loss: tensor(0.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1184/1442]: training_loss: tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1185/1442]: training_loss: tensor(0.5853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1186/1442]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1187/1442]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1188/1442]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1189/1442]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1190/1442]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1191/1442]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1192/1442]: training_loss: tensor(0.2907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1193/1442]: training_loss: tensor(0.5083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1194/1442]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1195/1442]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1196/1442]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1197/1442]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1198/1442]: training_loss: tensor(0.5541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1199/1442]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1200/1442]: training_loss: tensor(0.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1201/1442]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1202/1442]: training_loss: tensor(0.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1203/1442]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1204/1442]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1205/1442]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1206/1442]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1207/1442]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1208/1442]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1209/1442]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1210/1442]: training_loss: tensor(0.4224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1211/1442]: training_loss: tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1212/1442]: training_loss: tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1213/1442]: training_loss: tensor(0.7067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1214/1442]: training_loss: tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1215/1442]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1216/1442]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1217/1442]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1218/1442]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1219/1442]: training_loss: tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1220/1442]: training_loss: tensor(0.4367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1221/1442]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1222/1442]: training_loss: tensor(0.1739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1223/1442]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1224/1442]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1225/1442]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1226/1442]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1227/1442]: training_loss: tensor(0.4585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1228/1442]: training_loss: tensor(0.2807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1229/1442]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1230/1442]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1231/1442]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1232/1442]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1233/1442]: training_loss: tensor(0.3952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1234/1442]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1235/1442]: training_loss: tensor(0.4800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1236/1442]: training_loss: tensor(0.3457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1237/1442]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1238/1442]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1239/1442]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1240/1442]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1241/1442]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1242/1442]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1243/1442]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1244/1442]: training_loss: tensor(0.2114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1245/1442]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1246/1442]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1247/1442]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1248/1442]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1249/1442]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1250/1442]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1251/1442]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1252/1442]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1253/1442]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1254/1442]: training_loss: tensor(0.5976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1255/1442]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1256/1442]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1257/1442]: training_loss: tensor(0.2875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1258/1442]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1259/1442]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1260/1442]: training_loss: tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1261/1442]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1262/1442]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1263/1442]: training_loss: tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1264/1442]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1265/1442]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1266/1442]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1267/1442]: training_loss: tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1268/1442]: training_loss: tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1269/1442]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1270/1442]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1271/1442]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1272/1442]: training_loss: tensor(0.4769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1273/1442]: training_loss: tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1274/1442]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1275/1442]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1276/1442]: training_loss: tensor(0.8332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1277/1442]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1278/1442]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1279/1442]: training_loss: tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1280/1442]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1281/1442]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1282/1442]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1283/1442]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1284/1442]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1285/1442]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1286/1442]: training_loss: tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1287/1442]: training_loss: tensor(0.4951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1288/1442]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1289/1442]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1290/1442]: training_loss: tensor(0.4469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1291/1442]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1292/1442]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1293/1442]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1294/1442]: training_loss: tensor(0.5634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1295/1442]: training_loss: tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1296/1442]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1297/1442]: training_loss: tensor(0.5270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1298/1442]: training_loss: tensor(0.7496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1299/1442]: training_loss: tensor(0.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1300/1442]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1301/1442]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1302/1442]: training_loss: tensor(0.5173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1303/1442]: training_loss: tensor(0.5768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1304/1442]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1305/1442]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1306/1442]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1307/1442]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1308/1442]: training_loss: tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1309/1442]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1310/1442]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1311/1442]: training_loss: tensor(0.7771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1312/1442]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1313/1442]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1314/1442]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1315/1442]: training_loss: tensor(1.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1316/1442]: training_loss: tensor(0.4755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1317/1442]: training_loss: tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1318/1442]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1319/1442]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1320/1442]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1321/1442]: training_loss: tensor(0.3403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1322/1442]: training_loss: tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1323/1442]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1324/1442]: training_loss: tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1325/1442]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1326/1442]: training_loss: tensor(0.9986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1327/1442]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1328/1442]: training_loss: tensor(0.5005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1329/1442]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1330/1442]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1331/1442]: training_loss: tensor(0.2759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1332/1442]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1333/1442]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1334/1442]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1335/1442]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1336/1442]: training_loss: tensor(0.3136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1337/1442]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1338/1442]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1339/1442]: training_loss: tensor(0.5922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1340/1442]: training_loss: tensor(0.4694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1341/1442]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1342/1442]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1343/1442]: training_loss: tensor(0.5670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1344/1442]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1345/1442]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1346/1442]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1347/1442]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1348/1442]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1349/1442]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1350/1442]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1351/1442]: training_loss: tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1352/1442]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1353/1442]: training_loss: tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1354/1442]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1355/1442]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1356/1442]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1357/1442]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1358/1442]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1359/1442]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1360/1442]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1361/1442]: training_loss: tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1362/1442]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1363/1442]: training_loss: tensor(0.5692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1364/1442]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1365/1442]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1366/1442]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1367/1442]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1368/1442]: training_loss: tensor(1.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1369/1442]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1370/1442]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1371/1442]: training_loss: tensor(0.6468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1372/1442]: training_loss: tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1373/1442]: training_loss: tensor(0.9471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1374/1442]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1375/1442]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1376/1442]: training_loss: tensor(0.3133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1377/1442]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1378/1442]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1379/1442]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1380/1442]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1381/1442]: training_loss: tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1382/1442]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1383/1442]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1384/1442]: training_loss: tensor(0.2790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1385/1442]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1386/1442]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1387/1442]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1388/1442]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no[1389/1442]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1390/1442]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1391/1442]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1392/1442]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1393/1442]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1394/1442]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1395/1442]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1396/1442]: training_loss: tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1397/1442]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1398/1442]: training_loss: tensor(0.3786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1399/1442]: training_loss: tensor(0.4485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1400/1442]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1401/1442]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1402/1442]: training_loss: tensor(0.4251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1403/1442]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1404/1442]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1405/1442]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1406/1442]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1407/1442]: training_loss: tensor(0.4806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1408/1442]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1409/1442]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1410/1442]: training_loss: tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1411/1442]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1412/1442]: training_loss: tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1413/1442]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1414/1442]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1415/1442]: training_loss: tensor(0.4521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1416/1442]: training_loss: tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1417/1442]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1418/1442]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1419/1442]: training_loss: tensor(0.2745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1420/1442]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1421/1442]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1422/1442]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1423/1442]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1424/1442]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1425/1442]: training_loss: tensor(0.2650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1426/1442]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1427/1442]: training_loss: tensor(0.7870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1428/1442]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1429/1442]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1430/1442]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1431/1442]: training_loss: tensor(0.2758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1432/1442]: training_loss: tensor(0.4656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1433/1442]: training_loss: tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1434/1442]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1435/1442]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1436/1442]: training_loss: tensor(0.4178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1437/1442]: training_loss: tensor(0.3422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1438/1442]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1439/1442]: training_loss: tensor(0.6114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1440/1442]: training_loss: tensor(0.8112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no[1441/1442]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/3],global step [4326/4326], pt loss:0.0000, dev loss:0.6296\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 2e-6)\n",
    "Num_Epoch = 3\n",
    "train(model = model, train_iter=train_iter, dev_iter=dev_iter,optimizer=optimizer,epochs = Num_Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "# Evaluation Function\n",
    "import seaborn as sns\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (source, target), _ in test_loader:\n",
    "                mask = (source != pad_index).type(torch.uint8)\n",
    "                \n",
    "                output = model(source, attention_mask=mask)[0]\n",
    "                y_prob.extend(output.tolist())\n",
    "                y_pred.extend(torch.argmax(output, axis=-1).tolist())\n",
    "                y_true.extend(target.tolist())\n",
    "    \n",
    "    label_true = []\n",
    "    for i in y_true:\n",
    "        if i == 1:\n",
    "            label_true.append([1,0])\n",
    "        else:\n",
    "            label_true.append([0,1])\n",
    "    y_prob_final = []\n",
    "    for i in range(len(y_prob)):\n",
    "        tempA = abs(y_prob[i][0])\n",
    "        tempB = abs(y_prob[i][1])\n",
    "        y_prob_final.append(tempA/(tempA+tempB))\n",
    "\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    plt.figure(1, figsize=(20,8))\n",
    "\n",
    "    ax= plt.subplot(121)\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "    ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "    fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "    plt.subplot(122)\n",
    "    lw = 2\n",
    "    plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "             lw=lw, label='roc curve')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid()\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    print('Accuracy is:', accuracy_score(y_true, y_pred))\n",
    "    print('Precision is:', precision_score(y_true, y_pred))\n",
    "    print('Recall is:', recall_score(y_true, y_pred))\n",
    "    print('F1 score is:', f1_score(y_true, y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6356    0.5388    0.5832      1470\n",
      "           0     0.7659    0.8301    0.7967      2672\n",
      "\n",
      "    accuracy                         0.7267      4142\n",
      "   macro avg     0.7008    0.6844    0.6900      4142\n",
      "weighted avg     0.7197    0.7267    0.7209      4142\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7267020762916465\n",
      "Precision is: 0.6356340288924559\n",
      "Recall is: 0.5387755102040817\n",
      "F1 score is: 0.5832106038291606\n"
     ]
    }
   ],
   "source": [
    "pred = evaluate(model,dev_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5442    0.7029    0.6135       175\n",
      "           0     0.9150    0.8446    0.8784       663\n",
      "\n",
      "    accuracy                         0.8150       838\n",
      "   macro avg     0.7296    0.7738    0.7459       838\n",
      "weighted avg     0.8376    0.8150    0.8231       838\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.815035799522673\n",
      "Precision is: 0.5442477876106194\n",
      "Recall is: 0.7028571428571428\n",
      "F1 score is: 0.6134663341645886\n"
     ]
    }
   ],
   "source": [
    "pred = evaluate(model,test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS] all the home alones watching 8 mile [SEP...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CLS] all the home alones watching 8 mile [SEP...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS] the ending to 8 mile is my fav part of t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CLS] the ending to 8 mile is my fav part of t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[CLS] the ending to 8 mile is my fav part of t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  label\n",
       "0  [CLS] all the home alones watching 8 mile [SEP...      0\n",
       "1  [CLS] all the home alones watching 8 mile [SEP...      0\n",
       "2  [CLS] the ending to 8 mile is my fav part of t...      0\n",
       "3  [CLS] the ending to 8 mile is my fav part of t...      0\n",
       "4  [CLS] the ending to 8 mile is my fav part of t...      1"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample data set are:\n",
      "bert model 2: ('all the home alones watching 8 mile', 'the last rap battle in 8 mile nevr gets old ahah') Lable is: 0 Prediction: 0\n",
      "bert model 2: ('ok good the end of 8 mile is on', 'the end of 8 mile makes me so happy') Lable is: 1 Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"The sample data set are:\")\n",
    "print(\"bert model 2:\", (Test_cleaned['S1'][0], Test_cleaned['S2'][0]), 'Lable is:',test_df['label'][0], \"Prediction:\", pred[0])\n",
    "print(\"bert model 2:\", (Test_cleaned['S1'][20], Test_cleaned['S2'][20]), 'Lable is:',test_df['label'][20], \"Prediction:\", pred[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "5180 A3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a3df4068fb84ccabb53b8ba6c5fee33": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e00b6cd83fd41c29b93b3420efce1ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a3df4068fb84ccabb53b8ba6c5fee33",
      "placeholder": "​",
      "style": "IPY_MODEL_3d770df66e614d7288f2adc92d09f966",
      "value": " 232k/232k [00:00&lt;00:00, 805kB/s]"
     }
    },
    "2975d8d5f30c455faf7b4c3e2207a34f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d770df66e614d7288f2adc92d09f966": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a57d35be08c402a9e95fc460be6d893": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3053991b3054e60bf7bca1b823bec27",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_adc7f582ff6349d18a178239650c3667",
      "value": 231508
     }
    },
    "8eca8af0ddf845fb9af180af46a1c893": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a57d35be08c402a9e95fc460be6d893",
       "IPY_MODEL_1e00b6cd83fd41c29b93b3420efce1ff"
      ],
      "layout": "IPY_MODEL_2975d8d5f30c455faf7b4c3e2207a34f"
     }
    },
    "adc7f582ff6349d18a178239650c3667": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e3053991b3054e60bf7bca1b823bec27": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
